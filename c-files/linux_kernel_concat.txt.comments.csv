44	2023	"linux/kernel/acct.c

BSD Process Accounting for Linux

Author: Marco van Wieringen <mvw@planets.elm.net>

Some code based on ideas and code from:
Thomas K. Dyas <tdyas@eden.rutgers.edu>

This file implements BSD-style process accounting. Whenever any
process exits, an accounting record of type ""struct acct"" is
written to the file specified with the acct() system call. It is
up to user-level programs to do useful things with the accounting
log. The kernel just provides the raw accounting information.

(C) Copyright 1995 - 1997 Marco van Wieringen - ELM Consultancy B.V.

Plugged two leaks. 1) It didn't return acct_file into the free_filps if
the file happened to be read-only. 2) If the accounting was suspended
due to the lack of space it happily allowed to reopen it and completely
lost the old acct_file. 3/10/98, Al Viro.

Now we silently close acct_file on attempt to reopen. Cleaned sys_acct().
XTerms and EMACS are manifestations of pure evil. 21/10/98, AV.

Fixed a nasty interaction with with sys_umount(). If the accointing
was suspeneded we failed to stop it on umount(). Messy.
Another one: remount to readonly didn't stop accounting.
Question: what should we do if we have CAP_SYS_ADMIN but not
CAP_SYS_PACCT? Current code does the following: umount returns -EBUSY
unless we are messing with the root. In that case we are getting a
real mess with do_remount_sb(). 9/11/98, AV.

Fixed a bunch of races (and pair of leaks). Probably not the best way,
but this one obviously doesn't introduce deadlocks. Later. BTW, found
one race (and leak) in BSD implementation.
OK, that's better. ANOTHER race and leak in BSD variant. There always
is one more bug... 10/11/98, AV.

Oh, fsck... Oopsable SMP race in do_process_acct() - we must hold
->mmap_sem to walk the vma list of current->mm. Nasty, since it leaks
a struct file opened for write. Fixed. 2/6/2000, AV."
60	2418	sector_div
67	2637	"These constants control the amount of freespace that suspend and
resume the process accounting system, and the time delay between
each check.
Turned into sysctl-controllable parameters. AV, 12/11/98"
72	2732	">foo% free space - resume
<foo% free space - suspend
foo second timeout between checks"
76	2928	External references and all of the globals.
83	3170	"This structure is used so that all the data protected by lock
can be placed in the same cache line as the lock. This primes
the cache line to have the data after getting the lock."
97	3478	Called whenever the timer says to check the free space.
105	3632	Check the amount of free space and suspend/resume accordingly.
120	3924	May block
139	4331	"If some joker switched acct_globals.file under us we'ld better be
silent and _not_ touch anything."
174	5090	"Close the old accounting file (if currently open) and then replace
it with file (if non-NULL).

NOTE: acct_globals.lock MUST be held on entry and exit."
190	5514	It's been deleted if it was used before so this is safe
210	5993	Difference from BSD - they don't do O_APPEND
236	6556	it's pinned, now give up active reference
251	6985	"sys_acct - enable/disable process accounting
@name: file name for accounting records or NULL to shutdown accounting

Returns 0 for success or negative errno values for failure.

sys_acct() is the only system call needed to implement process
accounting. It takes the name of the file where accounting records
should be written. If the filename is NULL, accounting will be
shutdown."
282	7649	"acct_auto_close - turn off a filesystem's accounting if it is on
@m: vfsmount being shut down

If the accounting is turned on for a file in the subtree pointed to
to by m, turn accounting off. Done when m is about to die."
297	8076	"acct_auto_close - turn off a filesystem's accounting if it is on
@sb: super block for the filesystem

If the accounting is turned on for a file in the filesystem pointed
to by sb, turn accounting off."
314	8550	"encode an unsigned long into a comp_t

This routine has been adopted from the encode_comp_t() function in
the kern_acct.c file of the FreeBSD operating system. The encoding
is a 13-bit fraction with a 3-bit (base 8) exponent."
318	8596	"13 bit mantissa.
Base 8 (3 bit) exponent.
Maximum fractional value."
327	8885	"Round up?
Base 8 exponent == 3 bit shift."
333	9033	If we need to round up, do it (and handle overflow correctly).
343	9147	"Clean it up and polish it off.
Shift the exponent into place
and add on the mantissa."
355	9538	"encode an u64 into a comp2_t (24 bits)

Format: 5 bit base 2 exponent, 20 bits mantissa.
The leading bit of the mantissa is not stored, but implied for
non-zero exponents.
Largest encodable value is 50 bits."
360	9610	"20 bit mantissa.
5 bit base 2 exponent.
Maximum fractional value.
Maximum exponent."
376	10095	If we need to round up, do it (and handle overflow correctly).
383	10245	Overflow. Return largest representable number instead.
394	10443	encode an u64 into a 32 bit IEEE float
417	10988	"Write an accounting entry for an exiting process

The acct_process() call is the workhorse of the process
accounting system. The struct acct is built here and then written
into the accounting file. This function should only be called from
do_exit() or when switching to a different output file."
421	11074	do_acct_process does all actual work. Caller holds the reference to file.
436	11418	"First check to see if there is enough free_space to continue
the process accounting system."
443	11571	"Fill the accounting struct with the needed info as recorded
by the different kernel functions."
449	11753	calculate run_time in nsec
454	12014	convert nsec -> AHZ
464	12327	new enlarged etime field
472	12566	we really need to bite the bullet and change layout
479	12745	backward-compatible 16 bit fields
499	13510	current->io_usage
499	13521	%%
506	13692	"Kernel segment override to datasegment and write it
to the accounting file."
511	13796	Accounting records are not subject to resource limits.
523	14181	"acct_init_pacct - initialize a new pacct_struct
@pacct: per-process accounting info struct to initialize"
534	14508	"acct_collect - collect accounting information into pacct_struct
@exitcode: task exit code
@group_dead: not 0, if this thread is the last one in the process."
577	15716	"acct_process - now just a wrapper around do_acct_process
@exitcode: task exit code

handles process accounting for an exiting task"
584	15815	accelerate the common fastpath:
624	16557	with root here
631	16736	index; upper bit indicates 'will prune'
665	17974	"One struct chunk is attached to each inode of interest.
We replace struct chunk on tagging/untagging.
Rules have pointer to struct audit_tree.
Rules have struct list_head rlist forming a list of rules over
the same tree.
References to struct chunk are collected at audit_inode{,_child}()
time and used in AUDIT_TREE rule matching.
These references are dropped at the same time we are calling
audit_free_names(), etc.

Cyclic lists galore:
tree.chunks anchors chunk.owners[].list hash_lock
tree.rules anchors rule.rlist audit_filter_mutex
chunk.trees anchors tree.same_root hash_lock
chunk.hash is a hash with middle bits of watch.inode as
a hash function. RCU, hash_lock

tree is refcounted; one reference for ""some rules on rules_list refer to
it"", one for each chunk with pointer to it.

chunk is refcounted by embedded inotify_watch.

node.index allows to get from node.list to containing chunk.
MSB of that sucker is stolen to mark taggings that we might have to
revert - several operations have very unpleasant cleanup logics and
that makes a difference. Some."
704	18841	to avoid bringing the entire thing in audit.h
764	20209	hash_lock is held by caller
771	20391	called under rcu_read_lock
796	20964	tagging and untagging inodes with trees
854	22290	result of earlier fallback
870	22699	do the best we can
918	23868	the first tagged inode becomes root of tree
932	24230	are we already there?
967	25147	result of fallback in untag
1009	26220	not a half-baked one
1029	26784	finish killing struct audit_tree
1051	27276	trim the uncommitted chunks from tree
1061	27453	reorder
1077	27858	have we run out of marked?
1103	28390	called with audit_filter_mutex
1218	30946	called with audit_filter_mutex
1238	31448	do not set rule->tree yet
1412	35021	"That gets run when evict_chunk() ends up needing to kill audit_tree.
Runs from a separate thread, with audit_cmd_mutex held."
1435	35457	Here comes the stuff asynchronous to auditctl operations
1437	35495	inode->inotify_mutex is locked
1544	39049	"audit.c -- Auditing support
Gateway between the kernel (e.g., selinux) and the user-space audit daemon.
System-call specific features have moved to auditsc.c


All Rights Reserved.

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

Written by Rickard E. (Rik) Faith <faith@redhat.com>

Goals: 1) Integrate fully with SELinux.
2) Minimal run-time overhead:
a) Minimal when syscall auditing is disabled (audit_enable=0).
b) Small when syscall auditing is enabled and no audit record
is generated (defer as much work as possible to record
generation time):
i) context is allocated,
ii) names from getname are stored without a copy, and
iii) inode information stored from path_lookup.
3) Ability to disable syscall auditing at boot time (audit=0).
4) Usable by other parts of the kernel (if audit_log* is called,
then a syscall record will be generated automatically for the
current syscall).
5) Netlink interface to user-space.
6) Support low-overhead kernel-based filtering to minimize the
information that must be passed to user-space.

Example user-space utilities: http://people.redhat.com/sgrubb/audit/"
1568	39589	"No auditing will take place until audit_initialized != 0.
(Initialization happens after skb_init is called.)"
1573	39732	"0 - no auditing
1 - auditing enabled
2 - auditing enabled and configuration is locked/unchangeable."
1576	39815	Default state when kernel boots without any parameters.
1579	39912	If auditing cannot proceed, audit_failure selects what happens.
1583	40065	"If audit records are to be written to the netlink socket, audit_pid
contains the (non-zero) pid."
1588	40266	"If audit_rate_limit is non-zero, limit the rate of sending audit records
to that number per second. This prevents DoS attacks, but results in
audit records being dropped."
1591	40347	Number of outstanding audit_buffers allowed.
1596	40538	The identity of the user shutting down the audit system.
1607	40901	"Records can be lost in several ways:
0) [suppressed in audit_alloc]
1) out of memory in audit_log_start [kmalloc of struct audit_buffer]
2) out of memory in audit_log_move [alloc_skb]
3) suppressed due to audit_rate_limit
4) suppressed due to audit_backlog_limit"
1610	40976	The netlink socket.
1613	41031	Inotify handle.
1616	41098	Hash for inode-based rules
1621	41335	"The audit_freelist is a list of pre-allocated audit buffers (if more
than AUDIT_MAXFREE are in use, the audit buffer is freed instead of
being placed on the freelist)."
1631	41676	Serialize requests from userspace.
1636	41893	"AUDIT_BUFSIZ is the size of the temporary buffer used for formatting
audit records. Since printk uses a 1024 byte buffer, this buffer
should be at least that large."
1640	42058	"AUDIT_MAXFREE is the number of empty audit_buffers we keep on the
audit_freelist. Doing so eliminates many kmalloc/kfree calls."
1647	42406	"The audit_buffer is used when formatting an audit record. The caller
locks briefly to get the record off the freelist or to allocate the
buffer, and locks briefly to send the buffer to the netlink layer or
to place it on a transmit queue. Multiple audit_buffers can be in
use simultaneously."
1651	42518	"formatted skb ready to send
NULL or associated context"
1712	43817	"audit_log_lost - conditionally log lost audit message event
@message: the message stating reason for lost audit message

Emit at least 1 message per second, even if audit_rate_check is
throttling.
Always increment the lost messages counter."
1749	44638	check if we are locked
1765	45023	Something weird, deny request
1771	45199	If we are allowed, make the change
1774	45276	Not allowed, update reason
1784	45474	check if we are locked
1800	45862	Something weird, deny request
1806	46041	If we are allowed, make the change
1809	46121	Not allowed, update reason
1822	46355	check if we are locked
1838	46737	Something weird, deny request
1844	46910	If we are allowed, make the change
1847	46984	Not allowed, update reason
1862	47291	check if we are locked
1878	47673	Something weird, deny request
1884	47846	If we are allowed, make the change
1887	47920	Not allowed, update reason
1905	48327	Shoudn't happen
1961	49461	wait for parent to finish and send an ACK
2008	50453	Used by NLMSG_PUT
2026	50876	"audit_send_reply - send an audit reply message via netlink
@pid: process id to send reply to
@seq: sequence number
@type: audit message type
@done: done (last) flag
@multi: multi-part message flag
@payload: payload data
@size: payload size

Allocates an skb, builds the netlink message, and sends it to the pid.
No failure notifications."
2035	51204	"Ignore failure. It'll only happen if the sender goes away,
because our timeout is set to infinite."
2043	51354	"Check for appropriate CAP_AUDIT_ capabilities on incoming audit
control messages."
2071	52023	bad msg
2086	52339	loginuid of sender
2096	52555	"As soon as there's any sign of userspace auditd,
start kauditd to talk to it"
2188	55327	Maybe call audit_panic?
2226	56205	Maybe call audit_panic?
2238	56435	fallthrough
2260	57068	Maybe call audit_panic?
2272	57298	fallthrough
2319	58427	OK, here comes...
2416	60754	"Get message from skb (based on rtnetlink_rcv_skb). Each message is
processed by audit_receive_msg. Malformed skbs with wrong length are
discarded silently."
2438	61295	Receive messages from netlink socket.
2453	61656	Initialize audit support at boot time.
2472	62252	"Register the callback with selinux. This callback will be invoked
when a new policy is loaded."
2490	62726	Process kernel command-line parameter at boot time. audit=0 or audit=1.
2579	65113	"audit_serial - compute a serial number for the audit record

Compute a serial number for the audit record. Audit records are
written to user-space as soon as they are generated, so a complete
audit record may be written in several pieces. The timestamp of the
record and this serial number are used by the user-space tools to
determine which pieces belong to the same audit record. The
(timestamp,serial) tuple is unique for each syscall and is live from
syscall entry to syscall exit.

NOTE: Another possibility is to store the formatted records off the
audit context (for those records that have a context), and emit them
all at syscall exit. However, this could delay the reporting of
significant errors until syscall exit (or never, if the system
halts)."
2613	66003	"Obtain an audit buffer. This routine does locking to obtain the
audit buffer, but then no locking is required for calls to
audit_log_*format. If the tsk is a task that is currently in a
syscall, then the syscall is marked as auditable and an audit record
will be written at syscall exit. If there is no associated task, tsk
should be NULL."
2629	66610	"audit_log_start - obtain an audit buffer
@ctx: audit_context (may be NULL)
@gfp_mask: type of allocation
@type: audit message type

Returns audit_buffer pointer on success or NULL on error.

Obtain an audit buffer. This routine does locking to obtain the
audit buffer, but then no locking is required for calls to
audit_log_*format. If the task (ctx) is a task that is currently in a
syscall, then the syscall is marked as auditable and an audit record
will be written at syscall exit. If there is no associated task, then
task context (ctx) should be NULL."
2649	67090	"Allow atomic callers to go up to five
entries over the normal backlog limit"
2656	67380	Wait for auditd to drain the queue a little
2701	68614	"audit_expand - expand skb in the audit buffer
@ab: audit_buffer
@extra: space to add at tail of the skb

Returns 0 (no space) on failed expansion, or available space if
successful."
2719	69195	"Format an audit message into the audit buffer. If there isn't enough
room in the audit buffer, more room will be allocated and vsnprint
will be called a second time. Currently, we assume that a printk
can't format message larger than 1024 bytes, so we don't either."
2743	69793	"The printk buffer is 1024 bytes long, so if we get
here and AUDIT_BUFSIZ is at least 1024, then we can
log everything that printk could have logged."
2763	70224	"audit_log_format - format a message into the audit buffer.
@ab: audit_buffer
@fmt: format string
@...: optional parameters matching @fmt string

All the work is done in audit_log_vformat."
2785	70786	"audit_log_hex - convert a buffer to hex and append it to the audit skb
@ab: the audit_buffer
@buf: buffer to convert to hex
@len: length of @buf to be converted

No return value; failure to expand is silently ignored.

This function will take the passed buf and convert it into a string of
ascii hex digits. The new string is placed onto the skb."
2802	71180	Round the buffer request up to the next multiple
2812	71419	"Upper nibble
Lower nibble"
2815	71551	new string is twice the old string
2821	71663	"Format a string of no more than slen characters into the audit buffer,
enclosed in quote marks."
2835	71975	enclosing quotes + null terminator
2847	72238	don't include null terminator
2863	72851	"audit_log_n_untrustedstring - log a string that may contain random characters
@ab: audit_buffer
@len: lenth of string (not including trailing null)
@string: string to be logged

This code will escape a string that is passed to it if the string
contains a control character, unprintable character, double quote mark,
or a space. Unescaped strings will start and end with a double quote mark.
Strings that are escaped are printed in hex (2 digits per char).

The caller specifies the number of characters in the string to log, which may
or may not be the entire string."
2886	73467	"audit_log_untrustedstring - log a string that may contain random characters
@ab: audit_buffer
@string: string to be logged

Same as audit_log_n_untrustedstring(), except that strlen is used to
determine string length."
2892	73680	This is a helper-function to print the escaped d_path
2901	73939	We will allow 11 spaces for ' (deleted)' to be appended
2909	74164	"Should never happen since we send PATH_MAX
FIXME: can we save some information here?"
2924	74619	"audit_log_end - end one audit record
@ab: the audit_buffer

The netlink_* functions cannot be called inside an irq context, so
the audit buffer is placed on a queue and a tasklet is scheduled to
remove them from the queue outside the irq context. May be called in
any context."
2956	75451	"audit_log - Log an audit record
@ctx: audit context
@gfp_mask: type of allocation
@type: audit message type
@fmt: format string to use
@...: variable parameters matching the format string

This is a convenience function that calls audit_log_start,
audit_log_vformat, and audit_log_end. It may be called
in any context."
2997	76773	"auditfilter.c -- filtering of audit events





This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA"
3022	77559	"Locking model:

audit_filter_mutex:
Synchronizes writes and blocking reads of audit's filterlist
data. Rcu is used to traverse the filterlist and access
contents of structs audit_entry, audit_watch and opaque
selinux rules during filtering. If modified, these structures
must be copied and replace their counterparts in the filterlist.
An audit_parent struct is not accessed during filtering, so may
be written directly provided audit_filter_mutex is held."
3034	78017	"Reference counting:

audit_parent: lifetime is from audit_init_parent() to receipt of an IN_IGNORED
event. Each audit_watch holds a reference to its associated parent.

audit_watch: if added to lists, lifetime is from audit_init_watch() to
audit_remove_watch(). Additionally, an audit_watch may exist
temporarily to assist in searching existing filter data. Each
audit_krule holds a reference to its associated watch."
3040	78106	"entry in inotify registration list
associated watches
inotify watch data
status flags"
3051	78651	"audit_parent status flags:

AUDIT_PARENT_INVALID - set anytime rules/watches are auto-removed due to
a filesystem event to ensure we're adding audit watches to a valid parent.
Technically not needed for IN_DELETE_SELF or IN_UNMOUNT events, as we cannot
receive them while we have nameidata, but must be used for IN_MOVE_SELF which
we can receive while holding nameidata."
3054	78740	Audit filter lists, defined in <linux/audit.h>
3069	79165	Inotify handle
3072	79242	Inotify events we care about.
3104	80035	match initial get
3111	80155	some rules don't have associated watches
3131	80636	Initialize a parent watch entry.
3145	81021	grab a ref so inotify watch hangs around until we take audit_filter_mutex
3157	81285	Initialize a watch entry.
3175	81674	Initialize an audit filterlist entry.
3196	82137	"Unpack a filter field's string representation from user-space
buffer."
3206	82393	"Of the currently implemented string fields, PATH_MAX
defines the longest valid length."
3222	82682	Translate an inode field to kernel respresentation.
3234	82979	Translate a watch string to kernel respresentation.
3310	84718	"When arch is unspecified, we must check both masks on biarch
as syscall number alone is ambiguous."
3318	84953	native
3321	85069	32bit on biarch
3330	85244	Common user-space to kernel rule translation.
3396	86807	"Translate struct audit_rule to kernel's rule respresentation.
Exists for backward compatibility with userspace."
3436	87737	bit ops are only useful on syscall args
3448	87987	arch is only allowed to be = or !=
3471	88536	"Support for legacy operators where
AUDIT_NEGATE bit signifies != and otherwise assumes =="
3503	89077	Translate struct audit_rule_data to kernel's rule respresentation.
3578	90912	"Keep currently invalid fields around in case they
become valid after a policy reload."
3658	92519	Pack a filter field's string representation into data block.
3670	92795	"Translate kernel rule respresentation to struct audit_rule.
Exists for backward compatibility with userspace."
3699	93570	Translate kernel rule respresentation to struct audit_rule_data.
3757	95062	"Compare two rules in kernel format. Considered success if rules
don't match."
3797	96077	both filterkeys exist based on above type compare
3815	96450	"Duplicate the given audit watch. The new watch's rules list is initialized
to an empty list and wlist is undefined."
3841	96975	"Duplicate selinux field information. The se_rule is opaque, so must be
re-initialized."
3848	97136	our own copy of se_str
3854	97287	our own (refreshed) copy of se_rule
3858	97470	"Keep currently invalid fields around in case they
become valid after a policy reload."
3873	98013	"Duplicate an audit rule. This will be a deep copy with the exception
of the watch - that pointer is carried over. The selinux specific fields
will be updated in the copy. The point is to be able to replace the old
rule with the new rule in the filterlist, then free the old rule.
The rlist element is undefined; list manipulations are handled apart from
the initial copy."
3904	98957	"note that we are OK with not refcounting here; audit_match_tree()
never dereferences tree and we can't get false positives there
since we'd have to have rule gone from the list *and* removed
before the chunks found by lookup had been allocated, i.e. before
the beginning of list scan."
3909	99190	"deep copy this information, updating the se_rule fields, because
the originals will all be freed when the old rule is freed."
3946	99956	Update inode info in audit rules based on filesystem event.
3962	100544	"If the update involves invalidating rules, do the inode-based
filtering now, so we don't omit records."
4002	101878	event applies to a single watch
4013	102119	Remove all watches & rules associated with a parent that is going away.
4048	103197	"Unregister inotify watches for parents on in_list.
Generates an IN_IGNORED event."
4056	103463	the put matching the get in audit_do_del_rule()
4062	103603	"Find an existing audit rule.
Caller must hold audit_filter_mutex to prevent stale rule data."
4070	103846	we don't know the inode number, so must walk entire hash
4092	104275	Get path information necessary for adding watches.
4128	104944	Release resources used for watch path information.
4142	105230	"Associate the given rule with an existing parent inotify_watch.
Caller must hold audit_filter_mutex."
4155	105571	put krule's and initial refs to temporary watch
4174	105956	"Find a matching watch entry, or add this one.
Caller must hold audit_filter_mutex."
4183	106218	update watch filter fields
4193	106568	"The audit_filter_mutex must not be held during inotify calls because
we hold it during inotify event callback processing. If an existing
inotify watch is found, inotify_find_watch() grabs a reference before
returning."
4199	106774	caller expects mutex locked
4208	107005	parent was moved before we took audit_filter_mutex
4214	107170	match get in audit_init_parent or inotify_find_watch
4219	107277	Add rule to given filterlist if not a duplicate.
4232	107696	If either of these, don't count towards total
4248	108095	normally audit_add_tree_rule() will free it on failure
4254	108209	Avoid calling path_lookup under audit_filter_mutex.
4263	108416	audit_filter_mutex is dropped and re-taken during this call
4295	109129	NULL args OK
4299	109194	NULL args OK
4301	109269	tmp watch, matches initial get
4305	109332	Remove an existing rule from filterlist.
4318	109750	If either of these, don't count towards total
4350	110504	"Put parent on the inotify un-registration
list. Grab a reference before releasing
audit_filter_mutex, to be released in
audit_inotify_unregister()."
4377	111037	match initial get
4379	111103	that's the temporary one
4385	111216	"List rules using struct audit_rule. Exists for backward
compatibility with userspace."
4393	111455	"This is a blocking read, so use audit_filter_mutex instead of rcu
iterator to sync with list writers."
4427	112313	List rules using struct audit_rule_data.
4435	112554	"This is a blocking read, so use audit_filter_mutex instead of rcu
iterator to sync with list writers."
4469	113442	Log rule additions and removals
4507	114543	"audit_receive_filter - apply all rules to the specified message type
@type: audit message type
@pid: target pid for netlink audit messages
@uid: target uid for netlink audit messages
@seq: netlink audit message sequence (serial) number
@data: payload data
@datasz: size of payload data
@loginuid: loginuid of sender
@sid: SE Linux Security ID of sender"
4523	115113	"We can't just spew out the rules here because we might fill
the available socket buffer space and deadlock waiting for
auditctl to read from it... which isn't ever going to
happen if we're actually running in the context of auditctl
trying to _send_ the stuff"
4609	117085	"Compare given dentry name with last component in given path,
return of 0 indicates a match."
4624	117353	disregard trailing slashes
4629	117450	find last path component
4640	117620	return length of path's directory component
4697	118923	Audit by default
4729	119649	"Check to see if the rule contains any selinux fields. Returns 1 if there
are selinux fields specified in the rule, 0 otherwise."
4758	120435	"This function will re-initialize the se_rule field of all applicable rules.
It will traverse the filter lists serarching for rules that contain selinux
specific filter fields. When such a rule is found, it is copied, the
selinux field is re-initialized, and the old rule is replaced with the
updated rule."
4766	120637	audit_filter_mutex synchronizes the writers
4779	121057	"save the first error encountered for the
return value"
4805	121679	Update watch data in audit rules based on inotify events.
4818	122213	inotify automatically removes the watch and sends IN_IGNORED
4821	122362	inotify does not remove the watch, so remove it manually
4850	123368	"Generic process-grouping system.

Based originally on the cpuset system, extracted by Paul Menage



--------------------------------------------------



Portions derived from Patrick Mochel's sysfs code.
sysfs is Copyright (c) 2001-3 Patrick Mochel

2003-10-10 Written by Simon Derr.
2003-10-22 Updates by Stephen Hemminger.
2004 May-July Rework by Paul Jackson.
---------------------------------------------------

This file is subject to the terms and conditions of the GNU General Public
License. See the file COPYING in the main directory of the Linux
distribution for more details."
4879	124056	Generate an array of cgroup subsystem pointers
4890	124314	"A cgroupfs_root represents the root of a cgroup hierarchy,
and may be associated with a superblock to form an active
hierarchy"
4897	124448	"The bitmask of subsystems intended to be attached to this
hierarchy"
4900	124547	The bitmask of subsystems currently attached to this hierarchy
4903	124637	A list running through the attached subsystems
4906	124711	The root cgroup for this hierarchy
4909	124805	Tracks how many cgroups are currently defined in hierarchy.
4912	124884	A list running through the mounted hierarchies
4915	124946	Hierarchy-specific flags
4922	125216	"The path to use for release notifications. No locking
between setting and use - so if userspace updates this
while child cgroups exist, you could miss a
notification. We ensure that it's always a valid
NUL-terminated string"
4931	125463	"The ""rootnode"" hierarchy is the ""dummy hierarchy"", reserved for the
subsystems that are otherwise unattached - it never has more than a
single cgroup, and all tasks are part of that cgroup."
4934	125536	The list of hierarchy roots
4939	125653	dummytop is a shorthand for the dummy hierarchy's top cgroup
4946	125946	"This flag indicates whether tasks in the fork and exit paths should
take callback_mutex and check for fork/exit handlers to call. This
avoids us having to do extra work in the fork/exit path if none of the
subsystems need to be called."
4949	126022	bits in struct cgroup flags field
4951	126058	Control Group is dead
4954	126197	"Control Group has previously had a child cgroup or a task,
but no longer (only if CGRP_NOTIFY_ON_RELEASE is set)"
4956	126280	Control Group requires release notifications to userspace
4960	126347	convenient tests for these bits
4966	126501	bits in struct cgroupfs_root flags field
4968	126570	mounted subsystems have no named prefix
4987	126974	"for_each_subsys() allows you to iterate on each subsystem attached to
an active hierarchy"
4991	127142	for_each_root() allows you to iterate across the active hierarchies
4996	127312	"the list of cgroups eligible for automatic release. Protected by
release_list_lock"
5003	127629	Link structure for associating css_set objects with cgroups
5008	127762	"List running through cg_cgroup_links associated with a
cgroup, anchored on cgroup->css_sets"
5013	127917	"List running through cg_cgroup_links pointing at a
single css_set object, anchored on css_set->cg_links"
5023	128274	"The default css_set - used by init and its children prior to any
hierarchies being mounted. It contains a pointer to the root state
for each subsystem. Also used to anchor the list of css_sets. Not
reference-counted, to improve performance when child cgroups
haven't been created."
5030	128523	"css_set_lock protects the list of css_set objects, and the
chain of tasks off each css_set. Nests outside task->alloc_lock
due to cgroup_iter_start()"
5037	128840	"We don't maintain the lists running through each css_set to its
task until after the first call to cgroup_iter_start(). This
reduces the fork()/exit() overhead for people who have cgroups
compiled into their kernel but not actually in use"
5052	129594	"When we create or destroy a css_set, the operation simply
takes/releases a reference count on all the cgroups referenced
by subsystems in this css_set. This can end up multiple-counting
some cgroups, but that's OK - the ref-count is just a
busy/not-busy indicator; ensuring that we only count each cgroup
once would require taking a global lock to ensure that no
subsystems moved between hierarchies while we were doing so.

Possible TODO: decide at boot time based on the number of
registered subsystems and the number of CPUs or NUMA nodes whether
it's better for performance to ref-count every subsystem, or to
take a global lock and only add one ref count to each hierarchy."
5056	129648	unlink a css_set from the list and free it
5106	130714	refcounted get/put for css_set objects
5136	131483	"find_existing_css_set() is a helper for
find_css_set(), and checks to see whether an existing
css_set is suitable. This currently walks a linked-list for
simplicity; a later patch will use a hash table for better
performance

oldcg: the cgroup group that we're using before the cgroup
transition

cgrp: the cgroup that we're moving into

template: location in which to build the desired set of subsystem
state objects for the new cgroup group"
5148	131804	"Built the set of subsystem state objects that we want to
see in the new css_set"
5153	131994	"Subsystem is in this hierarchy. So we want
the subsystem state from the new
cgroup"
5157	132137	"Subsystem is not in this hierarchy, so we
don't want to change the subsystem state"
5162	132244	Look through existing cgroup groups to find one to reuse
5168	132405	All subsystems matched
5171	132457	Try the next cgroup group
5175	132549	No existing cgroup group matched
5183	132747	"allocate_cg_links() allocates ""count"" cg_cgroup_link structures
and chains them on tmp through their cgrp_link_list fields. Returns 0 on
success or a negative error"
5225	133733	"find_css_set() takes an existing cgroup group and a
cgroup object, and returns a css_set object that's
equivalent to the old group, but with the given cgroup
substituted into the appropriate hierarchy. Must be called with
cgroup_mutex held"
5238	134056	"First see if we already have a cgroup group that matches
the desired set"
5252	134355	Allocate all the cg_cgroup_link objects that we'll need
5263	134620	"Copy the set of subsystem state objects generated in
find_existing_css_set()"
5267	134762	Add reference counts and links from the new css_set.
5276	135047	"We want to add a link once per cgroup, so we
only do it for the first subsystem in each
hierarchy"
5300	135773	Link this cgroup group into the list
5361	138689	"There is one global cgroup mutex. We also require taking
task_lock() when dereferencing a task's cgroup subsys pointers.
See ""The task_lock() exception"", at the end of this comment.

A task must hold cgroup_mutex to modify cgroups.

Any task can increment and decrement the count field without lock.
So in general, code holding cgroup_mutex can't rely on the count
field not changing. However, if the count goes to zero, then only
attach_task() can increment it again. Because a count of zero
means that no tasks are currently attached, therefore there is no
way a task attached to that cgroup can fork (the other way to
increment the count). So code holding cgroup_mutex can safely
assume that if the count is zero, it will stay zero. Similarly, if
a task holds cgroup_mutex on a cgroup with zero count, it
knows that the cgroup won't be removed, as cgroup_rmdir()
needs that mutex.

The cgroup_common_file_write handler for operations that modify
the cgroup hierarchy holds cgroup_mutex across the entire operation,
single threading all such cgroup modifications across the system.

The fork and exit callbacks cgroup_fork() and cgroup_exit(), don't
(usually) take cgroup_mutex. These are the two most performance
critical pieces of code here. The exception occurs on cgroup_exit(),
when a task in a notify_on_release cgroup exits. Then cgroup_mutex
is taken, and if the cgroup count is zero, a usermode call made
to /sbin/cgroup_release_agent with the name of the cgroup (path
relative to the root of cgroup file system) as the argument.

A cgroup can only be deleted if both its 'count' of using tasks
is zero, and its list of 'children' cgroups is empty. Since all
tasks in the system use _some_ cgroup, and since there is always at
least one task in the system (init, pid == 1), therefore, top_cgroup
always has either children cgroups and/or using tasks. So we don't
need a special hack to ensure that top_cgroup cannot be deleted.

The task_lock() exception

The need for this exception arises from the action of
attach_task(), which overwrites one tasks cgroup pointer with
another. It does so using cgroup_mutexe, however there are
several performance critical places that need to reference
task->cgroup without the expense of grabbing a system global
mutex. Therefore except as noted below, when dereferencing or, as
in attach_task(), modifying a task'ss cgroup pointer we use
task_lock(), which acts on a spinlock (task->alloc_lock) already in
the task_struct routinely used for such matters.

P.S. One more locking exception. RCU is used to guard the
update of a tasks cgroup pointer by attach_task()"
5366	138760	cgroup_lock - lock out any changes to cgroup structures
5377	138935	"cgroup_unlock - release lock on cgroup changes

Undo the lock taken in a previous cgroup_lock() call."
5389	139230	"A couple of forward declarations required, due to cyclic reference loop:
cgroup_mkdir -> cgroup_create -> cgroup_populate_dir ->
cgroup_add_file -> cgroup_create_file -> cgroup_dir_inode_operations
-> cgroup_mkdir."
5418	140212	is dentry a directory ? if so, kfree() associated cgroup
5427	140608	"It's possible for external users to be holding css
reference counts on a cgroup; css_put() needs to
be able to access the cgroup after decrementing
the reference count in order to know if it needs to
queue the cgroup to be handled by the release
agent"
5455	141249	"This should never be called on a cgroup
directory with child cgroups"
5471	141561	NOTE : the dentry must have been dget()'ed
5491	142122	Check that any added subsystems are currently free
5498	142346	Subsystem isn't free
5506	142576	"Currently we don't handle adding/removing subsystems when
any child cgroups exist. This is theoretically supportable
but involves complex error handling, so it's being left until
later"
5510	142659	Process each subsystem
5515	142858	We're binding this subsystem to this hierarchy
5527	143250	We're removing this subsystem
5537	143617	Subsystem state should already exist
5540	143698	Subsystem state shouldn't exist
5573	144480	"Convert a hierarchy specifier into a bitmask of subsystems and
flags."
5591	145028	Specifying two release agents is forbidden
5614	145596	We can't have an empty hierarchy
5631	145974	See what subsystems are wanted
5636	146094	Don't allow flags to change at remount
5644	146255	(re)populate subsystem files
5684	147289	First check subsystems
5688	147373	Next check flags
5727	148188	"directories start off with i_nlink == 2 (for ""."" entry)"
5749	148683	First find the desired set of subsystems
5777	149289	Reusing an existing superblock
5782	149382	New superblock
5802	149902	"We're accessing css_set_count without locking
css_set_lock here, but that's OK - it can only be
increased by someone holding cgroup_lock, and
that's us. The worst that can happen is that we
have some link structures left over"
5817	150281	EBUSY should be the only error here
5827	150517	"Link the top cgroup in this hierarchy into all
the css_set objects"
5880	151838	Rebind all subsystems back to the default hierarchy
5882	151910	Shouldn't be able to fail ...
5888	152010	"Release all the links from css_sets to this hierarchy's
root cgroup"
5929	152898	"Called with cgroup_mutex held. Writes path of cgroup into buf.
Returns 0 on success, -errno on error."
5938	153084	"Inactive subsystems have no dentry for their root
cgroup"
5967	153613	"Return the first subsystem attached to a cgroup's hierarchy, and
its subsystem id."
5990	154199	"Attach task 'tsk' to cgroup 'cgrp'

Call holding cgroup_mutex. May take task_lock of
the task 'pid' during call."
6003	154561	Nothing to do if the task is already in that cgroup
6020	154880	"Locate or allocate a new css_set for this task,
based on its final set of cgroups"
6035	155182	Update the css_set linked lists if we're using them
6057	155648	"Attach task with pid 'pid' to cgroup 'cgrp'. Call with
cgroup_mutex, may take task_lock of task"
6092	156351	The various types of files and directories in a cgroup file system
6120	156904	nul-terminate
6122	156939	strip newline if necessary
6129	157102	Pass to subsystem
6149	157543	+1 for nul-terminator
6158	157751	nul-terminate
6181	158282	Strip trailing newline
6189	158562	"We never write anything other than '\0'
into the last char of release_agent_path,
so it always remains a NUL-terminated
string"
6327	161515	cgroup_rename - Only allow simple rename of directories in place.
6377	162740	"start off with i_nlink == 2 (for ""."" entry)"
6381	162872	"start with the directory inode held, so that we can
populate it without racing with another mkdir"
6389	163139	Extra count - pin the dentry in core
6400	163430	"cgroup_create_dir - create a directory for an object.
cgrp: the cgroup we create the directory for.
It must have a valid ->parent field
And we are going to fill its ->dentry field.
dentry: dentry of the new cgroup
mode: mode to set on new directory."
6461	164832	Count the number of tasks in a cgroup.
6483	165315	"Advance a list_head iterator. The iterator should be positioned at
the start of a css_set"
6491	165534	Advance to the next non-empty css_set
6511	166027	"The first time anyone tries to iterate across a cgroup,
we need to enable the list linking each css_set to its
tasks, and fix up all existing tasks."
6535	166647	If the iterator cg is NULL, we have no tasks
6539	166775	Advance iterator to find next entry
6543	166910	"We reached the end of this task list - move on to
the next cg_cgroup_link"
6569	167716	"Stuff for reading the 'tasks' file.

Reading this file can return large amounts of data if a cgroup has
*lots* of attached tasks. So it may need several calls to read(),
but we cannot guarantee that the information we produce is correct
unless we produce it entirely atomically.

Upon tasks file open(), a struct ctr_struct is allocated, that
will have a pointer to an array (also allocated here). The struct
ctr_struct * is stored in file->private_data. Its resources will
be freed by release() when the file is closed. The array is used
to sprintf the PIDs and then used by read()."
6581	168045	"Load into 'pidarray' up to 'npids' of the tasks using cgroup
'cgrp'. Return actual number of pids loaded. No need to
task_lock(p) when reading out p->cgroup, since we're in an RCU
read section, so the css_set can't go away, and is
immutable after creation."
6604	168612	"Build and fill cgroupstats so that taskstats can export it to user
space.

@stats: cgroupstats to fill information into
@dentry: A dentry entry belonging to the cgroup for which stats have
been requested."
6613	168845	Validate dentry by checking the superblock operations
6658	169784	"Convert array 'a' of 'npids' pid_t's to a string of newline separated
decimal pids in 'buf'. Don't write more than 'sz' chars, but return
count 'cnt' of how many chars would be written if buf were large enough."
6674	170203	"Handle an open on 'tasks' file. Prepare a buffer listing the
process id's of tasks currently attached to the cgroup being opened.

Does not require any specific cgroup mutexes, and does not take any."
6695	170755	"If cgroup gets more users after we read count, we won't have
enough space - tough. This race is indistinguishable to the
caller from the case that the additional cgroup users didn't
show up until sometime later on."
6705	171059	Call pid_array_to_buf() twice, first just to get bufsz
6764	172293	for the common functions, 'private' gives the type of file
6801	173082	First clear out any existing files
6841	174039	"cgroup_create - create a cgroup
parent: cgroup that will be parent of the new cgroup.
name: name of the new cgroup. Will be strcpy'ed.
mode: mode to set on new inode

Must be called with the mutex on the parent inode held"
6860	174603	"Grab a reference on the superblock so the hierarchy doesn't
get deleted on unmount if there are child cgroups. This
can be done outside cgroup_mutex, since the sb can't
disappear while someone has an open control file on the
fs"
6891	175317	The cgroup directory was pre-locked for us
6895	175476	If err < 0, we have a half-filled directory - oh well ;)
6916	175844	Release the reference count that we took on the superblock
6927	176076	the vfs holds inode->i_mutex already
6941	176697	"Check the reference count on each subsystem. Since we
already established that there are no tasks in the
cgroup, if the css refcount is also 0, then there should
be no outstanding references, so the subsystem is safe to
destroy. We scan across all subsystems rather than using
the per-hierarchy linked list of mounted subsystems since
we can be called via check_for_release() with no
synchronization other than RCU, and the subsystem linked
list isn't RCU-safe"
6946	176871	Skip subsystems not in this hierarchy
6955	177261	"When called from check_for_release() it's possible
that by this point the cgroup has been removed
and the css deleted. But a false-positive doesn't
matter, since it can only happen if the cgroup
has been deleted and hence no longer needs the
release agent to be called anyway."
6972	177627	the vfs holds both inode->i_mutex already
7003	178299	delete my sibling from parent->children
7019	178689	"Drop the active superblock reference that we took when we
created the cgroup"
7031	178961	Create the top cgroup state for this subsystem
7034	179066	We don't handle early failures gracefully
7041	179328	"Update all cgroup groups to contain a subsys
pointer to this state - since the subsystem is
newly registered, all tasks and hence all cgroup
groups are in the subsystem's top cgroup."
7054	179737	"If this subsystem requested that it be notified with fork
events, we should send it one now for every process in the
system"
7073	180116	"cgroup_init_early - initialize cgroups at system boot, and
initialize any subsystems that request early init."
7116	181210	"cgroup_init - register cgroup filesystem and /proc file, and
initialize any subsystems that didn't request early init."
7158	182271	"proc_cgroup_show()
- Print task's cgroup paths into seq_file, one line for each hierarchy
- Used for /proc/<pid>/cgroup.
- No need to task_lock(tsk) on this tsk->cgroup reference, as it
doesn't really matter if tsk->cgroup changes after we read it,
and we take cgroup_mutex, keeping attach_task() from changing it
anyway. No need to check that tsk->cgroup != NULL, thanks to
the_top_cgroup_hack in cgroup_exit(), which sets an exiting tasks
cgroup to top_cgroup."
7160	182315	TODO: Use a proper seq_file iterator
7190	182871	Skip this hierarchy if it has no active subsystems
7227	183728	Display information about each subsystem and each hierarchy
7271	185138	"cgroup_fork - attach newly forked task to its parents cgroup.
@tsk: pointer to task_struct of forking parent process.

Description: A task inherits its parent's cgroup at fork().

A pointer to the shared css_set was automatically copied in
fork.c by dup_task_struct(). However, we ignore that copy, since
it was not made under the protection of RCU or cgroup_mutex, so
might no longer be a valid cgroup pointer. attach_task() might
have already changed current->cgroups, allowing the previously
referenced cgroup group to be removed and freed.

At the point that cgroup_fork() is called, 'current' is the parent
task, and the passed argument 'child' points to the child task."
7285	185506	"cgroup_fork_callbacks - called on a new task very soon before
adding it to the tasklist. No need to take any locks since no-one
can be operating on this task"
7303	186071	"cgroup_post_fork - called on a new task after adding it to the
task list. Adds the task to the list running through its css_set
if necessary. Has to be after the task is visible on the task list
in case we race with the first call to cgroup_iter_start() - to
guarantee that the new task ends up on its list."
7347	187864	"cgroup_exit - detach cgroup from exiting task
@tsk: pointer to task_struct of exiting process

Description: Detach cgroup from @tsk and release it.

Note that cgroups marked notify_on_release force every task in
them to take the global cgroup_mutex mutex when exiting.
This could impact scaling on very large systems. Be reluctant to
use notify_on_release cgroups where very high task exit scaling
is required on large systems.

the_top_cgroup_hack:

Set the exiting tasks cgroup to the root cgroup (top_cgroup).

We call cgroup_exit() while the task is still competent to
handle notify_on_release(), then leave the task attached to the
root cgroup in each hierarchy for the remainder of its exit.

To do this properly, we would increment the reference count on
top_cgroup, and near the very end of the kernel/exit.c do_exit()
code we would add a second cgroup function call, to drop that
reference. This would just create an unnecessary hot spot on
the top_cgroup reference count, to no avail.

Normally, holding a reference to a cgroup without bumping its
count is unsafe. The cgroup could go away, or someone could
attach us to a different cgroup, decrementing the count on
the first cgroup that we never incremented. But in this case,
top_cgroup isn't going away, and either task has PF_EXITING set,
which wards off any attach_task() attempts, or task is a failed
fork, never visible to attach_task."
7365	188265	"Unlink from the css_set task list if necessary.
Optimistically check cg_list before taking
css_set_lock"
7373	188472	Reassign the task to the init_css_set.
7386	188756	"cgroup_clone - duplicate the current cgroup in the hierarchy
that the given subsystem is attached to, and move this task into
the new child"
7398	189100	We shouldn't be called by an unregistered subsystem
7402	189242	"First figure out what hierarchy and cgroup we're dealing
with, and pin them so we can drop cgroup_mutex"
7418	189633	Pin the hierarchy
7421	189705	Keep the cgroup alive
7425	189800	Now do the VFS work to create a cgroup
7429	189944	"Hold the parent directory mutex across this operation to
stop anyone else deleting the new cgroup"
7440	190297	Create the cgroup directory, which also creates the cgroup
7460	190769	"The cgroup now exists. Retake cgroup_mutex and check
that we're still in the same state that we thought we
were."
7464	190912	Aargh, we raced ...
7471	191120	"The cgroup is still accessible in the VFS, but
we're not going to try to rmdir() it at this
point."
7478	191268	do any required auto-setup
7484	191422	All seems fine. Finish by moving the task into the new cgroup
7506	191913	"See if ""cgrp"" is a descendant of the current task's cgroup in
the appropriate hierarchy

If we are sending in dummytop, then presumably we are creating
the top cgroup in the subsystem.

Called only by the ns (nsproxy) cgroup."
7527	192386	"All of these checks rely on RCU to keep the cgroup
structure alive"
7532	192644	"Control Group is currently removeable. If it's not
already queued for a userspace notification, queue
it now"
7580	194451	"Notify userspace when a cgroup is released, by running the
configured release agent with the name of the cgroup (path
relative to the root of cgroup file system) as the argument.

Most likely, this user command will try to rmdir this cgroup.

This races with the possibility that some other task will be
attached to this cgroup before it is removed, or that some other
user task will 'mkdir' a child cgroup of this cgroup. That's ok.
The presumed 'rmdir' will fail quietly if this cgroup is no longer
unused, and this cgroup will be reprieved from its death sentence,
to continue to serve a useful existence. Next time it's released,
we will get notified again, if it still has 'notify_on_release' set.

The final arg to call_usermodehelper() is UMH_WAIT_EXEC, which
means only wait until the task is successfully execve()'d. The
separate release agent task is forked by call_usermodehelper(),
then control in this thread returns here, without waiting for the
release agent task. We don't bother to wait because the caller of
this routine has no use for the exit status of the release agent
task, so no sense holding our caller up for that."
7614	195265	minimal command environment
7621	195499	"Drop the lock while we invoke the usermode helper,
since the exec could involve hitting disk and hence
be a slow process"
7651	196431	"kernel/cpuset.c

Processor and Memory placement constraints for sets of tasks.





Portions derived from Patrick Mochel's sysfs code.
sysfs is Copyright (c) 2001-3 Patrick Mochel

2003-10-10 Written by Simon Derr.
2003-10-22 Updates by Stephen Hemminger.
2004 May-July Rework by Paul Jackson.
2006 Rework by Paul Menage to use generic cgroups

This file is subject to the terms and conditions of the GNU General Public
License. See the file COPYING in the main directory of the Linux
distribution for more details."
7694	197484	"Tracks how many cpusets are currently defined in system.
When there is only one cpuset (the root cpuset) we can
short circuit some hooks."
7697	197562	Retrieve the cpuset from a cgroup
7701	197659	"See ""Frequency meter"" comments, below."
7707	197718	"unprocessed events count
most recent output value
clock (secs) when val computed
guards read or write of above"
7715	197980	"""unsigned long"" so bitops work
CPUs allowed to tasks in cpuset
Memory Nodes allowed to tasks"
7717	198148	my parent
7722	198269	"Copy of global cpuset_mems_generation as of the most
recent time this cpuset changed its mems_allowed."
7725	198345	memory_pressure filter
7727	198398	partition number for rebuild_sched_domains()
7731	198450	Retrieve the cpuset for a cgroup
7738	198646	Retrieve the cpuset for a task
7746	198847	bits in struct cpuset flags field
7756	199038	convenient tests for these bits
7805	200588	"Increment this integer everytime any cpuset changes its
mems_allowed value. Users of cpusets can track this generation
number, and avoid having to lock and reload mems_allowed unless
the cpuset they're using changes generation.

A single, global generation is needed because attach_task() could
reattach a task to a different cpuset, which must not have its
generation numbers aliased with those of that tasks previous cpuset.

Generations are needed for mems_allowed because one task cannot
modify anothers memory placement. So we must enable every task,
on every visit to __alloc_pages(), to efficiently check whether
its current->cpuset->mems_allowed has changed, requiring an update
of its current->mems_allowed.

Since cpuset_mems_generation is guarded by manage_mutex,
there is no need to mark it atomic."
7895	205106	"We have two global cpuset mutexes below. They can nest.
It is ok to first take manage_mutex, then nest callback_mutex. We also
require taking task_lock() when dereferencing a tasks cpuset pointer.
See ""The task_lock() exception"", at the end of this comment.

A task must hold both mutexes to modify cpusets. If a task
holds manage_mutex, then it blocks others wanting that mutex,
ensuring that it is the only task able to also acquire callback_mutex
and be able to modify cpusets. It can perform various checks on
the cpuset structure first, knowing nothing will change. It can
also allocate memory while just holding manage_mutex. While it is
performing these checks, various callback routines can briefly
acquire callback_mutex to query cpusets. Once it is ready to make
the changes, it takes callback_mutex, blocking everyone else.

Calls to the kernel memory allocator can not be made while holding
callback_mutex, as that would risk double tripping on callback_mutex
from one of the callbacks into the cpuset code from within
__alloc_pages().

If a task is only holding callback_mutex, then it has read-only
access to cpusets.

The task_struct fields mems_allowed and mems_generation may only
be accessed in the context of that task, so require no locks.

Any task can increment and decrement the count field without lock.
So in general, code holding manage_mutex or callback_mutex can't rely
on the count field not changing. However, if the count goes to
zero, then only attach_task(), which holds both mutexes, can
increment it again. Because a count of zero means that no tasks
are currently attached, therefore there is no way a task attached
to that cpuset can fork (the other way to increment the count).
So code holding manage_mutex or callback_mutex can safely assume that
if the count is zero, it will stay zero. Similarly, if a task
holds manage_mutex or callback_mutex on a cpuset with zero count, it
knows that the cpuset won't be removed, as cpuset_rmdir() needs
both of those mutexes.

The cpuset_common_file_write handler for operations that modify
the cpuset hierarchy holds manage_mutex across the entire operation,
single threading all such cpuset modifications across the system.

The cpuset_common_file_read() handlers only hold callback_mutex across
small pieces of code, such as when reading out possibly multi-word
cpumasks and nodemasks.

The fork and exit callbacks cpuset_fork() and cpuset_exit(), don't
(usually) take either mutex. These are the two most performance
critical pieces of code here. The exception occurs on cpuset_exit(),
when a task in a notify_on_release cpuset exits. Then manage_mutex
is taken, and if the cpuset count is zero, a usermode call made
to /sbin/cpuset_release_agent with the name of the cpuset (path
relative to the root of cpuset file system) as the argument.

A cpuset can only be deleted if both its 'count' of using tasks
is zero, and its list of 'children' cpusets is empty. Since all
tasks in the system use _some_ cpuset, and since there is always at
least one task in the system (init), therefore, top_cpuset
always has either children cpusets and/or using tasks. So we don't
need a special hack to ensure that top_cpuset cannot be deleted.

The above ""Tale of Two Semaphores"" would be complete, but for:

The task_lock() exception

The need for this exception arises from the action of attach_task(),
which overwrites one tasks cpuset pointer with another. It does
so using both mutexes, however there are several performance
critical places that need to reference task->cpuset without the
expense of grabbing a system global mutex. Therefore except as
noted below, when dereferencing or, as in attach_task(), modifying
a tasks cpuset pointer we use task_lock(), which acts on a spinlock
(task->alloc_lock) already in the task_struct routinely used for
such matters.

P.S. One more locking exception. RCU is used to guard the
update of a tasks cpuset pointer by attach_task() and the
access of task->cpuset->mems_generation via that pointer in
the routine cpuset_update_task_memory_state()."
7901	205329	"This is ugly, but preserves the userspace API for existing cpuset
users. If someone tries to mount the ""cpuset"" filesystem, we
silently switch it to mount ""cgroup"" instead"
7936	206392	"Return in *pmask the portion of a cpusets's cpus_allowed that
are online. If none are online, walk up the cpuset hierarchy
until we find one that does have some online cpus. If we get
all the way to the top and still haven't found any online cpus,
return cpu_online_map. Or if passed a NULL cs from an exit'ing
task, return cpu_online_map.

One way or another, we guarantee to return some non-empty subset
of cpu_online_map.

Call with callback_mutex held."
7960	207181	"Return in *pmask the portion of a cpusets's mems_allowed that
are online, with memory. If none are online with memory, walk
up the cpuset hierarchy until we find one that does have some
online mems. If we get all the way to the top and still haven't
found any online mems, return node_states[N_HIGH_MEMORY].

One way or another, we guarantee to return some non-empty subset
of node_states[N_HIGH_MEMORY].

Call with callback_mutex held."
8015	209517	"cpuset_update_task_memory_state - update task memory placement

If the current tasks cpusets mems_allowed changed behind our
backs, update current->mems_allowed, mems_generation and task NUMA
mempolicy to the new value.

Task mempolicy is updated by rebinding it relative to the
current->cpuset if a task has its memory placement changed.
Do not call this routine if in_interrupt().

Call without callback_mutex or task_lock() held. May be
called with or without manage_mutex held. Thanks in part to
'the_top_cpuset_hack', the tasks cpuset pointer will never
be NULL. This routine also might acquire callback_mutex and
current->mm->mmap_sem during call.

Reading current->cpuset->mems_generation doesn't need task_lock
to guard the current->cpuset derefence, because it is guarded
from concurrent freeing of current->cpuset by attach_task(),
using RCU.

The rcu_dereference() is technically probably not needed,
as I don't actually mind if I see a new cpuset pointer but
an old value of mems_generation. However this really only
matters on alpha systems using cpusets heavily. If I dropped
that rcu_dereference(), it would save them a memory barrier.
For all other arch's, rcu_dereference is a no-op anyway, and for
alpha systems not using cpusets, another planned optimization,
avoiding the rcu critical section for tasks in the root cpuset
which is statically allocated, so can't vanish, will make this
irrelevant. Better to use RCU as intended, than to engage in
some cute trick to save a memory barrier that is impossible to
test, for alpha systems using cpusets heavily, which might not
even exist.

This routine is needed to update the per-task mems_allowed data,
within the tasks context, when it is trying to allocate memory
(in various mm/mempolicy.c routines) and notices that some other
task has been modifying its cpuset."
8024	209739	Don't need rcu for top_cpuset. It's never freed.
8035	210071	Maybe changed when task not locked
8058	210749	"is_cpuset_subset(p, q) - Is cpuset p a subset of cpuset q?

One cpuset is a subset of another if all its allowed CPUs and
Memory Nodes are a subset of the other, and its exclusive flags
are only set if the other's are set. Call holding manage_mutex."
8086	211786	"validate_change() - Used to validate that any proposed cpuset change
follows the structural rules for cpusets.

If we replaced the flag and mask values of the current cpuset
(cur) with those values in the trial cpuset (trial), would
our various subset and exclusive rules still be valid? Presumes
manage_mutex held.

'cur' is the address of an actual, in-use cpuset. Operations
such as list traversal that depend on the actual address of the
cpuset in the list must use cur below, not trial.

'trial' is the address of bulk structure copy of cur, with
perhaps one or more of the fields cpus_allowed, mems_allowed,
or flags changed to new, trial values.

Return 0 if valid, -errno if not."
8093	211974	Each of our child cpusets must be a subset of us
8099	212162	Remaining checks don't apply to root cpuset
8105	212269	We must be a subset of our parent cpuset
8109	212398	If either I or some sibling (!= me) is exclusive, we can't overlap
8122	212876	Cpusets with tasks can't have empty cpus_allowed or mems_allowed
8136	213153	"Helper routine for rebuild_sched_domains().
Do cpusets a, b have overlapping cpus_allowed masks?"
8208	216647	"rebuild_sched_domains()

If the flag 'sched_load_balance' of any cpuset with non-empty
'cpus' changes, or if the 'cpus' allowed changes in any cpuset
which has that flag enabled, or if any cpuset with a non-empty
'cpus' is removed, then call this routine to rebuild the
scheduler's dynamic sched domains.

This routine builds a partial partition of the systems CPUs
(the set of non-overlappping cpumask_t's in the array 'part'
below), and passes that partial partition to the kernel/sched.c
partition_sched_domains() routine, which will rebuild the
schedulers load balancing domains (sched domains) as specified
by that partial partition. A 'partial partition' is a set of
non-overlapping subsets whose union is a subset of that set.

See ""What is sched_load_balance"" in Documentation/cpusets.txt
for a background explanation of this.

Does not return errors, on the theory that the callers of this
routine would rather not worry about failures to rebuild sched
domains when operating in the severe memory shortage situations
that could cause allocation failures below.

Call with cgroup_mutex held. May take callback_mutex during
call due to the kfifo_alloc() and kmalloc() calls. May nest
a call to the lock_cpu_hotplug()/unlock_cpu_hotplug() pair.
Must not be called holding callback_mutex, because we must not
call lock_cpu_hotplug() while holding callback_mutex. Elsewhere
the kernel nests callback_mutex inside lock_cpu_hotplug() calls.
So the reverse nesting would risk an ABBA deadlock.

The three key local variables below are:
q - a kfifo queue of cpuset pointers, used to implement a
top-down scan of all cpusets. This scan loads a pointer
to each cpuset marked is_sched_load_balance into the
array 'csa'. For our purposes, rebuilding the schedulers
sched domains, we can ignore !is_sched_load_balance cpusets.
csa - (for CpuSet Array) Array of pointers to all the cpusets
that need to be load balanced, for convenient iterative
access by the subsequent code that finds the best partition,
i.e the set of domains (subsets) of CPUs such that the
cpus_allowed of every cpuset marked is_sched_load_balance
is a subset of one of these domains, while there are as
many such domains as possible, each as small as possible.
doms - Conversion of 'csa' to an array of cpumasks, for passing to
the kernel/sched.c routine partition_sched_domains() in a
convenient format, that can be easily compared to the prior
value to determine what partition elements (sched domains)
were changed (added or removed.)

Finding the best partition (set of domains):
The triple nested loops below over i, j, k scan over the
load balanced cpusets (using the array of cpuset pointers in
csa[]) looking for pairs of cpusets that have overlapping
cpus_allowed, but which don't have the same 'pn' partition
number and gives them in the same partition number. It keeps
looping on the 'restart' label until it can no longer find
any such pairs.

The union of the cpus_allowed masks from the set of
all cpusets having the same 'pn' value then form the one
element of the partition (one sched domain) to be passed to
partition_sched_domains()."
8219	216745	"queue of cpusets to be scanned
scans q
array of all cpuset ptrs
how many cpuset ptrs in csa so far
indices for partition finding loops
resulting partition; i.e. sched domains
number of sched domains in result
next empty doms[] cpumask_t slot"
8225	217222	Special case for the 99% of systems with one, full, sched domain
8247	217797	scans child cpusets of cp
8261	218124	Find the best partition (set of sched domains)
8277	218478	one less element
8283	218552	Convert <csn, csa> to <ndoms, doms>
8323	219334	Have scheduler rebuild sched domains
8332	219541	Don't kfree(doms) -- partition_sched_domains() does that.
8353	220162	"Arbitrarily, if two processes started at the same
time, we'll say that the lower pointer value
started first. Note that t2 may have exited by now
so this may not be a valid pointer any longer, but
that's fine - it still serves to distinguish
between two tasks started (effectively)
simultaneously."
8367	220433	Call with manage_mutex held. May take callback_mutex during call.
8377	220716	Never dereference latest_task, since it's not refcounted
8382	220891	top_cpuset.cpus_allowed tracks cpu_online_map; it's read-only
8393	221189	"An empty cpus_allowed is ok iff there are no tasks in the cpuset.
Since cpulist_parse() fails on an empty mask, we special case
that parsing. The validate_change() call ensures that cpusets
with tasks have cpus."
8407	221557	Nothing to do if the cpus didn't change
8430	222370	"Scan tasks in the cpuset, and update the cpumasks of any
that need an update. Since we can't call set_cpus_allowed()
while holding tasklist_lock, gather tasks to be processed
in a heap structure. If the statically-sized heap fills up,
overflow tasks that started later, and in future iterations
only consider tasks that started after the latest task in
the previous pass. This guarantees forward progress and
that we don't miss any tasks"
8434	222527	Only affect tasks that don't have the right cpus_allowed
8440	222679	"Only process tasks that started after the last task
we processed"
8469	223487	"If we had to process any tasks at all, scan again
in case some of them were in the middle of forking
children that didn't notice the new cpumask
restriction. Not the most efficient way to do it,
but it avoids having to take callback_mutex in the
fork path"
8508	224907	"cpuset_migrate_mm

Migrate memory region from one set of nodes to another.

Temporarilly set tasks mems_allowed to target nodes of migration,
so that the migration code can allocate pages on these nodes.

Call holding manage_mutex, so our current->cpuset won't change
during this call, as manage_mutex holds off any attach_task()
calls. Therefore we don't need to take task_lock around the
call to guarantee_online_mems(), as we know no one is changing
our tasks cpuset.

Hold callback_mutex around the two modifications of our tasks
mems_allowed to synchronize with cpuset_mems_allowed().

While the mm_struct we are migrating is typically from some
other task, the task_struct mems_allowed that we are hacking
is for our current task, which must allocate new pages for that
migrating memory region.

We call cpuset_update_task_memory_state() before hacking
our tasks mems_allowed, so that we are assured of being in
sync with our tasks cpuset, and in particular, callbacks to
cpuset_update_task_memory_state() from nested page allocations
won't see any mismatch of our cpuset and task mems_generation
values, so won't overwrite our hacked tasks mems_allowed
nodemask."
8540	225939	"Handle user request to change the 'mems' memory placement
of a cpuset. Needs to validate the request, update the
cpusets mems_allowed and mems_generation, and for each
task in the cpuset, rebind any vma mempolicies and if
the cpuset is marked 'memory_migrate', migrate the tasks
pages to the new memory.

Call with manage_mutex held. May take callback_mutex during call.
Will take tasklist_lock, scan tasklist for tasks in cpuset cs,
lock each such tasks mm->mmap_sem, scan its vma's and rebind
their mempolicies to the cpusets new mems_allowed."
8559	226305	"top_cpuset.mems_allowed tracks node_stats[N_HIGH_MEMORY];
it's read-only"
8570	226606	"An empty mems_allowed is ok iff there are no tasks in the cpuset.
Since nodelist_parse() fails on an empty mask, we special case
that parsing. The validate_change() call ensures that cpusets
with tasks have memory."
8583	226992	Too easy - nothing to do
8595	227295	causes mpol_copy() rebind
8598	227340	"spare mmarray[] slots
imagine one fork-bomb/cpu"
8607	227717	"Allocate mmarray[] to hold mm reference for each task
in cpuset cs. Can't kmalloc GFP_KERNEL while holding
tasklist_lock. We could use GFP_ATOMIC, but with a
few more lines of code, we can retry until we get a big
enough mmarray[] w/o using GFP_ATOMIC."
8609	227789	guess
8614	227945	block fork
8617	228026	"got enough
try again"
8623	228173	Load up mmarray[] with mm reference for each task in cpuset.
8653	229240	"Now that we've dropped the tasklist spinlock, we can
rebind the vma mempolicies of each mm in mmarray[] to their
new cpuset, and release that mm. The mpol_rebind_mm()
call takes mmap_sem, which we couldn't take while holding
tasklist_lock. Forks can happen again now - the mpol_copy()
cpuset_being_rebound check will catch such forks, and rebind
their vma mempolicies too. Because we still hold the global
cpuset manage_mutex, we know that no other rebind effort will
be contending for the global variable cpuset_being_rebound.
It's ok if we rebind the same mm twice; mpol_rebind_mm()
is idempotent. Also migrate pages in each mm to new nodes."
8664	229534	We're done rebinding vma's to this cpusets new mems_allowed.
8679	229755	Call with manage_mutex held.
8700	230328	"update_flag - read a 0 or a 1 in a file and update associated flag
bit: the bit to update (CS_CPU_EXCLUSIVE, CS_MEM_EXCLUSIVE,
CS_SCHED_LOAD_BALANCE,
CS_NOTIFY_ON_RELEASE, CS_MEMORY_MIGRATE,
CS_SPREAD_PAGE, CS_SPREAD_SLAB)
cs: the cpuset to update
buf: the buffer where we read the 0 or 1

Call with manage_mutex held."
8778	233239	"Frequency meter - How fast is some event occurring?

These routines manage a digitally filtered, constant time based,
event frequency meter. There are four routines:
fmeter_init() - initialize a frequency meter.
fmeter_markevent() - called each time the event happens.
fmeter_getrate() - returns the recent rate of such events.
fmeter_update() - internal routine used to update fmeter.

A common data structure is passed to each of these routines,
which is used to keep track of the state required to manage the
frequency meter and its digital filter.

The filter works on the number of events marked per unit time.
The filter is single-pole low-pass recursive (IIR). The time unit
is 1 second. Arithmetic is done using 32-bit integers scaled to
simulate 3 decimal digits of precision (multiplied by 1000).

With an FM_COEF of 933, and a time base of 1 second, the filter
has a half-life of 10 seconds, meaning that if the events quit
happening, then the rate returned from the fmeter_getrate()
will be cut in half each 10 seconds, until it converges to zero.

It is not worth doing a real infinitely recursive filter. If more
than FM_MAXTICKS ticks have elapsed since the last filter event,
just compute FM_MAXTICKS ticks worth, by which point the level
will be stable.

Limit the count of unprocessed events to FM_MAXCNT, so as to avoid
arithmetic overflow in the fmeter_update() routine.

Given the simple 32 bit integer arithmetic used, this meter works
best for reporting rates between one per millisecond (msec) and
one per 32 (approx) seconds. At constant rates faster than one
per msec it maxes out at values just under 1,000,000. At constant
rates between one per msec, and one per second it will stabilize
to a value N*1000, where N is the rate of events per second.
At constant rates between one per second and one per 32 seconds,
it will be choppy, moving up on the seconds that have an event,
and then decaying until the next event. At rates slower than
about one in 32 seconds, it decays all the way back to zero between
each event."
8783	233304	"coefficient for half-life of 10 secs
useless computing more ticks than this
limit cnt to avoid overflow
faux fixed point scale"
8785	233530	Initialize a frequency meter
8794	233720	Internal meter update - process cnt events and update value
8812	234126	Process any previous ticks, then bump cnt by one (times scale).
8821	234361	Process any previous ticks, then return current value.
8871	235496	The various types of files and directories in a cpuset file system
8897	236116	Crude upper limit on largest legitimate cpulist user might write.
8901	236216	+1 for nul-terminator
8909	236414	nul-terminate
8975	238176	"These ascii lists should be read in a single call, by using a user
buffer large enough to hold the entire map. If read in smaller
chunks, there is no guarantee of atomicity. Since the display format
used, list of ranges of sequential numbers, is variable length,
and since these maps can change value dynamically, one could read
gibberish by doing partial reads while a list was changing.
A single large read to a buffer that crosses a page boundary is
ok, because the result being copied to user land is not recomputed
across a page fault."
9065	240106	for the common functions, 'private' gives the type of file
9159	242694	memory_pressure_enabled is in root cpuset only
9181	243465	"post_clone() is called at the end of cgroup_clone().
'cgroup' was just created automatically as a result of
a cgroup_clone(), and the current task is about to
be moved into 'cgroup'.

Currently we refuse to set up the cgroup - thereby
refusing the task to be entered, and as a result refusing
the sys_unshare() or clone() which initiated it - if any
sibling cpusets have exclusive cpus or mem.

If this becomes a problem for some users who wish to
allow that scenario, then cpuset_post_clone() could be
changed to grant parent->cpus_allowed-sibling_cpus_exclusive
(and likewise for mems) to the new cgroup."
9209	244203	"cpuset_create - create a cpuset
parent: cpuset that will be parent of the new cpuset.
name: name of the new cpuset. Will be strcpy'ed.
mode: mode to set on new inode

Must be called with the mutex on the parent inode held"
9219	244428	This is early initialization for the top cgroup
9255	245552	"Locking note on the strange update_flag() call below:

If the cpuset being removed has its flag 'sched_load_balance'
enabled, then simulate turning sched_load_balance off, which
will call rebuild_sched_domains(). The lock_cpu_hotplug()
call in rebuild_sched_domains() must not be made while holding
callback_mutex. Elsewhere the kernel nests callback_mutex inside
lock_cpu_hotplug() calls. So the reverse nesting would risk an
ABBA deadlock."
9286	246249	"cpuset_init_early - just enough so that the calls to
cpuset_update_task_memory_state() in early init code
are harmless."
9299	246496	"cpuset_init - initialize cpusets at system boot

Description: Initialize top_cpuset and the cpuset internal file system,"
9339	247829	"If common_cpu_mem_hotplug_unplug(), below, unplugs any CPUs
or memory nodes, we need to walk over the cpuset hierarchy,
removing that CPU or node from all cpusets. If this removes the
last CPU or node from a cpuset, then the guarantee_online_cpus()
or guarantee_online_mems() code will use that emptied cpusets
parent online CPUs or nodes. Cpusets that were already empty of
CPUs or nodes are left empty.

This routine is intentionally inefficient in a couple of regards.
It will check all cpusets in a subtree even if the top cpuset of
the subtree has no offline CPUs or nodes. It checks both CPUs and
nodes, even though the caller could have been coded to know that
only one of CPUs or nodes needed to be checked on a given call.
This was done to minimize text size rather than cpu cycles.

Call with both manage_mutex and callback_mutex held.

Recursive, on depth of cpuset subtree."
9346	248003	Each of our child cpusets mems must be online
9372	249065	"The cpus_allowed and mems_allowed nodemasks in the top_cpuset track
cpu_online_map and node_states[N_HIGH_MEMORY]. Force the top cpuset to
track what's online after any CPU or memory node hotplug or unplug
event.

To ensure that we don't remove a CPU or node from the top cpuset
that is currently in use by a child cpuset (which would violate
the rule that cpusets must be subsets of their parent), we first
call the recursive routine guarantee_online_cpus_mems_in_subtree().

Since there are two callers of this routine, one for CPU hotplug
events and one for memory node hotplug events, we could have coded
two separate routines here. We code it as a single common routine
in order to minimize text size."
9395	249727	"The top_cpuset tracks what CPUs and Memory Nodes are online,
period. This is necessary in order to make cpusets transparent
(of no affect) on systems that are actively using CPU hotplug
but making no active use of cpusets.

This routine ensures that top_cpuset.cpus_allowed tracks
cpu_online_map on each CPU hotplug (cpuhp) event."
9413	250201	"Keep top_cpuset.mems_allowed tracking node_states[N_HIGH_MEMORY].
Call this routine anytime after you change
node_states[N_HIGH_MEMORY].
See also the previous routine cpuset_handle_cpuhp()."
9425	250413	"cpuset_init_smp - initialize cpus_allowed

Description: Finish top cpuset after cpu, node maps are initialized"
9444	250973	"cpuset_cpus_allowed - return cpus_allowed mask from a tasks cpuset.
@tsk: pointer to task_struct from which to obtain cpuset->cpus_allowed.

Description: Returns the cpumask_t cpus_allowed of the cpuset
attached to the specified @tsk. Guaranteed to return some non-empty
subset of cpu_online_map, even if this means going outside the
tasks cpuset."
9460	251302	"cpuset_cpus_allowed_locked - return cpus_allowed mask from a tasks cpuset.
Must be called with callback_mutex held."
9485	251963	"cpuset_mems_allowed - return mems_allowed mask from a tasks cpuset.
@tsk: pointer to task_struct from which to obtain cpuset->mems_allowed.

Description: Returns the nodemask_t mems_allowed of the cpuset
attached to the specified @tsk. Guaranteed to return some non-empty
subset of node_states[N_HIGH_MEMORY], even if this means going outside the
tasks cpuset."
9505	252401	"cpuset_zonelist_valid_mems_allowed - check zonelist vs. curremt mems_allowed
@zl: the zonelist to be checked

Are any of the nodes on zonelist zl allowed in current->mems_allowed?"
9524	252860	"nearest_exclusive_ancestor() - Returns the nearest mem_exclusive
ancestor to the specified cpuset. Call holding callback_mutex.
If no ancestor is mem_exclusive (an unusual configuration), then
returns the root cpuset."
9594	256029	"cpuset_zone_allowed_softwall - Can we allocate on zone z's memory node?
@z: is this zone on an allowed node?
@gfp_mask: memory allocation flags

If we're in interrupt, yes, we can always allocate. If
__GFP_THISNODE is set, yes, we can always allocate. If zone
z's node is in our tasks mems_allowed, yes. If it's not a
__GFP_HARDWALL request and this zone's nodes is in the nearest
mem_exclusive cpuset ancestor to this tasks cpuset, yes.
If the task has been OOM killed and has access to memory reserves
as specified by the TIF_MEMDIE flag, yes.
Otherwise, no.

If __GFP_HARDWALL is set, cpuset_zone_allowed_softwall()
reduces to cpuset_zone_allowed_hardwall(). Otherwise,
cpuset_zone_allowed_softwall() might sleep, and might allow a zone
from an enclosing cpuset.

cpuset_zone_allowed_hardwall() only handles the simpler case of
hardwall cpusets, and never sleeps.

The __GFP_THISNODE placement logic is really handled elsewhere,
by forcibly using a zonelist starting at a specified node, and by
(in get_page_from_freelist()) refusing to consider the zones for
any node on the zonelist except the first. By the time any such
calls get to this routine, we should just shut up and say 'yes'.

GFP_USER allocations are marked with the __GFP_HARDWALL bit,
and do not allow allocations outside the current tasks cpuset
unless the task has been OOM killed as is marked TIF_MEMDIE.
GFP_KERNEL allocations are not so marked, so can escape to the
nearest enclosing mem_exclusive ancestor cpuset.

Scanning up parent cpusets requires callback_mutex. The
__alloc_pages() routine only calls here with __GFP_HARDWALL bit
_not_ set if it's a GFP_KERNEL allocation, and all nodes in the
current tasks mems_allowed came up empty on the first pass over
the zonelist. So only GFP_KERNEL allocations, if all nodes in the
cpuset are short of memory, might require taking the callback_mutex
mutex.

The first call here from mm/page_alloc:get_page_from_freelist()
has __GFP_HARDWALL set in gfp_mask, enforcing hardwall cpusets,
so no allocation on a node outside the cpuset is allowed (unless
in interrupt, of course).

The second pass through get_page_from_freelist() doesn't even call
here for GFP_ATOMIC calls. For those calls, the __alloc_pages()
variable 'wait' is not set, and the bit ALLOC_CPUSET is not set
in alloc_flags. That logic and the checks below have the combined
affect that:
in_interrupt - any node ok (current task context irrelevant)
GFP_ATOMIC - any node ok
TIF_MEMDIE - any node ok
GFP_KERNEL - any node in enclosing mem_exclusive cpuset ok
GFP_USER - only nodes in current tasks mems allowed ok.

Rule:
Don't call cpuset_zone_allowed_softwall if you can't sleep, unless you
pass in the __GFP_HARDWALL flag set in gfp_flag, which disables
the code that might scan up ancestor cpusets and sleep."
9600	256141	"node that zone z is on
current cpuset ancestors
is allocation in zone z allowed?"
9611	256570	"Allow tasks that have access to memory reserves because they have
been OOM killed to get memory anywhere."
9614	256696	If hardwall request, stop here
9617	256776	Let dying task have memory
9620	256856	Not hardwall and node outside mems_allowed: scan up cpusets
9654	258174	"cpuset_zone_allowed_hardwall - Can we allocate on zone z's memory node?
@z: is this zone on an allowed node?
@gfp_mask: memory allocation flags

If we're in interrupt, yes, we can always allocate.
If __GFP_THISNODE is set, yes, we can always allocate. If zone
z's node is in our tasks mems_allowed, yes. If the task has been
OOM killed and has access to memory reserves as specified by the
TIF_MEMDIE flag, yes. Otherwise, no.

The __GFP_THISNODE placement logic is really handled elsewhere,
by forcibly using a zonelist starting at a specified node, and by
(in get_page_from_freelist()) refusing to consider the zones for
any node on the zonelist except the first. By the time any such
calls get to this routine, we should just shut up and say 'yes'.

Unlike the cpuset_zone_allowed_softwall() variant, above,
this variant requires that the zone be in the current tasks
mems_allowed or that we're in interrupt. It does not scan up the
cpuset hierarchy for the nearest enclosing mem_exclusive cpuset.
It never sleeps."
9658	258286	node that zone z is on
9668	258556	"Allow tasks that have access to memory reserves because they have
been OOM killed to get memory anywhere."
9683	259052	"cpuset_lock - lock out any changes to cpuset structures

The out of memory (oom) code needs to mutex_lock cpusets
from being changed while it scans the tasklist looking for a
task in an overlapping cpuset. Expose callback_mutex via this
cpuset_lock() routine, so the oom code can lock it, before
locking the task list. The tasklist_lock is a spinlock, so
must be taken inside callback_mutex."
9694	259229	"cpuset_unlock - release lock on cpuset changes

Undo the lock taken in a previous cpuset_lock() call."
9725	260559	"cpuset_mem_spread_node() - On which node to begin search for a page

If a task is marked PF_SPREAD_PAGE or PF_SPREAD_SLAB (as for
tasks in a cpuset with is_spread_page or is_spread_slab set),
and if the memory allocation used cpuset_mem_spread_node()
to determine on which node to start looking, as it will for
certain page cache or slab cache pages such as used for file
system buffers and inode caches, then instead of starting on the
local node to look for a free page, rather spread the starting
node around the tasks mems_allowed nodes.

We don't have to worry about the returned node being offline
because ""it can't happen"", and even if it did, it would be ok.

The routines calling guarantee_online_mems() are careful to
only set nodes in task->mems_allowed that are online. So it
should not be possible for the following code to return an
offline node. But if it did, that would be ok, as this routine
is not returning the node where the allocation must be, only
the node where the search should start. The zonelist passed to
__alloc_pages() will include all nodes. If the slab allocator
is passed an offline node, it will fall back to the local node.
See kmem_cache_alloc_node()."
9748	261267	"cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?
@tsk1: pointer to task_struct of some task.
@tsk2: pointer to task_struct of some other task.

Description: Return true if @tsk1's mems_allowed intersects the
mems_allowed of @tsk2. Used by the OOM killer to determine if
one of the task's memory usage might impact the memory available
to the other."
9760	261621	"Collection of memory_pressure is suppressed unless
this flag is enabled by writing ""1"" to the special
cpuset file 'memory_pressure_enabled' in the root cpuset."
9780	262387	"cpuset_memory_pressure_bump - keep stats of per-cpuset reclaims.

Keep a running average of the rate of synchronous (direct)
page reclaim efforts initiated by tasks in each cpuset.

This represents the rate at which some task in the cpuset
ran low on memory on all nodes it was allowed to use, and
had to enter the kernels page reclaim code in an effort to
create more free memory by tossing clean pages or swapping
or writing dirty pages.

Display to user space in the per-cpuset read-only file
""memory_pressure"". Value displayed is an integer
representing the recent rate of entry into the synchronous
(direct) page reclaim by any task attached to the cpuset."
9800	263040	"proc_cpuset_show()
- Print tasks cpuset path into seq_file.
- Used for /proc/<pid>/cpuset.
- No need to task_lock(tsk) on this tsk->cpuset reference, as it
doesn't really matter if tsk->cpuset changes after we read it,
and we take manage_mutex, keeping attach_task() from changing it
anyway. No need to check that tsk->cpuset != NULL, thanks to
the_top_cpuset_hack in cpuset_exit(), which sets an exiting tasks
cpuset to top_cpuset."
9849	264054	CONFIG_PROC_PID_CPUSET
9851	264129	Display task cpus_allowed, mems_allowed in /proc/<pid>/status file.
9903	266343	"Fast Userspace Mutexes (which I call ""Futexes!"").
(C) Rusty Russell, IBM 2002

Generalized futexes, futex requeueing, misc fixes by Ingo Molnar
(C) Copyright 2003 Red Hat Inc, All Rights Reserved

Removed page pinning, fix privately mapped COW pages and other cleanups
(C) Copyright 2003, 2004 Jamie Lokier

Robust futex support started by Ingo Molnar
(C) Copyright 2006 Red Hat Inc, All Rights Reserved
Thanks to Thomas Gleixner for suggestions, analysis and fixes.

PI-futex support started by Ingo Molnar and Thomas Gleixner



PRIVATE futexes by Eric Dumazet


Thanks to Ben LaHaise for yelling ""hashed waitqueues"" loudly
enough at me, Linus for the original (flawed) idea, Matthew
Kirkwood for proof-of-concept implementation.

""The futexes are also cursed.""
""But they come in a choice of three flavours!""

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA"
9928	266862	Priority Inheritance state:
9933	267012	"list of 'owned' pi_state instances - these have to be
cleaned up in do_exit() if the task exits prematurely:"
9938	267065	The PI object:
9955	267593	"We use this hashed waitqueue instead of a normal wait_queue_t, so
we can wake only the relevant ones (hashed queues may be shared).

A futex_q has a woken state, just like tasks have TASK_RUNNING.
It is considered woken when plist_node_empty(&q->list) || q->lock_ptr == 0.
The order of wakup is always to make the first condition true, then
wake up q->waiters, then make the second condition true."
9960	267700	Which hash list lock to use:
9963	267765	Key which the futex is hashed on:
9966	267827	For fd, sigio sent using these:
9970	267901	Optional priority inheritance state:
9977	268031	Split the global futex_lock into every hash list lock.
9985	268203	Futex-fs vfsmount entry:
9990	268289	Take mm->mmap_sem, when futex is shared
9999	268450	Release mm->mmap_sem, when the futex is shared
10008	268625	We hash on the keys returned from get_futex_key (see below).
10019	268935	Return 1 if two futex_keys are equal, 0 otherwise.
10044	269890	"get_futex_key - Get parameters which are the keys for a futex.
@uaddr: virtual address of the futex
@shared: NULL for a PROCESS_PRIVATE futex,
&current->mm->mmap_sem for a PROCESS_SHARED futex
@key: address where result is stored.

Returns a negative error code or 0
The key words are stored in *key on success.

For shared mappings, it's (page->index, vma->vm_file->f_path.dentry->d_inode,
offset_within_page). For private mappings, it's (uaddr, current->mm).
We can usually work out the index without swapping in the page.

fshared is NULL for PROCESS_PRIVATE futexes
For other futexes, it points to &current->mm->mmap_sem and
caller must have taken the reader lock. but NOT any spinlocks."
10056	270196	"The futex address must be ""naturally"" aligned."
10068	270634	"PROCESS_PRIVATE futexes are fast.
As the mm cannot disappear under us and the 'key' only needs
virtual address, we dont even have to find the underlying vma.
Note : We do have to check 'uaddr' is a valid user address,
but access_ok() should be faster than find_vma()"
10079	270933	"The futex is hashed differently depending on whether
it's in a shared or private mapping. So check vma first."
10086	271036	Permissions.
10098	271507	"Private mappings are handled in a simple way.

NOTE: When userspace waits on a MAP_SHARED mapping, even if
it's a read-only handle, it's expected that futexes attach to
the object not the particular process. Therefore we use
VM_MAYSHARE here, not VM_SHARED which is restricted to shared
mappings of _writable_ handles."
10100	271622	reference taken on mm
10108	271747	Linear file mappings are also simple.
10110	271865	inode-based key.
10122	272253	"We could walk the page table to read the non-linear
pte, and get the page index without fetching the page
from swap. But that's a lot of code to duplicate here
for a rare case, so we simply fetch the page."
10137	272567	"Take a reference to the resource addressed by a key.
Can be called while holding spinlocks."
10155	272990	"Drop a reference to the resource addressed by a key.
The hash bucket spinlock must not be held."
10195	273790	"Fault handling.
if fshared is non NULL, current->mm->mmap_sem is already held"
10215	274329	XXX: let's do this when we verify it is OK
10234	274575	PI code:
10248	274863	pi_mutex gets initialized later
10275	275407	"If pi_state->owner is NULL, the owner is most probably dying
and has cleaned up the pi_state already"
10291	275799	"pi_state->list is already empty.
clear pi_state->owner.
refcount is at 0 - put it back to 1."
10301	275992	"Look up the task based on what TID userspace gave us.
We dont trust it."
10322	276482	"This task is holding PI mutexes at exit time => bad.
Kernel cleans up PI-state, but userspace is likely hosed.
(Robust-futex cleanup is separate and might save the day for userspace.)"
10334	276834	"We are a ZOMBIE and nobody can enqueue itself on
pi_state_list anymore, but we have to be careful
versus waiters unqueueing themselves:"
10350	277219	"We dropped the pi-lock, so re-check whether this
task still owns the PI-state:"
10388	278099	"Another waiter already exists - bump up
the refcount and return its pi_state:"
10392	278201	Userspace might have messed up non PI and PI futexes
10410	278580	"We are the first waiter - try to look up the real owner and attach
the new pi_state to it, but bail out when TID = 0"
10422	278879	"We need to look at the task state flags to figure out,
whether the task is exiting. To protect against the do_exit
change of the task flags, we do this protected by
p->pi_lock:"
10429	279077	"The task is on the way out. When PF_EXITPIDONE is
set, we know that the task has finished the
cleanup:"
10442	279327	"Initialize the pi_mutex in locked state and make 'p'
the owner of it:"
10445	279430	Store the key for possible exit cleanups:
10463	279763	"The hash bucket lock must be held when this is called.
Afterwards, the futex_q must not be accessed."
10472	280043	"The lock in wake_up_all() is a crucial memory barrier after the
plist_del() and also before assigning to q->lock_ptr."
10482	280419	"The waiting task can free the futex_q as soon as this is written,
without taking any locks. This must come last.

A memory barrier is required here to prevent the following store
to lock_ptr from getting ahead of the wakeup. Clearing the lock
at the end of wake_up_all() does not prevent this store from
moving."
10504	281034	"This happens when we have stolen the lock and the original
pending owner did not enqueue itself back on the rt_mutex.
Thats not a tragedy. We know that way, that a lock waiter
is on the fly. We make the futex_q waiter the pending owner."
10512	281244	"We pass it to the next owner. (The WAITERS bit is always
kept enabled while there is PI state around. We must also
preserve the owner died bit.)"
10554	282242	"There is no waiter, so we unlock the futex. The owner died
bit has not to be preserved here. We are the owner:"
10567	282448	Express the locking dependencies for lockdep:
10575	282688	hb1 > hb2
10584	282874	"Wake up all waiters hashed on the physical page that is mapped
to this virtual address:"
10625	283656	"Wake up all waiters hashed on the physical page that is mapped
to this virtual address:"
10665	284555	"we don't get EFAULT from MMU faults if we don't have an MMU,
but we might get them from range checking"
10681	284931	"futex_atomic_op_inuser needs to both read and write
*(int __user *)uaddr2, but we can't modify it
non-atomically. Therefore, if get_user below is not
enough, we need to handle the fault ourselves, while
still holding the mmap_sem."
10693	285176	"If we would have faulted, release mmap_sem,
fault it in and start all over again."
10739	285920	"Requeue all waiters hashed on one physical page to another
physical page."
10778	286803	"If we would have faulted, release mmap_sem, fault
it in and start all over again."
10804	287265	"If key1 and key2 hash to the same bucket, no need to
requeue."
10827	287773	drop_futex_key_refs() must be called outside the spinlocks.
10836	287927	The key must be already stored in q->key.
10866	288673	"The priority used to register this element is
- either the real thread-priority for the real-time threads
(i.e. threads with a priority lower than MAX_RT_PRIO)
- or MAX_RT_PRIO for non-RT threads.
Thus, all RT-threads are woken first in priority order, and
the others are woken last, in FIFO order."
10888	289178	"queue_me and unqueue_me must be called as a pair, each
exactly once. They are called with the hashed spinlock held."
10890	289227	The key must be already stored in q->key.
10899	289449	Return 1 if we were still queued (ie. 0 means we were woken)
10905	289599	In the common case we don't take the spinlock, which is nice.
10923	290253	"q->lock_ptr can change between reading it and
spin_lock(), causing us to take the wrong lock. This
corrects the race condition.

Reasoning goes like this: if we have the wrong lock,
q->lock_ptr must have changed (maybe several times)
between reading it and the spin_lock(). It can
change again after the spin_lock() but only if it was
already changed before the spin_lock(). It cannot,
however, change back to the original value. Therefore
we can detect whether we acquired the correct lock."
10945	290697	"PI futexes can not be requeued and must remove themself from the
hash bucket. The hash bucket lock (i.e. lock_ptr) is held on entry
and dropped here."
10965	291105	"Fixup the pi_state owner with the new owner.

Must be called with hash bucket lock held and mm->sem held for non
private futexes."
10974	291370	Owner died?
10994	291946	"We own it, so we have to replace the pending owner
TID. This must be atomic as we have preserve the
owner died bit here."
11014	292343	"In case we must use restart_block to restart a futex_wait,
we encode in the 'flags' shared capability"
11060	293746	"Access the page AFTER the futex is queued.
Order is important:

Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);
Userspace waker: if (cond(var)) { var = new; futex_wake(&var); }

The basic logical guarantee of a futex is that it blocks ONLY
if cond(var) is known to be true at the time of blocking, for
any cond. If we queued after testing *uaddr, that would open
a race condition where we could block indefinitely with
cond(var) false, which would violate the guarantee.

A consequence is that futex_wait() can return zero and absorb
a wakeup when *uaddr != val on entry to the syscall. This is
rare, but normal.

for shared futexes, we hold the mmap semaphore, so the mapping
cannot have changed since we looked it up in get_futex_key."
11069	293942	"If we would have faulted, release mmap_sem, fault it in and
start all over again."
11082	294171	Only actually queue if *uaddr contained val.
11088	294311	"Now the futex is queued and we have checked the data, we
don't want to hold mmap_sem while we sleep."
11098	294686	"There might have been scheduling since the queue_me(), as we
cannot hold a spinlock across the get_user() in case it
faults, and we cannot just set TASK_INTERRUPTIBLE state when
queueing ourselves into the futex hash. This code thus has to
rely on the futex_wake() code removing us from hash when it
wakes us up."
11100	294751	add_wait_queue is the barrier after __set_current_state.
11106	294962	"!plist_node_empty() is safe here without any lock.
q.lock_ptr != 0 is not safe, because of ordering against wakeup."
11121	295400	"the timer could have already expired, in which
case current would be flagged for rescheduling.
Don't bother calling schedule."
11127	295505	Flag if a timeout occured
11136	295680	"NOTE: we don't remove ourselves from the waitqueue because
we are the only user of it."
11138	295745	If we were woken (and unqueued), we succeeded, whatever.
11147	295918	"We expect signal_pending(current), but another thread may
have handled it for us already."
11192	297086	"Userspace tried a 0 -> TID atomic transition of the futex value
and failed. The kernel side here does the whole locking operation:
if there are waiters then it will block, it does PI, etc. (Due to
races the kernel might see a 0 value of the futex too.)"
11231	298042	"To avoid races, we attempt to take the lock here again
(by doing a 0 -> TID atomic cmpxchg), while holding all
the locks. It will most likely not succeed."
11242	298310	"Detect deadlocks. In case of REQUEUE_PI this is a valid
situation and we return success to user space."
11250	298499	Surprise - we got the lock. Just return to userspace:
11259	298672	"Set the WAITERS flag, so the owner will know it has someone
to wake at next unlock"
11269	298998	"There are two cases, where a futex might have no owner (the
owner TID is 0): OWNER_DIED. We take over the futex in this
case. We also do an unconditional take over, when the owner
of the futex died.

This is safe as we are protected by the hash bucket lock !"
11271	299088	Keep the OWNER_DIED bit
11286	299418	We took the lock due to owner died take over.
11293	299581	"We dont have the lock. Look up the PI state (or create it if
we are the first waiter):"
11303	299777	"Task is exiting and we just wait for the
exit to complete."
11314	300031	"No owner found for this futex. Check if the
OWNER_DIED bit is set to figure out whether
this is a robust futex or not."
11322	300235	"We simply start over in case of a robust
futex. The code above will take the futex
and return happy."
11334	300436	Only actually queue now that the atomic ops are done:
11340	300576	"Now the futex is queued and we have checked the data, we
don't want to hold mmap_sem while we sleep."
11346	300663	Block on the PI mutex:
11351	300834	Fixup the trylock return value:
11363	301070	"Got the lock. We might not be the anticipated owner
if we did a lock-steal - fix up the PI-state in
that case:"
11371	301299	"Catch the rare case, where the lock was released
when we were on the way back before we locked the
hash bucket."
11378	301514	"Try to get the rt_mutex now. This might
fail as some other task acquired the
rt_mutex after we removed ourself from the
rt_mutex waiters list."
11392	301925	"pi_state is incorrect, some other
task did a lock steal and we
returned due to timeout or signal
without taking the rt_mutex. Too
late. We can access the
rt_mutex_owner without locking, as
the other task is now blocked on
the hash bucket lock. Fix the state
up."
11402	302188	propagate -EFAULT, if the fixup failed
11412	302422	"Paranoia check. If we did not take the lock
in the trylock above, then we should not be
the owner of the rtmutex, neither the real
nor the pending one:"
11421	302680	Unqueue and drop the lock
11442	303156	"We have to r/w *(int __user *)uaddr, but we can't modify it
non-atomically. Therefore, if get_user below is not
enough, we need to handle the fault ourselves, while
still holding the mmap_sem.

... and hb->lock. :-) --ANK"
11466	303628	"Userspace attempted a TID -> 0 atomic transition, and failed.
This is the in-kernel slowpath: we look up the PI state (if any),
and do the rt-mutex unlock."
11481	303956	We release only a lock we actually own:
11486	304081	First take all the futex related locks:
11501	304403	"To avoid races, try to do the TID -> 0 atomic transition
again. If it succeeds then we can return without waking
anyone else up:"
11511	304662	"Rare case: we managed to release the lock atomically,
no need to wake anyone else up:"
11518	304833	"Ok, other tasks may need to be woken up - check waiters
and do the wakeup if necessary:"
11529	305128	"The atomic access to the futex value
generated a pagefault, so retry the
user-access and the wakeup:"
11536	305244	No waiters - kernel unlocks the futex:
11558	305711	"We have to r/w *(int __user *)uaddr, but we can't modify it
non-atomically. Therefore, if get_user below is not
enough, we need to handle the fault ourselves, while
still holding the mmap_sem.

... and hb->lock. --ANK"
11589	306223	This is one-shot: once it's gone off you need a new fd
11601	306544	"plist_node_empty() is safe here without any lock.
q->lock_ptr != 0 is not safe, because of ordering against wakeup."
11616	306838	"Signal allows caller to avoid the race which would occur if they
set the sigio stuff up afterwards."
11677	308179	"queue_me() must be called before releasing mmap_sem, because
key->shared.inode needs to be referenced while holding it."
11683	308307	Now we map fd to filp, so userspace can access it
11707	309119	"Support for robust futexes: the kernel cleans up held futexes at
thread exit time.

Implementation: user-space maintains a per-thread list of locks it
is holding. Upon do_exit(), the kernel carefully walks this list,
and marks all locks that are owned by this thread with the
FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is
always manipulated with the lock held, so the list is private and
per-thread. Userspace also maintains a per-thread 'list_op_pending'
field, to allow the kernel to clean up if the thread dies after
acquiring the lock, but just before it could have added itself to
the list. There can only be one such pending lock."
11713	309284	"sys_set_robust_list - set the robust-futex list head of a task
@head: pointer to the list-head
@len: length of the list-head, as userspace expects"
11720	309431	The kernel knows only one size for now:
11734	309803	"sys_get_robust_list - get the robust-futex list head of a task
@pid: pid of the process [zero for current task]
@head_ptr: pointer to a list-head pointer, the kernel fills it in
@len_ptr: pointer to a length field, the kernel fills in the header size"
11773	310596	"Process a futex-list entry, check whether it's owned by the
dying task, and do notification if so:"
11792	311203	"Ok, this dying thread is truly holding a futex
of interest. Set the OWNER_DIED bit atomically
via cmpxchg, and if the value had FUTEX_WAITERS
set, wake up a waiter (if any). (We have to do a
futex_wake() even if OWNER_DIED is already set -
to handle the rare but possible case of recursive
thread-death.) The rest of the cleanup is done in
userspace."
11805	311496	"Wake robust non-PI futexes here. The wakeup of
PI futexes happens in exit_pi_state():"
11814	311662	Fetch a robust-list pointer. Bit 0 signals PI futexes:
11835	312177	"Walk curr->robust_list (very carefully, it's a userspace list!)
and mark any locks found there dead, and notify any waiters.

We silently return on any sign of list-walking problem."
11847	312538	"Fetch the list head (which was registered earlier, via
sys_set_robust_list()):"
11852	312650	Fetch the relative futex offset:
11858	312798	"Fetch any possibly pending lock-add first, and handle it
if it exists:"
11862	312923	avoid warning with gcc
11867	313044	"Fetch the next entry in the list before calling
handle_futex_death:"
11872	313201	"A pending lock might already be on the list, so
don't process it twice:"
11883	313434	Avoid excessively long or circular lists:
11913	314081	non-zero val means F_SETOWN(getpid()) & F_SETSIG(val)
11964	315342	"requeue parameter in 'utime' if cmd == FUTEX_REQUEUE.
number of waiters to wake in 'utime' if cmd == FUTEX_WAKE_OP."
12037	317179	"linux/kernel/hrtimer.c





High-resolution kernel timers

In contrast to the low-resolution timeout API implemented in
kernel/timer.c, hrtimers provide finer resolution and accuracy
depending on system configuration and capabilities.

These timers are currently used for:
- itimers
- POSIX timers
- nanosleep
- precise in-kernel timing

Started by: Thomas Gleixner and Ingo Molnar

Credits:
based on kernel/timer.c

Help, testing, suggestions, bugfixes, improvements were
provided by:

George Anzinger, Andrew Morton, Steven Rostedt, Roman Zippel
et. al.

For licencing details see kernel-base/COPYING"
12058	317625	"ktime_get - get the monotonic time in ktime_t format

returns the time in ktime_t format"
12073	317874	"ktime_get_real - get the real (wall-) time in ktime_t format

returns the time in ktime_t format"
12093	318386	"The timer bases:

Note: If we want to add new timer bases, we have to skip the two
clock ids captured by the cpu-timers. We do this by holding empty
entries rather than doing math adjustment of the clock ids.
This ensures that we capture erroneous accesses to these clock ids
rather than moving them into the range of valid clock id's."
12119	318976	"ktime_get_ts - get the monotonic clock in timespec format
@ts: pointer to timespec variable

The function calculates the monotonic clock from the realtime
clock and the wall_to_monotonic offset and stores the result
in normalized timespec format in the variable pointed to by @ts."
12140	319425	"Get the coarse grained time at the softirq based on xtime and
wall_to_monotonic."
12163	320002	"Helper function to check, whether the timer is running the callback
function"
12172	320220	"Functions and macros which are different for UP/SMP systems are kept in a
single place"
12186	320711	"We are using hashed locking: holding per_cpu(hrtimer_bases)[n].lock
means that all timers which are tied to this base via timer->base are
locked, and the base itself is locked too.

So __run_timers/migrate_timers can safely modify all timers which could
be found on the lists/queues.

When the timer's base is locked, and the timer removed from list, it is
possible to set timer->base = NULL and drop the lock: the timer remains
locked."
12199	321079	The timer has migrated to another CPU:
12208	321228	Switch the timer base to the current CPU when possible.
12227	321964	"We are trying to schedule the timer on the local CPU.
However we can't change timer's base while it is running,
so we keep it on the same CPU. No hassle vs. reprogramming
the event source in the high resolution case. The softirq
code will take care of this when the timer function has
completed. There is no conflict as we hold the lock until
the timer is enqueued."
12231	322075	See the comment in lock_timer_base()
12240	322248	CONFIG_SMP
12254	322545	!CONFIG_SMP
12259	322648	"Functions for the union type storage format of ktime_t which are
too large for inlining:"
12268	322889	"ktime_add_ns - Add a scalar nanoseconds value to a ktime_t variable
@kt: addend
@nsec: the scalar nsec value to add

Returns the sum of kt and nsec in ktime_t format"
12292	323389	"ktime_sub_ns - Subtract a scalar nanoseconds value from a ktime_t variable
@kt: minuend
@nsec: the scalar nsec value to subtract

Returns the subtraction of @nsec from @kt in ktime_t format"
12309	323710	!CONFIG_KTIME_SCALAR
12313	323764	Divide a ktime value by a nanosecond value
12321	323946	Make sure the divisor is less than 2^32:
12331	324109	BITS_PER_LONG >= 64
12333	324156	High resolution timer related functions
12338	324229	High resolution timer enabled ?
12343	324330	Enable / Disable high resolution mode
12359	324641	hrtimer_high_res_enabled - query, if the highres mode is enabled
12367	324771	Is the high resolution mode active ?
12377	325010	"Reprogram the event source with checking both queues for the
next event
Called with interrupts disabled and base->lock held"
12409	325956	"Shared reprogramming for clock_realtime and clock_monotonic

When a timer is enqueued and expires earlier than the already enqueued
timers, we have to check, whether it expires earlier than the timer for
which the clock event device was armed.

Called with interrupts disabled and base->cpu_base.lock held"
12423	326530	"When the callback is running, we do not reprogram the clock event
device. The timer callback is either running on a different CPU or
the callback is executed in the hrtimer_interrupt context. The
reprogramming is handled either by the softirq, which called the
callback or at the end of the hrtimer_interrupt."
12432	326708	Clockevents returns -ETIME, when the event was in the past.
12444	326933	"Retrigger next event is called after clock was set

Called with interrupts disabled via on_each_cpu()"
12463	327379	Adjust CLOCK_REALTIME offset
12482	327911	"Clock realtime was set

Change the offset of the realtime clock vs. the monotonic
clock.

We might have to reprogram the high resolution timer interrupt. On
SMP we call the architecture specific code to retrigger _all_ high
resolution timer interrupts. On UP we just disable interrupts and
call the high resolution interrupt code."
12485	327987	Retrigger the CPU local events everywhere
12492	328148	"During resume we might have to reprogram the high resolution timer
interrupt (on the local CPU):"
12497	328258	Retrigger the CPU local events:
12503	328357	Check, whether the timer is on the callback pending list
12511	328530	Remove a timer from the callback pending list
12519	328704	Initialize the high resolution related parts of cpu_base
12529	328943	Initialize the high resolution related parts of a hrtimer
12540	329361	"When High resolution timers are active, try to reprogram. Note, that in case
the state has HRTIMER_STATE_CALLBACK set, no reprogramming and no expiry
check happens. The timer gets enqueued into the rbtree. The reprogramming
and expiry check is done in the hrtimer_interrupt or in the softirq."
12546	329596	Timer is expired, act upon the callback mode
12552	329768	"We can call the callback from here. No restart
happens, so no danger of recursion"
12562	330133	"This is solely for the sched tick emulation with
dynamic tick support to ensure that we do not
restart the tick right on the edge and end up with
the tick timer in the softirq ! The calling site
takes care of this."
12568	330272	Move everything else into the softirq pending list !
12583	330519	Switch to high resolution mode
12607	331139	"""Retrigger"" the interrupt to get things going"
12631	332020	CONFIG_HIGH_RES_TIMERS
12647	332340	Counterpart to lock_hrtimer_base above:
12662	332751	"hrtimer_forward - forward the timer expiry
@timer: hrtimer to forward
@now: forward past this time
@interval: the interval to forward

Forward the timer expiry so it will expire in the future.
Returns the number of overruns."
12687	333372	"This (and the ktime_add() below) is the
correction for exact:"
12694	333525	"Make sure, that the result did not wrap with a very large
interval."
12707	333845	"enqueue_hrtimer - internal function to (re)start a timer

The timer is inserted in expiry order. Insertion into the
red black tree is O(log(n)). Must hold the base lock."
12718	334126	Find the right place in the rbtree:
12725	334310	"We dont care about collisions. Nodes with
the same expiry time stay together."
12737	334558	"Insert the timer to the rbtree and check whether it
replaces the first pending timer"
12746	334824	"Reprogram the clock event device. When the timer is already
expired hrtimer_enqueue_reprogram has either called the
callback or added it to the pending list and raised the
softirq.

This is a NOP for !HIGHRES"
12758	335143	"HRTIMER_STATE_ENQUEUED is or'ed to the current state to preserve the
state of a possibly running callback."
12771	335554	"__remove_hrtimer - internal function to remove a timer

Caller must hold the base lock.

High resolution timer mode reprograms the clock event device when the
timer is the one which expires next. The caller can disable this by setting
reprogram to zero. This is useful, when the context does a reprogramming
anyway (e.g. timer interrupt)"
12776	335745	High res. callback list. NOP for !HIGHRES
12783	335925	"Remove the timer from the rbtree and replace the
first entry pointer if necessary."
12786	336056	Reprogram the clock event device. if enabled
12797	336274	remove hrtimer, called with base lock held
12811	336783	"Remove the timer and force reprogramming when high
resolution mode is active and the timer is on the current
CPU. If we remove a timer on another CPU, reprogramming is
skipped. The interrupt event on this CPU is fired and
reprogramming happens in the interrupt handler. This is a
rare case and less expensive than a smp call."
12830	337259	"hrtimer_start - (re)start an relative timer on the current CPU
@timer: the timer to be added
@tim: expiry time
@mode: expiry mode: absolute (HRTIMER_ABS) or relative (HRTIMER_REL)

Returns:
0 on success
1 when the timer was active"
12840	337512	Remove an active timer from the queue:
12843	337593	Switch the timer base, if necessary:
12854	338022	"CONFIG_TIME_LOW_RES is a temporary way for architectures
to signal that they simply return xtime in
do_gettimeoffset(). In this case we want to round up by
resolution when starting a relative timer, to avoid short
timeouts. This will go away with the GTOD framework."
12863	338294	"Careful here: User space might have asked for a
very long sleep, so the add above might result in a
negative number, which enqueues the timer in front
of the queue."
12874	338545	"Only allow reprogramming if the new base is on this CPU.
(it might still be on another CPU if the timer was pending)"
12893	338989	"hrtimer_try_to_cancel - try to deactivate a timer
@timer: hrtimer to stop

Returns:
0 when the timer was not active
1 when the timer was active
-1 when the timer is currently excuting the callback function and
cannot be stopped"
12919	339528	"hrtimer_cancel - cancel a timer and wait for the handler to finish.
@timer: the timer to be cancelled

Returns:
0 when the timer was not active
1 when the timer was active"
12935	339811	"hrtimer_get_remaining - get remaining time for the timer
@timer: the timer to read"
12956	340351	"hrtimer_get_next_event - get the time until next expiry event

Returns the delta to the next expiry event or KTIME_MAX if no timer
is pending."
12995	341298	"hrtimer_init - initialize a timer to the given clock
@timer: the timer to be initialized
@clock_id: the clock to be used
@mode: timer mode abs/rel"
13026	342128	"hrtimer_get_res - get the timer resolution for a clock
@which_clock: which clock to query
@tp: pointer to timespec variable to store the resolution

Store the resolution of the clock selected by @which_clock in the
variable pointed to by @tp."
13043	342515	"High resolution timer interrupt
Called with interrupts disabled"
13085	343457	Move softirq callbacks to the pending list
13104	343977	"Note: We clear the CALLBACK bit after
enqueue_hrtimer to avoid reprogramming of
the event hardware. This happens at the end
of this function anyway."
13117	344293	Reprogramming necessary ?
13123	344417	Raise softirq ?
13158	345270	"Enqueue the timer, allow reprogramming of the event
device"
13164	345445	"If the timer was rearmed on another CPU, reprogram
the event device."
13172	345612	CONFIG_HIGH_RES_TIMERS
13176	345658	Expire the per base hrtimer-queue:
13228	346960	"Called from timer softirq every jiffy, expire hrtimers:

For HRT its the fall back code to run the softirq in the timer
softirq context in case the hrtimer initialization failed or has
not been done yet."
13244	347445	"This _is_ ugly: We have to check in the softirq context,
whether we can switch to highres and / or nohz mode. The
clocksource switch happens in the timer interrupt with
xtime_lock held. Notification from there only sets the
check bit in the tick_oneshot code, otherwise we might
deadlock vs. xtime_lock."
13257	347706	Sleep related functions:
13323	349260	The other values in restart are already filled in
13339	349696	Absolute timers do not update the rmtp value and restart:
13385	350711	Functions related to boot-time initialization:
13415	351551	Enqueue the timer. Allow reprogramming of the event device
13445	352249	CONFIG_HOTPLUG_CPU
13501	353555	"kallsyms.c: in-kernel printing of symbolic oopses and stack traces.

Rewritten and vastly simplified by Rusty Russell for in-kernel
module loader:


ChangeLog:

(25/Aug/2004) Paulo Marques <pmarques@grupopie.com>
Changed the compression method from stem compression to ""table lookup""
compression (see scripts/kallsyms.c for a more complete description)"
13509	353781	for cond_resched
13521	354018	These will be re-linked against their real values during the second link stage
13527	354251	"tell the compiler that the count isn't in the small data section if the arch
has one (eg: FRV)"
13576	355541	"expand a compressed symbol data into the resulting uncompressed string,
given the offset to where the symbol is in the compressed stream"
13582	355739	get the compressed symbol length from the first symbol byte
13588	355888	"update the offset to return the offset for the next symbol on
the compressed stream"
13592	355998	"for every byte on the compressed symbol data, copy the table
entry for that byte"
13610	356284	return to offset to the next symbol
13615	356406	"get symbol type information. This is encoded as a single char at the
begining of the symbol name"
13619	356572	"get just the first code, look it up in the token table, and return the
first char from this token"
13625	356743	"find the offset on the compressed stream given and index in the
kallsyms array"
13632	356939	"use the closest marker we have. We have markers every 256 positions,
so that should be close enough"
13638	357235	"sequentially scan all the symbols up to the point we're searching for.
Every symbol is stored in a [<len>][<len> bytes of data] format, so we
just need to add the len to the current pointer for every symbol we
wish to skip"
13645	357398	Lookup the address for this symbol. Returns 0 if not found.
13668	357993	This kernel should never had been booted.
13671	358089	do a binary search on the sorted kallsyms_addresses array
13686	358362	"search for the first aliased symbol. Aliased
symbols are symbols with the same address"
13692	358525	Search for next non-aliased symbol
13700	358742	if we found no next symbol, we use the end of the section
13720	359131	Lookup an address but don't bother to find any names.
13736	359638	"Lookup an address
- modname is set to NULL if it's in the kernel
- we guarantee that the returned name is valid until we reschedule even if
it resides in a module
- we also guarantee that modname will be valid until rescheduled"
13751	359978	Grab name
13758	360125	see if it's in a module
13775	360498	Grab name
13779	360603	see if it's in a module
13793	360938	Grab name
13798	361061	see if it's in a module
13802	361197	Look up a kernel symbol and return it in a text buffer.
13821	361707	Look up a kernel symbol and print it to the kernel messages.
13831	361940	To avoid using get_symbol_offset for every symbol, we carry prefix along.
13836	362062	If iterating in core kernel symbols
13852	362415	Returns space to next name.
13874	362930	Returns false if pos at or past end of file.
13877	363042	Module symbols can be accessed randomly.
13883	363199	If we're not on the desired position, reset to new position.
13917	363794	Some debugging symbols have no name. Ignore them.
13925	363947	"Label it ""global"" if it is exported,
""local"" if not exported."
13949	364629	"We keep iterator in m->private, since normal case is to
s_start from where we left off, so we avoid doing
using get_symbol_offset for every symbol"
13993	365593	"kexec.c - kexec system call


This source code is licensed under the GNU General Public License,
Version 2. See the file COPYING for more details."
14021	366273	Per cpu memory for storing cpu states in case of system crash.
14024	366322	vmcoreinfo stuff
14030	366552	Location of the reserved area for the crash kernel
14083	368753	"When kexec transitions to the new kernel there is a one-to-one
mapping between physical and virtual addresses. On processors
where you can disable the MMU this is trivial, and easy. For
others it is still a simple predictable page table to setup.

In that environment kexec copies the new kernel to its final
resting place. This means I can only support memory whose
physical address can fit in an unsigned long. In particular
addresses where (pfn << PAGE_SHIFT) > ULONG_MAX cannot be handled.
If the assembly stub has more restrictive requirements
KEXEC_SOURCE_MEMORY_LIMIT and KEXEC_DEST_MEMORY_LIMIT can be
defined more restrictively in <asm/kexec.h>.

The code for the transition from the current kernel to the
the new kernel is placed in the control_code_buffer, whose size
is given by KEXEC_CONTROL_CODE_SIZE. In the best case only a single
page of memory is necessary, but some architectures require more.
Because this memory must be identity mapped in the transition from
virtual to physical addresses it must live in the range
0 - TASK_SIZE, as only the user space mappings are arbitrarily
modifiable.

The assembly stub in the control code buffer is passed a linked list
of descriptor pages detailing the source pages of the new kernel,
and the destination addresses of those source pages. As this data
structure is not used in the context of the current OS, it must
be self-contained.

The code has been made to work with highmem pages and will use a
destination page in its final resting place (if it happens
to allocate it). The end product of this is that most of the
physical address space, and most of RAM can be used.

Future directions include:
- allocating a page table with the control code buffer identity
mapped, to simplify machine_kexec and make kexec_on_panic more
reliable."
14088	368892	"KIMAGE_NO_DEST is an impossible destination address..., for
allocating pages whose destination address we do not care about."
14106	369463	Allocate a controlling structure
14115	369701	By default this does not apply
14119	369804	Initialize the list of control pages
14122	369893	Initialize the list of destination pages
14125	369977	Initialize the list of unuseable pages
14128	370048	Read in the segments
14147	370781	"Verify we have good destination addresses. The caller is
responsible for making certain we don't attempt to load
the new image into invalid or reserved areas of RAM. This
just verifies it is an address we can use.

Since the kernel does everything in page size chunks ensure
the destination addreses are page aligned. Too many
special cases crop of when we don't do this. The most
insidious is getting overlapping destination addresses
simply because addresses are changed to page size
granularity."
14164	371294	"Verify our destination addresses do not overlap.
If we alloed overlapping destination addresses
through very weird things can happen with no
easy explanation as one segment stops on another."
14176	371654	Do the segments overlap ?
14186	371904	"Ensure our buffer sizes are strictly less than
our memory sizes. This should always be the case,
and it is easier to check up front than to be surprised
later on."
14211	372376	Allocate and initialize a controlling structure
14223	372663	"Find a location for the control code buffer, and add it
the vector of segments so that it's pages will also be
counted as destination pages."
14251	373263	Verify we have a valid entry point
14257	373424	Allocate and initialize a controlling structure
14264	373592	"Enable the special crash kernel control page
allocation policy."
14276	374051	"Verify we have good destination addresses. Normally
the caller is responsible for making certain we don't
attempt to load the new image into invalid or reserved
areas of RAM. But crash kernels are preloaded into a
reserved area of ram. We must ensure the addresses
are in the reserved area otherwise preloading the
kernel could corrupt things."
14283	374279	Ensure we are within the crash kernel limits
14292	374517	"Find a location for the control code buffer, and add
the vector of segments so that it's pages will also be
counted as destination pages."
14385	376668	"Control pages are special, they are the intermediaries
that are needed while we copy the rest of the pages
to their final resting place. As such they must
not conflict with either the destination addresses
or memory the kernel is already using.

The only case where we really need more than one of
these are for architectures where we cannot disable
the MMU and must instead generate an identity mapped
page table for all of the memory.

At worst this runs in O(N) of the image size."
14395	376887	"Loop while I can allocate a page and the page allocated
is a destination page."
14414	377375	Remember the allocated page...
14422	377681	"Because the page is already in it's destination
location we will never allocate another page at
that address. Therefore kimage_alloc_pages
will not return it (again) and we don't need
to give it an entry in image->segment[]."
14430	377940	"Deal with the destination pages I have inadvertently allocated.

Ideally I would convert multi-page allocations into single
page allocations, and add everyting to image->dest_pages.

For now it is simpler to just free the pages."
14459	379008	"Control pages are special, they are the intermediaries
that are needed while we copy the rest of the pages
to their final resting place. As such they must
not conflict with either the destination addresses
or memory the kernel is already using.

Control pages are also the only pags we must allocate
when loading a crash kernel. All of the other pages
are specified by the segments and we just memcpy
into them directly.

The only case where we really need more than one of
these are for architectures where we cannot disable
the MMU and must instead generate an identity mapped
page table for all of the memory.

Given the low demand this implements a very simple
allocator that finds the first hole of the appropriate
size in the reserved memory region, and allocates all
of the memory up to and including the hole."
14474	379426	See if I overlap any of the segments
14481	379696	Advance the hole to the end of the segment
14487	379870	If I don't overlap any segments I have found my hole!
14572	381611	Walk through and free any extra destination pages I may have
14575	381719	Walk through and free any unuseable pages I have cached
14613	382538	Free the previous indirection page
14618	382669	"Save this indirection page until we are
done with it."
14624	382791	Free the final indirection page
14628	382889	Handle any machine specific cleanup
14631	382960	Free the kexec control pages...
14676	384224	"Here we implement safeguards to ensure that a source page
is not copied to its destination page before the data on
the destination page is no longer useful.

To do this we maintain the invariant that a source page is
either its own destination page, or it is not a
destination page at all.

That is slightly stronger than required, but the proof
that no problems will not occur is trivial, and the
implementation is simply to verify.

When allocating all pages normally this algorithm will run
in O(N) time, but in the worst case it will run in O(N^2)
time. If the runtime is a problem the data structures can
be fixed."
14683	384354	"Walk through the list of destination pages, and see if I
have a match."
14695	384635	Allocate a page, if we run out of memory give up
14699	384754	If the page cannot be used file it away
14707	384992	If it is the destination page we want use it
14711	385083	If the page is not a destination page use it
14720	385345	"I know that the page is someones destination page.
See if there is already a source page for this
destination page. And if so swap the source pages."
14723	385419	If so move it
14734	385718	"The old page I have found cannot be a
destination page, so return it."
14742	385860	"Place the page on the destination list I
will use it later."
14784	386660	Start with a clear page
14816	387364	"For crash dumps kernels we simply copy the data from
user space to it's destination.
We do things a page at a time for the sake of kmap."
14846	387989	Zero the trailing part of the page
14901	389425	"Exec Kernel system call: for obvious reasons only root may call it.

This call breaks up into three pieces.
- A generic part which loads the new kernel from the current
address space, and very carefully places the data in the
allocated pages.

- A generic part that interacts with the kernel and tells all of
the devices to shut down. Preventing on-going dmas, and placing
the devices in a consistent state so a later kernel can
reinitialize them.

- A machine specific part that includes the syscall number
and the copies the image to it's final destination. And
jumps into the image at entry.

kexec does not sync, or unmount filesystems so if you need
that to happen you need to do that yourself."
14908	389600	"A home grown binary mutex.
Nothing can wait so this mutex is safe to use
in interrupt context :)"
14919	389899	We only trust the superuser with rebooting the system.
14926	390042	"Verify we have a legal set of flags
This leaves us room for future extensions."
14930	390172	Verify we are on the appropriate architecture
14937	390377	"Put an artificial cap on the number
of segments passed to kexec_load."
14951	390786	"Because we write directly to the reserved memory
region when loading crash kernels we need a mutex here to
prevent multiple crash kernels from attempting to load
simultaneously, and to prevent a crash kernel from loading
over the top of a in use crash kernel.

KISS: always take the mutex."
14962	391031	Loading another kernel to reboot into
14966	391210	Loading another kernel to switch to if this one crashes
14970	391323	"Free any current crash dump kernel before
we corrupt it."
14991	391804	Install the new kernel, and Uninstall the old
14995	391900	Release the mutex
15014	392348	"Don't allow clients that don't understand the native
architecture to do anything."
15053	393326	"Take the kexec_lock here to prevent sys_kexec_load
running on one cpu from replacing the crash kernel
we are using after a panic on a different cpu.

If the crash kernel was not located in a fixed area
of memory the xchg(&kexec_crash_image) would be
sufficient. But since I reuse the memory..."
15110	394661	"Using ELF notes here is opportunistic.
I need a well defined structure format
for the data I pass, and I need tags
on the data to indicate what information I have
squirrelled away. ELF notes happen to provide
all of that, so there is no need to invent something new."
15124	395073	Allocate memory for saving cpu registers.
15140	395412	"parsing the ""crashkernel"" commandline

this code is intended to be called from architecture specific code"
15149	395591	"This function parses command lines in the format

crashkernel=ramsize-range:size[,...][@offset]

The function returns 0 on success and -EINVAL on failure."
15157	395840	for each entry of the comma-separated list
15161	395934	get the start of the range
15174	396224	if no ':' is here, than we read the end
15206	396842	match ?
15236	397390	"That function parses ""simple"" (old) crashkernel command lines like

crashkernel=size[@offset]

It returns 0 on success and -EINVAL on failure."
15258	397889	"That function is the entry point for command line parsing and should be
called from the arch-specific code."
15271	398270	find crashkernel and use the last one if there are more
15281	398457	"strlen(""crashkernel="")"
15286	398583	"if the commandline contains a ':', then that's the extended
syntax -- if not, it must be the classic syntax"
15339	399644	"provide an empty default implementation here -- architecture
code may override this"
15422	402153	"kmod, the new module loader (replaces kerneld)
Kirk Petersen

Reorganized not to be a daemon by Adam Richter, with guidance
from Greg Zornetzer.

Modified to avoid chroot and file sharing problems.
Mikael Pettersson

Limit the concurrent number of kmod modprobes to catch loops from
""modprobe needs a service that is in a module"".
Keith Owens <kaos@ocs.com.au> December 1999

Unblock all signals when we exec a usermode process.
Shuu Yamaguchi <shuu@wondernetworkresources.com> December 2000

call_usermodehelper wait flag, and remove exec_usermodehelper.
Rusty Russell <rusty@rustcorp.com.au> Jan 2003"
15450	402767	modprobe_path is set via /proc/sys.
15466	403443	"request_module - try to load a kernel module
@fmt: printf style format string for the name of the module
@varargs: arguements as specified in the format string

Load a module using the user mode module loader. The function returns
zero on success or a negative errno code on failure. Note that a
successful module load does not mean the module did not then unload
and exit on an error of its own. Callers must check that the service
they requested is now available not blindly invoke it.

If module auto-loading support is disabled then this function
becomes a no-operation."
15479	403870	Completely arbitrary value - KAO
15499	404686	"If modprobe needs a service that is in a module, we get a recursive
loop. Limit the number of running kmod threads to max_threads/2 or
MAX_KMOD_CONCURRENT, whichever is the smaller. A cleaner method
would be to run the parents of this process, counting how many times
kmod was invoked. That would mean accessing the internals of the
process tables to get the command line, proc_pid_cmdline is static
and it is not worth changing the proc code just to handle this case.
KAO.

""trace the ppid"" is simple, but will fail if someone's
parent exits. I think this is as good as it gets. --RR"
15503	404886	We may be blaming an innocent here, but unlikely
15517	405226	CONFIG_KMOD
15534	405534	This is the task which runs the usermode application
15541	405735	Unblock all signals and set the session keyring.
15552	406083	Install input pipe when needed
15556	406227	no races because files should be private here
15565	406459	and disallow core files too
15569	406582	We can run anywhere, unlike our parent keventd().
15575	406760	"Our parent is keventd, which runs with elevated scheduling priority.
Avoid propagating that into the userspace child."
15583	406930	Exec failed?
15596	407217	Keventd can't block, but this (a child) can.
15603	407429	"Install a handler: if SIGCLD isn't handled sys_wait4 won't
populate the status, but will return -ECHILD."
15620	407979	"Normally it is bogus to call wait4() from in-kernel because
wait4() wants to write the exit code to a userspace address.
But wait_for_helper() always runs as keventd, and put_user()
to a kernel address works OK for kernel threads, due to their
having an mm_segment_t which spans the entire address space.

Thus the __user pointer cast is valid here."
15627	408220	"If ret is 0, either ____call_usermodehelper failed and the
real error code is already in sub_info->retval or
sub_info->retval is 0 anyway, so don't mess with it then."
15639	408429	This is run by khelper thread
15649	408779	"CLONE_VFORK: wait until the usermode helper has execve'd
successfully We need the data structures to stay around
until that is done."
15665	409156	FALLTHROUGH
15677	409475	"If set, call_usermodehelper_exec() will exit immediately returning -EBUSY
(used for preventing user land processes from being created after the user
land has been frozen during a system-wide hibernation or suspend operation)."
15680	409544	Number of helpers running
15686	409704	"Wait queue head used by usermodehelper_pm_callback() to wait for all running
helpers to finish."
15692	409905	"Time to wait for running_helpers to become zero before the setting of
usermodehelper_disabled in usermodehelper_pm_callback() fails"
15711	410421	"From now on call_usermodehelper_exec() won't start any new
helpers, so it is sufficient if running_helpers turns out to
be zero at one point (it may be increased later, but that
doesn't matter)."
15746	411111	CONFIG_PM
15752	411309	CONFIG_PM
15763	411671	"call_usermodehelper_setup - prepare to call a usermode helper
@path: path to usermode executable
@argv: arg vector for process
@envp: environment for process

Returns either %NULL on allocation failure, or a subprocess_info
structure. This should be passed to call_usermodehelper_exec to
exec the process and free the structure."
15786	412299	"call_usermodehelper_setkeys - set the session keys for usermode helper
@info: a subprocess_info returned by call_usermodehelper_setup
@session_keyring: the session keyring for the process"
15803	412902	"call_usermodehelper_setcleanup - set a cleanup function
@info: a subprocess_info returned by call_usermodehelper_setup
@cleanup: a cleanup function

The cleanup function is just befor ethe subprocess_info is about to
be freed. This can be used for freeing the argv and envp. The
Function must be runnable in either a process context or the
context in which call_usermodehelper_exec is called."
15818	413413	"call_usermodehelper_stdinpipe - set up a pipe to be used for stdin
@sub_info: a subprocess_info returned by call_usermodehelper_setup
@filp: set to the write-end of a pipe

This constructs a pipe, and sets the read end to be the stdin of the
subprocess, and returns the write-end in *@filp."
15851	414319	"call_usermodehelper_exec - start a usermode application
@sub_info: information about the subprocessa
@wait: wait for the application to finish and return status.
when -1 don't wait at all, but you get no useful error back when
the program couldn't be exec'ed. This makes it safe to call
from interrupt context.

Runs a user-space application. The application is started
asynchronously if wait is not set, and runs as a child of keventd.
(ie. it runs with full root capabilities)."
15871	414760	task has freed sub_info
15894	415384	"call_usermodehelper_pipe - call a usermode helper process with a pipe stdin
@path: path to usermode executable
@argv: arg vector for process
@envp: environment for process
@filp: set to the write-end of a pipe

This is a simple wrapper which executes a usermode-helper function
with a pipe as stdin. It is implemented entirely in terms of
lower-level call_usermodehelper_* functions."
15956	417556	"Kernel Probes (KProbes)
kernel/kprobes.c

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.



2002-Oct Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel
Probes initial implementation (includes suggestions from
Rusty Russell).
2004-Aug Updated by Prasanna S Panchamukhi <prasanna@in.ibm.com> with
hlists and exceptions notifier as suggested by Andi Kleen.
2004-July Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes
interface to access function arguments.
2004-Sep Prasanna S Panchamukhi <prasanna@in.ibm.com> Changed Kprobes
exceptions notifier to be first on the priority list.
2005-May Hien Nguyen <hien@us.ibm.com>, Jim Keniston
<jkenisto@us.ibm.com> and Prasanna S Panchamukhi
<prasanna@in.ibm.com> added function-return probes."
15982	418180	"Some oddball architectures like 64bit powerpc have function descriptors
so this must be overridable."
15991	418497	NOTE: change this value only with kprobe_mutex held
15995	418583	"Protects kprobe_table
Protects kretprobe_inst_table"
16004	418990	"kprobe->ainsn.insn points to the copy of the instruction to be
single-stepped. x86_64, POWER4 and above have no-exec support and
stepping on the instruction on a vmalloced/kmalloced/data page
is a recipe for disaster"
16009	419178	Page of instruction slots
16052	420073	"get_insn_slot() - Find a slot on an executable page for an instruction.
We allocate an executable page if there's no room on existing ones."
16069	420546	Surprise! No unused slots. Fix kip->nused.
16074	420651	If there are any garbage slots, collect it and try again.
16078	420796	All out of space. Need to allocate a new page. Use slot 0.
16087	421086	"Use module_alloc so this page is within +/- 2GB of where the
kernel image and loaded module images reside. This is required
so x86_64 can correctly handle the %rip-relative fixups."
16102	421462	Return 1 if all garbages are collected, otherwise 0.
16113	421806	"Page is no longer in use. Free it unless
it's the last one. We keep the last one
so as not to have to set it up again the
next time somebody inserts a probe."
16133	422227	Ensure no-one is preepmted on the garbages
16141	422448	we will collect all garbages
16176	423245	We have preemption disabled.. so it is safe to use __ versions
16192	423613	"This routine is called either:
- under the kprobe_mutex - during kprobe_[un]register()
OR
- with preemption disabled - from arch/xxx/kernel/kprobes.c"
16210	424040	"Aggregate handlers for multiple kprobes support - these handlers
take care of invoking the individual kprobe handlers on p->list"
16248	424925	"if we faulted ""during"" the execution of a user specified
probe handler, invoke just that probe's fault handler"
16269	425383	Walks the list and increments nmissed count for multiprobe case
16282	425650	Called with kretprobe_lock held
16286	425791	remove rp inst off the rprobe_inst_table
16289	425871	remove rp inst off the used list
16291	425941	put rp inst back onto the free list
16295	426059	Unregistering
16309	426525	"This function is called from finish_task_switch when task tk becomes dead,
so that we can recycle any function-return probe instances associated
with this task. These left over instances represent probed functions
that have been called but will never return."
16345	427386	Keep all fields in the kprobe consistent
16355	427715	"Add the new probe to old_p->list. Fail if this is the
second jprobe at the address - two jprobes can't coexist"
16373	428236	"Fill in the required fields of the ""manager kprobe"". Replace the
earlier kprobe in the hlist with the manager kprobe"
16395	428784	"This is the second or subsequent kprobe at the address - handle
the intricacies"
16435	429722	"If we have a symbol_name argument look it up,
and add it to the address. That way the addr
field can either be global or relative to a symbol."
16454	430123	Check if are we probing a module.
16462	430440	"We must allow modules to probe themself and in this case
avoid incrementing the module refcount, so as to allow
unloading of self probing modules."
16518	431590	kprobe p is a valid probe
16531	431978	"Only probe on the hash list. Disarm only if kprobes are
enabled - otherwise, the breakpoint would already have
been removed. We save on flushing icache."
16576	432912	we need to be notified first
16591	433226	Todo: Verify probepoint is a function entry point
16609	433642	"This kprobe pre_handler is registered with every kretprobe. When probe
hits it will set up the return probe."
16616	433899	TODO: consider to only swap the RA after the last pre_handler fired
16627	434230	XXX(hch): why is there no hlist_move_head?
16660	435056	Pre-allocate memory for max kretprobe instances
16681	435590	Establish function entry probe point
16688	435755	ARCH_SUPPORTS_KRETPROBES
16701	435978	ARCH_SUPPORTS_KRETPROBES
16711	436176	No race here
16726	436535	"FIXME allocate the probe table, currently defined statically
initialize all list heads"
16733	436780	lookup the function address from its name
16743	437097	By default, kprobes are enabled
16787	438126	Nothing to do
16843	439495	If kprobes are already enabled, just return
16870	440040	If kprobes are already disabled, just return
16885	440439	Allow all currently running kprobes to complete
16898	440687	"XXX: The debugfs bool file interface doesn't allow for callbacks
when the bool state is switched. We can reuse that facility when
available"
16971	442078	CONFIG_DEBUG_FS
17000	442744	"kernel/lockdep_proc.c

Runtime locking correctness validator

Started by Ingo Molnar:




Code for /proc/lockdep and /proc/lockdep_stats:"
17060	443855	Recurse this class's dependency list:
17074	444142	Recurse this class's dependency list:
17268	450315	"Total number of dependencies:

All irq-safe locks may nest inside irq-unsafe locks,
plus all the other known dependencies:"
17277	450545	"Estimated factor between direct and indirect
dependencies:"
17369	453317	sort on absolute number of contentions
17396	453939	XXX: do_div_signed
17428	454636	XXX truncates versions > 9
17646	459322	CONFIG_LOCK_STAT
17698	460776	"kernel/lockdep.c

Runtime locking correctness validator

Started by Ingo Molnar:




this code maps all the lock dependencies as they occur in a live kernel
and will warn about the following classes of locking bugs:

- lock inversion scenarios
- circular lock dependencies
- hardirq/softirq safe/unsafe locking bugs

Bugs are reported even if the current locking scenario does not cause
any deadlock at this point.

I.e. if anytime in the past two locks were taken in a different order,
even if it happened for another task, even if those were different
locks (but of the same class as this lock), this code will detect it.

Thanks to Arjan van de Ven for coming up with the initial idea of
mapping lock dependencies runtime."
17739	461746	"lockdep_lock: protects the lockdep graph, the hashes and the
class/list/hash allocators.

This is one of the rare exceptions where it's justified
to use a raw spinlock - we really dont want the spinlock
code to recurse back into the lockdep code..."
17750	462086	"Make sure that if another CPU detected a bug while
walking the graph we dont change it (while the other
CPU is busy printing out stuff with the graph lock
dropped already)"
17770	462463	"Turn lock debugging off and return with 0 if it was off already,
and also release the graph lock:"
17790	462939	"All data structures here are protected by the global debug_lock.

Mutex key structs only get allocated, once during bootup, and never
get freed - this significantly simplifies the debugging code."
17909	465719	"We keep a global list of all lock classes. The list only grows,
never shrinks. The list is only accessed with the lockdep
spinlock lock held."
17914	465825	The lockdep classes are in a hash-table as well, for fast lookup:
17925	466226	"We put the lock dependency chains into a hash-table as well, to cache
their existence:"
17938	466743	"The hash key of the lock dependency chains is a hash itself too:
it's a hash of all locks taken up to that lock, including that lock.
It's a 64-bit hash, because it's important for the keys to be
unique."
17960	467081	Debugging switches:
17976	467361	Quick filtering for interesting events:
17980	467434	Example
17988	467682	Filter everything else. 1 would be to allow everything else
18004	467918	"Stack-trace: tightly packed array of stack backtrace
addresses. Protected by the graph_lock."
18047	468996	"We cannot printk in early bootup code. Not even early_printk()
might work. So we mark any initialization errors and printk
about it later on, in lockdep_info()."
18057	469258	Various lockdep statistics:
18086	470146	Locking printouts:
18223	473393	printk all lock dependencies starting at <entry>:
18262	474190	Is this the address of a static object:
18274	474413	static variable?
18281	474507	percpu var?
18294	474778	module var?
18301	474925	"To make lock name printouts unique, we calculate a unique
class->name_version generation counter:"
18324	475574	"Register a lock's class in the hash-table, if the class is not present
yet. Otherwise we look it up. We cache the result in the lock object
itself, so actual lookup of the hash should be once per lock object."
18337	475951	"If the architecture calls into lockdep before initializing
the hashes then we'll warn about it later. (we cannot printk
right now)"
18348	476195	"Static locks do not have their class-keys yet - for them the key
is the lock object itself:"
18357	476513	"NOTE: the class-key must be unique. For dynamic locks, a static
lock_class_key variable is passed in through the mutex_init()
(or spin_lock_init()) call - which acts as the key. For static
locks we use the lock object itself as the key."
18368	476794	"We can walk the hash lockfree, because the hash only
grows, and we are careful when adding entries to the end:"
18383	477185	"Register a lock's class in the hash-table, if the class is not present
yet. Otherwise we look it up. We cache the result in the lock object
itself, so actual lookup of the hash should be once per lock object."
18398	477554	Debug-check: all keys must be persistent!
18420	478080	"We have to do the hash-walk again, to avoid races
with another CPU:"
18427	478263	"Allocate a new key from the static array, and add it to
the hash:"
18451	478980	"We use RCU's safe list-add method to make
parallel walking of the hash-list safe:"
18487	479710	"Allocate a lockdep entry. (assumes the graph_lock held, returns
with NULL on failure)"
18503	480093	Add a new dependency to the head of the list:
18511	480355	"Lock not present yet - get a new dependency struct and
add it to the list:"
18527	480810	"Since we never remove from the dependency list, the list can
be walked lockless by other CPUs, it's only allocation
that must be protected by the spinlock. But this also means
we must make new entries visible only once writes to the
entry become visible - hence the RCU op:"
18541	481177	"Recursive, forwards-direction lock-dependency checking, used for
both noncyclic checking and for hardirq-unsafe/softirq-unsafe
checking.

(to keep the stackframe of the recursive functions small we
use these global variables, and we also mark various helper
functions as noinline.)"
18547	481331	"Print a dependency chain entry (this is only done when a deadlock
has been detected):"
18564	481669	"When a circular dependency is detected, print the
header first:"
18628	483207	"Prove that the dependency graph starting at <entry> can not
lead to <target>. Print an error and return 0 if it does."
18641	483554	Check this lock's dependency list:
18657	484107	"Forwards and backwards subgraph searching, for the purposes of
proving that two subgraphs can be connected by a new dependency
without creating any illegal irq-safe -> irq-unsafe lock dependency."
18670	484517	"Find a node in the forwards-direction dependency sub-graph starting
at <source> that matches <find_usage_bit>.

Return 2 if such a node exists in the subgraph, and put that node
into <forwards_match>.

Return 1 otherwise and keep <forwards_match> unchanged.
Return 0 on error."
18690	484974	Check this lock's dependency list:
18709	485510	"Find a node in the backwards-direction dependency sub-graph starting
at <source> that matches <find_usage_bit>.

Return 2 if such a node exists in the subgraph, and put that node
into <backwards_match>.

Return 1 otherwise and keep <backwards_match> unchanged.
Return 0 on error."
18732	486047	Check this lock's dependency list:
18811	488446	fills in <backwards_match>
18820	488658	ret == 2
18834	489099	"Prove that the new dependency does not connect a hardirq-safe
lock with a hardirq-unsafe lock - to achieve this we search
the backwards-subgraph starting at <prev>, and the
forwards-subgraph starting at <next>:"
18844	489448	"Prove that the new dependency does not connect a hardirq-safe-read
lock with a hardirq-unsafe lock - to achieve this we search
the backwards-subgraph starting at <prev>, and the
forwards-subgraph starting at <next>:"
18854	489802	"Prove that the new dependency does not connect a softirq-safe
lock with a softirq-unsafe lock - to achieve this we search
the backwards-subgraph starting at <prev>, and the
forwards-subgraph starting at <next>:"
18863	490150	"Prove that the new dependency does not connect a softirq-safe-read
lock with a softirq-unsafe lock - to achieve this we search
the backwards-subgraph starting at <prev>, and the
forwards-subgraph starting at <next>:"
18932	491630	"Check whether we are holding such a class already.

(Note that this has to be done separately, because the graph cannot
detect such classes of deadlocks.)

Returns: 0 on deadlock detected, 1 on OK, 2 on recursive read"
18947	492034	"Allow read-after-read recursion of the same
lock class (i.e. read_lock(lock)+read_lock(lock)):"
18976	493120	"There was a chain-cache miss, and we are about to add a new dependency
to a previous lock. We recursively validate the following rules:

- would the adding of the <prev> -> <next> dependency create a
circular dependency in the graph? [== circular deadlock]

- does the new prev->next dependency connect any hardirq-safe lock
(in the full backwards-subgraph starting at <prev>) with any
hardirq-unsafe lock (in the full forwards-subgraph starting at
<next>)? [== illegal lock inversion with hardirq contexts]

- does the new prev->next dependency connect any softirq-safe lock
(in the full backwards-subgraph starting at <prev>) with any
softirq-unsafe lock (in the full forwards-subgraph starting at
<next>)? [== illegal lock inversion with softirq contexts]

any of these scenarios could lead to a deadlock.

Then if all the validations pass, we add the forwards and backwards
dependency."
18992	493645	"Prove that the new <prev> -> <next> dependency would not
create a circular dependency in the graph. (We do this by
forward-recursing into the graph starting at <next>, and
checking whether we can reach <prev>.)

We are using global variables to control the recursion, to
keep the stackframe size of the recursive functions low:"
19008	494165	"For recursive read-locks we do all the dependency checks,
but we dont store read-triggered dependencies (only
write-triggered dependencies). This ensures that only the
write-side dependencies matter, and that if for example a
write-lock never takes any other locks, then the reads are
equivalent to a NOP."
19018	494537	"Is the <prev> -> <next> dependency already present?

(this may occur even though this is a new chain: consider
e.g. the L1 -> L2 -> L3 -> L4 and the L5 -> L1 -> L2 -> L3
chains - the second one will be new, but L1 already has
L2 added to its dependency list, due to the first chain.)"
19030	494808	"Ok, all validations passed, add the new lock
to the previous lock's dependency list:"
19044	495124	Debugging printouts:
19063	495670	"Add the dependency to all directly-previous locks that are 'relevant'.
The ones that are relevant are (in increasing distance from curr):
all consecutive trylock entries and the final non-trylock entry - or
the end of this context's lock-chain - whichever comes first."
19074	495893	"Debugging checks.

Depth must not be zero for a non-head lock:"
19080	496001	"At least two relevant locks must exist for this
to be a head:"
19091	496285	"Only non-recursive-read entries get new dependencies
added:"
19100	496574	"Stop after the first non-trylock entry,
as non-trylock entries have added their
own direct dependencies already, so this
lock is connected to them indirectly:"
19107	496659	End of lock-stack?
19112	496749	Stop the search if we cross into another context:
19135	497273	"Look up a dependency chain. If the key is not present yet then
add it and return 1 - in this case the new dependency chain is
validated. If the key is already hashed, return 0.
(On return with 1 graph_lock is held.)"
19146	497582	"We can walk it lock-free, because entries only get added
to the hash:"
19165	498162	"Allocate a new chain entry from the static array, and add
it to the hash:"
19170	498269	We have to walk the chain again locked - to avoid duplicates:
19206	499324	"Trylock needs to maintain the stack of held locks, but it
does not add new dependencies, because trylock can be done
in any order.

We look up the chain_key and do the O(N^2) check and update of
the dependencies only if this is a new dependency chain.
(If lookup_chain_cache() returns with 1 it acquires
graph_lock for us)"
19220	499760	"Check whether last held lock:

- is irq-safe, if this lock is irq-unsafe
- is softirq-safe, if this lock is hardirq-unsafe

And check whether the new lock's dependency graph
could lead back to the previous lock.

any of these scenarios could lead to a deadlock. If
All validations"
19229	499981	"Mark recursive read, as we jump over it when
building dependencies (just like we jump over
trylock entries):"
19235	500140	"Add dependency only if this lock is not the head
of the chain, and if it's not a secondary read-lock:"
19241	500285	after lookup_chain_cache():
19259	500654	"We are building curr_chain_key incrementally, so double-check
it from scratch, to make sure that it's done correctly:"
19337	502833	Print out an error if an invalid bit is set:
19354	503279	print irq inversion bug:
19395	504607	"Prove that in the forwards-direction subgraph starting at <this>
there is no lock matching <mask>:"
19403	504812	fills in <forwards_match>
19414	505084	"Prove that in the backwards-direction subgraph starting at <this>
there is no lock matching <mask>:"
19422	505292	fills in <backwards_match>
19476	506745	"just marked it hardirq-safe, check that this lock
took no hardirq-unsafe lock in the past:"
19484	506977	"just marked it hardirq-safe, check that this lock
took no hardirq-unsafe-read lock in the past:"
19501	507444	"just marked it softirq-safe, check that this lock
took no softirq-unsafe lock in the past:"
19509	507676	"just marked it softirq-safe, check that this lock
took no softirq-unsafe-read lock in the past:"
19523	508066	"just marked it hardirq-read-safe, check that this lock
took no hardirq-unsafe lock in the past:"
19536	508442	"just marked it softirq-read-safe, check that this lock
took no softirq-unsafe lock in the past:"
19552	508906	"just marked it hardirq-unsafe, check that no hardirq-safe
lock in the system ever took it in the past:"
19561	509156	"just marked it hardirq-unsafe, check that no
hardirq-safe-read lock in the system ever took
it in the past:"
19578	509637	"just marked it softirq-unsafe, check that no softirq-safe
lock in the system ever took it in the past:"
19587	509887	"just marked it softirq-unsafe, check that no
softirq-safe-read lock in the system ever took
it in the past:"
19602	510315	"just marked it hardirq-read-unsafe, check that no
hardirq-safe lock in the system ever took it in the past:"
19617	510734	"just marked it softirq-read-unsafe, check that no
softirq-safe lock in the system ever took it in the past:"
19635	510992	Mark all held locks with a usage bit:
19667	511668	"Debugging helper: via this flag we know that we are in
'early bootup code', and will warn about any invalid irqs-on event:"
19682	511872	Hardirqs will be enabled:
19698	512242	we'll do an OFF -> ON transition:
19709	512531	"We are going to turn hardirqs on, so set the
usage bit for all held locks:"
19716	512727	"If we have softirqs enabled, then set the usage
bit for all held locks. (disabled hardirqs prevented
this bit from being set before)"
19730	512994	Hardirqs were disabled:
19744	513274	We have done an ON -> OFF transition:
19757	513565	Softirqs will be enabled:
19775	513877	We'll do an OFF -> ON transition:
19784	514152	"We are going to turn softirqs on, so set the
usage bit for all held locks, if hardirqs are
enabled too:"
19791	514246	Softirqs were disabled:
19805	514508	We have done an ON -> OFF transition:
19820	514955	"If non-trylock use in a hardirq or softirq context, then
mark the lock as used in these contexts:"
19870	516104	Keep track of points where we cross into an interrupt context:
19881	516436	"If we cross into another context, reset the
hash key (this also prevents the checking and the
adding of the dependency to 'prev'):"
19914	516960	Mark a lock with a usage bit, and validate the state transition:
19923	517202	"If already set then do not dirty the cacheline,
nor do any checks:"
19931	517334	Make sure we didnt race:
19958	517945	Add it to the global list of classes:
19973	518223	We must printk outside of the graph_lock:
19986	518449	Initialize a lock instance's lock-class mapping info:
19999	518758	Sanity check, the lock-class key must be persistent:
20020	519243	"This gets called for every mutex_lock*()/spin_lock*() operation.
We maintain the dependency maps and validate the locking attempt:"
20052	519984	Not cached yet or subclass?
20071	520479	"Add the lock to the list of currently held locks.
(we dont increase the depth just yet, up until the
dependency checks are done)"
20093	520975	mark it as used:
20106	521405	"Calculate the chain hash: it's the combined hash of all the
lock keys along the dependency chain. We save the hash value
at every step so that we can get the current hash easily
after unlock. The chain hash is then used to cache dependency
results.

The 'key ID' is what is the most compact key value to drive
the hash, not class->key."
20177	523118	Common debugging checks for both nested and non-nested unlock:
20197	523648	"Remove the lock to the list of currently held locks in a
potentially non-nested (out of order) manner. This is a
relatively rare operation, as all the unlock APIs default
to nested mode (which uses lock_release()):"
20209	523910	"Check whether the lock exists in the current stack
of held locks:"
20219	524129	We must not cross into another context:
20235	524564	"We have the right lock to unlock, 'hlock' points to it.
Now we remove it from the stack, and add back the other
entries (if any), recalculating the hash along the way:"
20258	525227	"Remove the lock to the list of currently held locks - this gets
called on mutex_unlock()/spin_unlock*() (or on a failed
mutex_lock_interruptible()). This is done for unlocks that nest
perfectly. (i.e. the current top of the lock-stack is unlocked)"
20267	525436	Pop off the top of the lock stack:
20273	525545	Is the unlock non-nested:
20299	526224	"Remove the lock to the list of currently held locks - this gets
called on mutex_unlock()/spin_unlock*() (or on a failed
mutex_lock_interruptible()). This is done for unlocks that nest
perfectly. (i.e. the current top of the lock-stack is unlocked)"
20321	526629	Check whether we follow the irq-flags state precisely:
20342	527203	"We dont accurately track softirq state in e.g.
hardirq contexts (such as on 4KSTACKS), so only
check if not in hardirq contexts:"
20358	527535	"We are not always called with irqs disabled - do that here,
and also avoid lockdep recursion:"
20448	529646	We must not cross into another context:
20491	530639	We must not cross into another context:
20565	532189	"Used by the testsuite, sanitize the validator state
after a simulated failure:"
20593	532769	"Remove all dependencies this lock is
involved in:"
20600	532968	Unhash the class and remove it from the all_lock_classes list:
20624	533454	Unhash all classes that were created by this module:
20654	534065	Remove all classes this lock might have:
20658	534175	If the class exists we look it up and zap it:
20666	534327	"Debug check: in the end all mapped classes should
be gone."
20696	534991	"Some architectures have their own start_kernel()
code which calls lockdep_init(), while we also
call lockdep_init() from the start_kernel() itself,
and we want to initialize the hashes only once:"
20772	537457	"Called when kernel memory is freed (or unmapped), or if a lock
is destroyed or reinitialized - this code checks whether there is
any held lock in the memory range of <from> to <to>:"
20839	539110	"Here we try to get the tasklist_lock as hard as possible,
if not successful after 2 seconds we ignore it (but keep
trying). This is to enable a debug printout even if a
tasklist_lock-holding task deadlocks or crashes."
20861	539563	"It's not reliable to print a task's held locks
if it's not sleeping (or if it's not the current
task):"
20923	541404	"This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA."
20939	541834	"markers_mutex nests inside module_mutex. Markers mutex protects the builtin
and module markers, the hash table and deferred_sync."
20949	542288	"Marker deferred synchronization.
Upon marker probe_unregister, we delay call to synchronize_sched() to
accelerate mass unregistration (only when there is no more reference to a
given module do we call synchronize_sched()). However, we need to make sure
every critical region has ended before we re-arm a marker that has been
unregistered and then registered back with a different probe data."
20955	542405	"Marker hash table, containing the active markers.
Protected by module_mutex."
20965	542648	"Number of times armed. 0 if disarmed.
Contains name'\0'format'\0'"
20980	543219	"__mark_empty_function - Empty probe callback
@mdata: pointer of type const struct marker
@fmt: format string
@...: variable argument list

Empty callback provided as a probe to the markers. By providing this to a
disabled marker, we make sure the execution flow is always valid even
though the function pointer change and the marker enabling are two distinct
operations that modifies the execution flow of preemptible code."
20991	543506	"Get marker if the marker is present in the marker hash table.
Must be called with markers_mutex held.
Returns NULL if not present."
21010	543947	"Add the marker to the marker hash table. Must be called with markers_mutex
held."
21029	544548	Already there
21035	544681	"Using kmalloc here to allocate a variable length element. Could
cause some memory fragmentation if overused."
21058	545229	"Remove the marker from the marker hash table. Must be called with mutex_lock
held."
21086	545797	Set the mark_entry format to the format found in the element.
21114	546579	Sets the probe callback corresponding to one marker.
21146	547415	"Disable a marker and its probe callback.
Note: only after a synchronize_sched() issued after setting elem->call to the
empty function insures that the original callback is not used anymore. This
insured by preemption disabling around the call site."
21155	547714	"Leave the private data and id there, because removal is racy and
should be done only after a synchronize_sched(). These are never used
until the next initialization anyway."
21166	548039	"marker_update_probe_range - Update a probe range
@begin: beginning of the range
@end: end of the range
@probe_module: module address of the probe being updated
@refcount: number of references left to the given probe_module (out)

Updates the probe callback corresponding to a range of markers."
21181	548447	ignore error, continue
21198	548871	"Update probes, removing the faulty probes.
Issues a synchronize_sched() when no reference to the module passed
as parameter is found in the probes so the probe module can be
safely unloaded from now on."
21203	548982	Core kernel markers
21206	549102	Markers in modules.
21223	549520	"marker_probe_register - Connect a probe to a marker
@name: marker name
@format: format string
@probe: probe handler
@private: probe private data

private data must be a valid allocated memory address, or NULL.
Returns 0 if ok, error value on error."
21257	550274	"marker_probe_unregister - Disconnect a probe from a marker
@name: marker name

Returns the private data given to marker_probe_register, or an ERR_PTR()."
21271	550583	In what module is the probe handler ?
21290	551149	"marker_probe_unregister_private_data - Disconnect a probe from a marker
@private: probe private data

Unregister a marker by providing the registered private data.
Returns the private data given to marker_probe_register, or an ERR_PTR()."
21316	551723	In what module is the probe handler ?
21336	552244	"marker_arm - Arm a marker
@name: marker name

Activate a marker. It keeps a reference count of the number of
arming/disarming done.
Returns 0 if ok, error value on error."
21350	552500	Only need to update probes when refcount passes from 0 to 1.
21367	552848	"marker_disarm - Disarm a marker
@name: marker name

Disarm a marker. It keeps a reference count of the number of arming/disarming
done.
Returns 0 if ok, error value on error."
21382	553142	"Only permit decrement refcount if higher than 0.
Do probe update only on 1 -> 0 transition."
21405	553722	"marker_get_private_data - Get a marker's probe private data
@name: marker name

Returns the private data pointer, or an ERR_PTR.
The private data pointer should _only_ be dereferenced if the caller is the
owner of the data, or its content could vanish. This is mostly used to
confirm that a caller is the owner of a registered probe."
21443	555024	"This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA"
21487	556060	If this is set, the section belongs in the init part of the module
21491	556216	"List of modules, protected by module_mutex or preempt_disable
(add/delete uses stop_machine)."
21510	556791	"We require a truly strong try_module_get(): 0 means failure due to
ongoing or failed initialization etc."
21527	557206	"A thread that wants to hold a reference to a module only while it
is running can call this to safely exit. nfsd and lockd use this."
21535	557389	Find a module section: 0 means not found.
21544	557619	"Alloc bit cleared means ""ignore it."""
21551	557774	Provided by the linker
21576	558992	lookup symbol in given range of kernel_symbols
21599	559760	Find a symbol, return value, crc and module which owns it
21608	559978	Core kernel first.
21660	561457	Now try modules.
21717	563092	Search for module by name: must hold module_mutex.
21730	563344	Number of blocks used and allocated.
21732	563442	Size of each block. -ve means used.
21737	563558	Reallocation required?
21750	563811	Insert a new subblock
21767	564103	Created by linker magic
21785	564583	Extra for alignment requirement.
21792	564792	Transfer extra to previous block.
21800	564956	Split block if warranted
21805	565078	Mark allocated
21820	565392	First entry is core kernel percpu data.
21830	565585	Merge with previous?
21838	565790	Merge with next?
21860	566369	Static in-kernel percpu data (used).
21862	566437	Free room.
21872	566660	... !CONFIG_SMP
21891	567125	pcpusec should be 0, and size of that section should be 0.
21894	567171	CONFIG_SMP
21927	568980	Init the unload section of the module.
21935	569214	Hold reference count during initialization.
21937	569334	Backwards compatibility macros put refcount during init.
21941	569395	modules using other modules
21948	569506	Does a already use b?
21963	569845	Module a uses b
21988	570436	Clear the unload stuff of the module.
22003	570883	There can be at most one match.
22023	571204	CONFIG_MODULE_FORCE_UNLOAD
22032	571340	Whole machine is stopped with interrupts off when this runs.
22037	571477	If it's not unused, quit unless we are told to block.
22043	571656	Mark it as dying.
22065	572146	This exists whether we can unload or not
22070	572317	Since we might sleep for some time, drop the semaphore first
22107	573135	Other modules depend on us: get rid of them first.
22112	573209	Doing init or already dying?
22115	573337	"FIXME: if (force), slam module count and wake up
waiter --RR"
22121	573479	If it has an init func, it must have an exit func to unload
22125	573602	This module can't be removed
22131	573685	Set this up before setting mod->state
22134	573778	Stop the machine so refcounts can't move and disable module.
22139	573880	Never wait if forced.
22143	574003	Final destruction now noone is using it.
22164	574479	"Always include a trailing , so userspace can differentiate
between this and the old multi-field proc format."
22221	575720	Maybe they're waiting for us to drop reference?
22229	575877	!CONFIG_MODULE_UNLOAD
22232	576014	We don't know the usage count, or what modules are using.
22248	576304	CONFIG_MODULE_UNLOAD
22296	577325	Exporting module didn't supply crcs? OK, we're already tainted.
22316	577871	Not in module's version table. OK, but that taints the kernel.
22337	578432	First part is kernel version, which we ignore.
22365	579066	CONFIG_MODVERSIONS
22368	579176	"Resolve a symbol for this module. I.e. if we find one, record usage.
Must be holding module_mutex."
22382	579573	"use_module can fail due to OOM,
or module initialization or unloading"
22394	579767	"/sys/module/foo/sections stuff
J. Corbet <corbet@lwn.net>"
22421	580543	Count loaded sections and allocate structures
22433	580929	Setup section attributes.
22472	581994	"We are positive that no one is using any sect attrs
at this point. Deallocate immediately."
22480	582145	/sys/module/foo/notes/.section.name gives contents of SHT_NOTE sections.
22494	582457	The caller checked the pos and count against our size.
22518	583070	Count notes sections and allocate structures.
22591	584685	CONFIG_KALLSYMS
22625	585518	pick a field to test for end of list
22666	586326	delay uevent until full sysfs population
22712	587335	"unlink the module with the whole machine is stopped with interrupts off
- this defends against kallsyms not taking locks"
22720	587511	Free a module, remove from lists, etc (must hold module_mutex).
22723	587590	Delete from various lists
22731	587792	Arch-specific cleanup.
22734	587847	Module unload stuff
22737	587913	This may be NULL, but that's OK
22743	588045	Free lock-classes:
22746	588169	Finally, free the core (containing the module structure)
22769	588678	"Ensure that an exported symbol [global namespace] does not already exist
in the kernel or in some other module's exported symbol table."
22799	589387	Change all symbols so that st_value encodes the pointer directly.
22816	589894	"We compiled with -fno-common. These are not
supposed to happen."
22824	590107	Don't need to do anything
22834	590337	Ok if resolved.
22837	590399	Ok if weak.
22847	590650	Divert to percpu allocation if a percpu var.
22860	590902	Update size with this section: return offset.
22873	591309	"Lay out the SHF_ALLOC sections in a way not dissimilar to how ld
might -- code, read-only data, read-write data, small data. Tally
sizes, and place the offsets into sh_entsize fields: high bit means it
belongs in init."
22882	591633	"NOTE: all executable code must be the first section
in this array; otherwise modify the text_size
finder in the two loops below"
22944	593382	Parse tag=value strings from .modinfo section
22947	593474	Skip non-zero chars
22954	593582	Skip any zero padding.
23005	594641	As per nm
23058	596059	Set types up while we still have access to sections.
23071	596379	CONFIG_KALLSYMS
23074	596506	"Allocate and load the module: note that size of section 0 is always
zero, and we rely on this for optional sections."
23107	597436	Stops spurious gcc warning
23117	597684	"Suck in entire file: we'll want most of it.
vmalloc barfs on ""unusual"" numbers. Check here"
23126	598001	"Sanity checks against insmoding binaries or wrong arch,
weird elf version"
23138	598296	Convenience variables
23149	598662	"Mark all sections sh_addr with their address in the
temporary image."
23152	598760	Internal symbols and strings.
23159	598975	Don't load .exit sections
23181	599511	Optional sections
23202	600744	Don't keep modinfo section
23205	600883	Keep symbol and string tables for decoding later.
23212	601102	Check module struct version now, before we try to use module.
23219	601317	This is allowed: modprobe --force will invalidate it.
23231	601680	Now copy in args
23245	601947	Allow arches to frob section contents and sizes.
23251	602117	We have a special allocation for this section.
23265	602546	"Determine total sizes, and put offsets in sh_entsize. For now
this is done generically; there doesn't appear to be any
special cases for the architectures."
23268	602618	Do the allocs.
23285	602987	Transfer each section which specifies SHF_ALLOC
23302	603486	Update sh_addr to point to copy in image.
23306	603642	Module has been moved.
23309	603746	Now we've moved module, initialize linked lists, etc.
23312	603824	Initialize kobject, so we can reference it.
23317	603932	Set up license info based on the info section
23325	604210	Set up MODINFO_ATTR fields
23328	604315	Fix up syms, so that st_value is a pointer to location.
23334	604505	Set up EXPORTed & EXPORT_GPLed symbols (section 0 is 0 length)
23375	606237	Now do relocations.
23380	606416	Not a valid relocation section?
23384	606507	Don't bother with non-allocated sections
23402	607010	Find duplicate symbols
23408	607118	Set up and sort exception table
23413	607344	Finally, copy percpu area over.
23428	607766	flush the icache in correct context
23436	608000	"Flush the instruction cache, since we've played with text.
Do it before processing of module parameters, so the module
can provide parameter accessor functions of its own."
23451	608454	Size of section 0 is 0, so this works well if no params
23471	609066	Size of section 0 is 0, so this works well if no unwind info.
23476	609223	Get rid of temporary copy
23479	609250	Done!
23507	609786	"link the module with the whole machine is stopped with interrupts off
- this defends against kallsyms not taking locks"
23515	609942	This is where the real work happens
23524	610108	Must have permission
23528	610203	Only one module load at a time, please
23532	610300	Do all the hard work
23540	610525	"Now sew it into the lists. They won't access us, since
strong_try_module_get() will fail."
23543	610611	Drop lock so they can recurse
23549	610748	Start the module
23554	610907	"Init routine failed: abort. Try to protect us from
buggy refcounters."
23564	611121	Now it's a first class citizen!
23567	611213	Drop initial reference.
23588	611705	"This ignores the intensely annoying ""mapping symbols"" found
in ARM ELF files: $a, $t and $d."
23603	612107	At worse, next value is at end of module
23610	612393	"Scan for closest preceeding symbol, and next symbol. (ELF
starts real symbols at 1)."
23616	612578	"We ignore unnamed symbols: they're uninformative
and inserted at a whim."
23642	613430	"For kallsyms to ask for address resolution. NULL means not found.
We don't lock, as this is used for oops resolution and races are a
lesser concern.
FIXME: Risky: returns a pointer into a module w/o lock"
23749	616046	Look for this name: can be of form module:name.
23756	616220	Don't lock: we're in enough trouble already.
23771	616578	CONFIG_KALLSYMS
23773	616646	Called by the /proc file system to return a list of modules.
23804	617328	"TAINT_FORCED_RMMOD: could be added.
TAINT_UNSAFE_SMP, TAINT_MACHINE_CHECK, TAINT_BAD_PAGE don't
apply to modules."
23821	617640	Informative for users.
23826	617830	Used by oprofile and other similar tools.
23829	617893	Taints info
23841	618147	"Format: modulename size refcount deps address

Where refcount is a number or -, and deps is a comma-separated list
of depends or -."
23849	618331	Given an address, look for it in the module exception tables.
23869	618849	"Now, if we found one, we are running inside it now, hence
we cannot unload the module, hence no refcnt needed."
23875	618905	Is this a valid module address?
23895	619207	Is this a valid kernel address?
23918	619691	Don't grab lock, we're oopsing.
23966	620685	Lookup built-in module entry in /sys/modules
23970	620853	remember our module structure
23972	620913	kset_find_obj took a reference
23980	621025	Don't check return codes; these calls are idempotent
24018	621904	Generate the signature for struct module here, too, for modversions.
24049	622657	"Notifier list for kernel code which wants to be called
at shutdown. This is used to stop any idling DMA operations
and the like."
24055	622837	"Notifier chain core routines. The exported routines below
are layered on top of these, with appropriate locking added."
24094	623886	"notifier_call_chain - Informs the registered notifiers about an event.
@nl: Pointer to head of the blocking notifier chain
@val: Value passed unmodified to notifier function
@v: Pointer passed unmodified to notifier function
@nr_to_call: Number of notifier functions to be called. Don't care
value of this parameter is -1.
@nr_calls: Records the number of notifications sent. Don't care
value of this field is NULL.
@returns: notifier_call_chain returns the value returned by the
last notifier function called."
24122	624517	"Atomic notifier chain routines. Registration and unregistration
use a spinlock, and call_chain is synchronized by RCU (no locks)."
24132	624779	"atomic_notifier_chain_register - Add notifier to an atomic notifier chain
@nh: Pointer to head of the atomic notifier chain
@n: New entry in notifier chain

Adds a notifier to an atomic notifier chain.

Currently always returns zero."
24154	625402	"atomic_notifier_chain_unregister - Remove notifier from an atomic notifier chain
@nh: Pointer to head of the atomic notifier chain
@n: Entry to remove from notifier chain

Removes a notifier from an atomic notifier chain.

Returns zero on success or %-ENOENT on failure."
24187	626610	"__atomic_notifier_call_chain - Call functions in an atomic notifier chain
@nh: Pointer to head of the atomic notifier chain
@val: Value passed unmodified to notifier function
@v: Pointer passed unmodified to notifier function
@nr_to_call: See the comment for notifier_call_chain.
@nr_calls: See the comment for notifier_call_chain.

Calls each function in a notifier chain in turn. The functions
run in an atomic context, so they must not block.
This routine uses RCU to synchronize with changes to the chain.

If the return value of the notifier can be and'ed
with %NOTIFY_STOP_MASK then atomic_notifier_call_chain()
will return immediately, with the return value of
the notifier function which halted execution.
Otherwise the return value is the return value
of the last notifier function called."
24211	627257	"Blocking notifier chain routines. All access to the chain is
synchronized by an rwsem."
24222	627563	"blocking_notifier_chain_register - Add notifier to a blocking notifier chain
@nh: Pointer to head of the blocking notifier chain
@n: New entry in notifier chain

Adds a notifier to a blocking notifier chain.
Must be called in process context.

Currently always returns zero."
24232	627855	"This code gets used during boot-up, when task switching is
not yet working and interrupts must remain disabled. At
such times we must not call down_write()."
24252	628458	"blocking_notifier_chain_unregister - Remove notifier from a blocking notifier chain
@nh: Pointer to head of the blocking notifier chain
@n: Entry to remove from notifier chain

Removes a notifier from a blocking notifier chain.
Must be called from process context.

Returns zero on success or %-ENOENT on failure."
24262	628752	"This code gets used during boot-up, when task switching is
not yet working and interrupts must remain disabled. At
such times we must not call down_write()."
24290	629812	"__blocking_notifier_call_chain - Call functions in a blocking notifier chain
@nh: Pointer to head of the blocking notifier chain
@val: Value passed unmodified to notifier function
@v: Pointer passed unmodified to notifier function
@nr_to_call: See comment for notifier_call_chain.
@nr_calls: See comment for notifier_call_chain.

Calls each function in a notifier chain in turn. The functions
run in a process context, so they are allowed to block.

If the return value of the notifier can be and'ed
with %NOTIFY_STOP_MASK then blocking_notifier_call_chain()
will return immediately, with the return value of
the notifier function which halted execution.
Otherwise the return value is the return value
of the last notifier function called."
24301	630179	"We check the head outside the lock, but if this access is
racy then it does not matter what the result of the test
is, we re-check the list after having taken the lock anyway:"
24322	630738	"Raw notifier chain routines. There is no protection;
the caller must provide it. Use at your own risk!"
24333	631033	"raw_notifier_chain_register - Add notifier to a raw notifier chain
@nh: Pointer to head of the raw notifier chain
@n: New entry in notifier chain

Adds a notifier to a raw notifier chain.
All locking must be provided by the caller.

Currently always returns zero."
24350	631554	"raw_notifier_chain_unregister - Remove notifier from a raw notifier chain
@nh: Pointer to head of the raw notifier chain
@n: Entry to remove from notifier chain

Removes a notifier from a raw notifier chain.
All locking must be provided by the caller.

Returns zero on success or %-ENOENT on failure."
24376	632544	"__raw_notifier_call_chain - Call functions in a raw notifier chain
@nh: Pointer to head of the raw notifier chain
@val: Value passed unmodified to notifier function
@v: Pointer passed unmodified to notifier function
@nr_to_call: See comment for notifier_call_chain.
@nr_calls: See comment for notifier_call_chain

Calls each function in a notifier chain in turn. The functions
run in an undefined context.
All locking must be provided by the caller.

If the return value of the notifier can be and'ed
with %NOTIFY_STOP_MASK then raw_notifier_call_chain()
will return immediately, with the return value of
the notifier function which halted execution.
Otherwise the return value is the return value
of the last notifier function called."
24395	633138	"SRCU notifier chain routines. Registration and unregistration
use a mutex, and call_chain is synchronized by SRCU (no locks)."
24406	633430	"srcu_notifier_chain_register - Add notifier to an SRCU notifier chain
@nh: Pointer to head of the SRCU notifier chain
@n: New entry in notifier chain

Adds a notifier to an SRCU notifier chain.
Must be called in process context.

Currently always returns zero."
24416	633714	"This code gets used during boot-up, when task switching is
not yet working and interrupts must remain disabled. At
such times we must not call mutex_lock()."
24436	634303	"srcu_notifier_chain_unregister - Remove notifier from an SRCU notifier chain
@nh: Pointer to head of the SRCU notifier chain
@n: Entry to remove from notifier chain

Removes a notifier from an SRCU notifier chain.
Must be called from process context.

Returns zero on success or %-ENOENT on failure."
24446	634589	"This code gets used during boot-up, when task switching is
not yet working and interrupts must remain disabled. At
such times we must not call mutex_lock()."
24475	635663	"__srcu_notifier_call_chain - Call functions in an SRCU notifier chain
@nh: Pointer to head of the SRCU notifier chain
@val: Value passed unmodified to notifier function
@v: Pointer passed unmodified to notifier function
@nr_to_call: See comment for notifier_call_chain.
@nr_calls: See comment for notifier_call_chain

Calls each function in a notifier chain in turn. The functions
run in a process context, so they are allowed to block.

If the return value of the notifier can be and'ed
with %NOTIFY_STOP_MASK then srcu_notifier_call_chain()
will return immediately, with the return value of
the notifier function which halted execution.
Otherwise the return value is the return value
of the last notifier function called."
24508	636746	"srcu_init_notifier_head - Initialize an SRCU notifier head
@nh: Pointer to head of the srcu notifier chain

Unlike other sorts of notifier heads, SRCU notifier heads require
dynamic initialization. Be sure to call this routine before
calling any of the other SRCU notifier routines for this head.

If an SRCU notifier head is deallocated, it must first be cleaned
up by calling srcu_cleanup_notifier_head(). Otherwise the head's
per-cpu data (used by the SRCU mechanism) will leak."
24527	637264	"register_reboot_notifier - Register function to be called at reboot time
@nb: Info about notifier function to be called

Registers a function with the list of functions
to be called at reboot time.

Currently always returns zero, as blocking_notifier_chain_register()
always returns zero."
24542	637682	"unregister_reboot_notifier - Unregister previously registered reboot notifier
@nb: Hook to be unregistered

Unregisters a previously registered reboot
notifier function.

Returns zero on success, or %-ENOENT on failure."
24594	639339	"Helpers for initial module or kernel cmdline parsing


This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA"
24634	640115	Find parameter
24653	640552	"You can use "" around spaces, but can't escape "".
Hyphens and underscores equivalent in parameter names."
24684	641147	Don't include quotes in value.
24700	641409	Chew up trailing spaces.
24706	641508	"Args looks like ""foo=bar,bar2 baz=fuz wiz""."
24717	641756	Chew leading spaces
24752	642549	All parsed OK.
24756	642587	Lazy bastard, eh?
24808	644174	"No equals means ""set""..."
24811	644221	One of =[yYnN01]
24825	644516	Y and N chosen as being relatively non-coder friendly
24846	644990	We break the rule and mangle the string.
24858	645304	Get the name right for errors.
24862	645361	No equals sign?
24869	645510	We expect a comma-separated list of values.
24880	645709	nul-terminate and parse
24952	647295	sysfs output in /sys/modules/XYZ/parameters/
24990	648108	sysfs always hands a nul-terminated string in buf. We rely on that.
25025	649123	"param_sysfs_setup - setup sysfs support for one module or KBUILD_MODNAME
@mk: struct module_kobject (contains parent kobject)
@kparam: array of struct kernel_param, the actual parameter definitions
@num_params: number of entries in array
@name_skip: offset where the parameter name start in kparam[].name. Needed for built-in ""modules""

Create a kobject for a (per-module) group of parameters, and create files
in sysfs. A pointer to the param_kobject is returned on success,
NULL if there's no parameter to export, or other ERR_PTR(err)."
25090	650741	"module_param_sysfs_setup - setup sysfs support for one module
@mod: module
@kparam: module parameters (array)
@num_params: number of module parameters

Adds sysfs entries for module parameters, and creates a link from
/sys/module/[mod->name]/parameters to /sys/parameters/[mod->name]/"
25111	651207	"module_param_sysfs_remove - remove sysfs support for one module
@mod: module

Remove sysfs entries for module parameters and the corresponding
kobject."
25118	651462	"We are positive that no one is using any param
attrs at this point. Deallocate immediately."
25127	651602	kernel_param_sysfs_setup - wrapper for built-in params support
25163	652747	"param_sysfs_builtin - add contents in /sys/parameters for built-in modules

Add module_parameters to sysfs for ""modules"" built into the kernel.

The ""module"" name (KBUILD_MODNAME) is stored before a dot, the
""parameter"" name is stored behind a dot in kernel_param->name. So,
extract the ""module"" name for all built-in kernel_param-eters,
and for all who have the same, call kernel_param_sysfs_setup."
25186	653360	new kbuild_modname?
25189	653503	add a new kobject for previous kernel_params.
25204	653794	last kernel_params need to be registered as well
25211	653922	module-related sysfs stuff
25283	655523	"Stupid empty release function to allow the memory for the kobject to
be properly cleaned up. This will not need to be present for 2.6.25
with the upcoming kobject core rework."
25293	655697	param_sysfs_init - wrapper for built-in params support
25372	658070	"Generic pidhash and scalable, time-bounded PID allocator

(C) 2002-2003 William Irwin, IBM
(C) 2004 William Irwin, Oracle
(C) 2002-2004 Ingo Molnar, Red Hat

pid-structures are backing objects for tasks sharing a given ID to chain
against. There is very little to them aside from hashing them and
parking tasks using given ID's on a list.

The hash is always changed with the tasklist_lock write-acquired,
and the hash is only accessed with the tasklist_lock at least
read-acquired, so there's no additional SMP locking needed here.

We have a list of bitmap pages, which bitmaps represent the PID space.
Allocating and freeing PIDs is completely lockless. The worst-case
allocation scenario when all but one out of 1 million PIDs possible are
allocated already: the scanning of 32 list entries and at most PAGE_SIZE
bytes. The typical fastpath is a single successful setbit. Freeing is O(1).

Pid namespaces:
(C) 2007 Pavel Emelyanov <xemul@openvz.org>, OpenVZ, SWsoft Inc.
(C) 2007 Sukadev Bhattiprolu <sukadev@us.ibm.com>, IBM
Many thanks to Oleg Nesterov for comments and help"
25415	659252	"PID-map pages start out as NULL, they get allocated upon
first use and are never deallocated. This way a low pid_max
value does not cause lots of bitmaps to be allocated, but
the scheme scales to up to 4 million PIDs, runtime."
25456	660329	"Note: disable interrupts while the pidmap_lock is held as an
interrupt might come in and do read_lock(&tasklist_lock).

If we don't disable interrupts there is a nasty deadlock between
detach_pid()->free_pid() and another cpu that does
spin_lock(&pidmap_lock) followed by an interrupt routine that does
read_lock(&tasklist_lock);

After we clean up the tasklist_lock and know there are no
irq handlers that take it we can leave the interrupts enabled.
For now it is easier to be safe than to prove it can't happen."
25486	661166	"Free the page if someone raced with us
installing it:"
25510	661832	"find_next_offset() found a bit, the pid from it
is in-bounds, and if we fell back to the last
bitmap block and the final block was the same
as the starting point, pid is before last_pid."
25571	663227	We can be called with write_lock_irq(&tasklist_lock) held
25664	665168	attach_pid() must be called with the tasklist_lock write-held.
25696	665800	transfer_pid is an optimization of attach_pid(new), detach_pid(old)
25719	666438	Must be called under rcu_read_lock() or with tasklist_lock read-held.
25819	668665	"Used by proc to find the first pid that is greater then or equal to nr.

If there is a pid at nr this function is exactly the same as find_pid."
25848	669198	"creates the kmem cache to allocate pids from.
@nr_ids: the number of numerical ids this pid will have to carry"
25970	671745	CONFIG_PID_NS
25989	672373	"The last thread in the cgroup-init thread group is terminating.
Find remaining pid_ts in the namespace, signal and wait for them
to exit.

Note: This signals each threads in the namespace - even those that
belong to the same thread group, To avoid this, we would have
to walk the entire tasklist looking a processes in this
namespace, but that could be unnecessarily expensive if the
pid namespace has just a few processes. Or we need to
maintain a tasklist for each pid namespace."
26004	672730	Child reaper for the pid namespace is going away
26013	672937	"The pid hash table is scaled according to the amount of memory in the
machine. From a minimum of 16 slots up to 4096 slots at one gigabyte or
more."
26037	673656	Reserve PID 0. We never call free_pidmap(0)
26050	673995	Implement CPU time clocks for the POSIX clock interface.
26084	674779	high half always zero when .cpu used
26142	676296	"Divide and limit the result to res >= 1

This is necessary to prevent signal delivery starvation, when the result of
the division would be rounded down to 0."
26153	676565	"Update expiry time from increment, and increase overrun count,
given the current clock sample."
26169	677009	Don't use (incr*2 < delta), incr*2 might overflow.
26187	677514	Don't use (incr*2 < delta), incr*2 might overflow.
26225	678573	"If sched_clock is using a cycle counter, we
don't have any idea of its true resolution
exported, but it is much more than 1s/HZ."
26237	678819	"You can never reset a CPU clock, but we check for other errors
in the call before failing with EPERM."
26248	678973	Sample a per-thread clock for the given task.
26272	679565	"Sample a process (thread group) clock for the given group_leader task.
Must be called with tasklist_lock held for reading.
Must be called with tasklist_lock held for reading, and p->sighand->siglock."
26297	680237	Add in each other live thread.
26310	680511	"Sample a process (thread group) clock for the given group_leader task.
Must be called with tasklist_lock held for reading."
26335	681182	"Special case constant value for our own clocks.
We don't have to do any lookup to find ourselves."
26339	681293	Sampling just ourselves we can do with no locking.
26352	681616	"Find the given PID, and validate that the caller
should be able to see it."
26385	682347	"Validate the clockid_t for a new CPU-clock timer, and initialize the timer.
This is called from sys_timer_create with the new timer already locked."
26433	683485	"Clean up a CPU-clock timer that is about to be destroyed.
This is called from timer deletion with the timer already locked.
If we return TIMER_RETRY, it's necessary to release the timer's lock
and try again. (This happens when the timer is in the middle of firing.)"
26445	683801	"We raced with the reaping of the task.
The deletion should have cleared us off the list."
26469	684364	"Clean out CPU timers still ticking when a thread exited. The task
pointer is cleared, and the expiry time is replaced with the residual
time for later timer_gettime calls to return.
This must be called with the siglock held."
26513	685587	"These are both called with the siglock held, when the current thread
is being reaped. When the final (leader) thread in the group is reaped,
posix_cpu_timers_exit_group will be called after posix_cpu_timers_exit."
26532	686167	"Set the expiry times of all the threads in the process so one of them
will go off before the process cumulative expiry total is reached."
26603	687971	"That's all for this thread or process.
We leave our residual in expires to be reported."
26615	688325	"Insert the timer on the appropriate list before any timers that
expire later. This must be called with the tasklist_lock held
for reading, and interrupts disabled."
26652	689347	"We are the new earliest-expiring timer.
If we are a thread timer, there can always
be a process timer telling us to stop earlier."
26682	690169	"For a process timer, we must balance
all the live threads' expirations."
26720	691103	The timer is locked, fire it and arrange for its reload.
26727	691298	"This a special case for clock_nanosleep,
not a normal timer from sys_timer_create."
26733	691481	One-shot timer. Clear it as soon as it's fired.
26742	691836	"The signal did not get queued because the signal
was ignored, so we won't get any callback to
reload the timer. But we need to keep it
ticking in case the signal is deliverable next time."
26752	692143	"Guts of sys_timer_settime for CPU timers.
This is called with the timer locked and interrupts disabled.
If we return TIMER_RETRY, it's necessary to release the timer's lock
and try again. (This happens when the timer is in the middle of firing.)"
26763	692445	Timer refers to a dead task's clock.
26774	692742	"We need the tasklist_lock to protect against reaping that
clears p->signal. If p has just been reaped, we can no
longer get any information about it at all."
26784	692949	Disarm any old timer after extracting its expiry time.
26804	693597	"We need to sample the current value to convert the new
value from to relative and absolute, and to convert the
old value from absolute to relative. To set a process
timer, we need a sample to balance the thread expiry
times (in arm_timer). With an absolute time, we must
check if it's already passed. In short, we need a sample."
26825	694157	"Update the timer in case it has
overrun already. If it has,
we'll report it as having overrun
and with the next reloaded timer
already ticking, though we are
swallowing that pending
notification here to install the
new setting."
26848	694783	"We are colliding with the timer actually firing.
Punt after filling in the timer's old value, and
disable this firing since we are already reporting
it as an overrun (thanks to bump_cpu_timer above)."
26861	695118	"Install the new expiry time (or zero).
For a timer with no notification action, we don't actually
arm the timer (we'll just fake it for timer_gettime)."
26874	695468	"Install the new reload setting, and
set up the signal and overrun bookkeeping."
26882	695718	"This acts as a modification timestamp for the timer,
so any automatic reload attempt will punt on seeing
that we have reset the timer manually."
26895	696170	"The designated time already passed, so we notify
immediately, even if the thread never runs to
accumulate more time on this clock."
26916	696545	Easy part: convert the reload time.
26920	696701	Timer not armed at all.
26929	696920	"This task already died and the timer will never fire.
In this case, expires is actually the dead value."
26938	697101	Sample the clock to take the difference with the expiry time.
26949	697443	"The process has been reaped.
We can't even collect a sample any more.
Call the timer disarmed, nothing else to do."
26970	698043	"Do-nothing timer expired and has no reload,
so it's as if it was never set."
26978	698235	"Account for any expirations and reloads that should
have happened."
26987	698428	"We've noticed that the thread is dead, but
not yet reaped. Take this opportunity to
drop our task ref."
27001	698820	"The timer should have expired already, but the firing
hasn't taken place yet. Say it's just about to expire."
27011	699108	"Check for any per-thread CPU timers that have fired and move them off
the tsk->cpu_timers[N] list onto the firing list. Here we update the
tsk->it_*_expires values to reflect the remaining thread CPU timers."
27067	700575	"Check for any per-thread CPU timers that have fired and move them
off the tsk->*_timers list onto the firing list. Per-thread timers
have already been taken off."
27080	700993	Don't sample the current process CPU clocks if there are no timers.
27091	701347	Collect the current process totals.
27150	702775	Check for the special case process timers.
27153	702921	ITIMER_PROF fires and reloads.
27169	703523	ITIMER_VIRTUAL fires and reloads.
27190	704243	"At the hard limit, we just die.
No need to calculate anything else now."
27197	704429	At the soft limit, send a SIGXCPU every second.
27217	704999	"Rebalance the threads' expiry times for the remaining
process CPU timers."
27270	706586	"This is called from the signal code (via do_schedule_next_timer)
when the last timer signal was delivered and we have to reload the timer."
27279	706809	The task was cleaned up already, no future firings.
27284	706896	Fetch the current sample and update the timer's expiry time.
27292	707154	arm_timer needs it.
27299	707325	"The process has been reaped.
We can't even collect a sample any more."
27309	707638	"We've noticed that the thread is dead, but
not yet reaped. Take this opportunity to
drop our task ref."
27315	707836	Leave the tasklist_lock locked for the call below.
27320	707889	Now re-arm for the new expiry time.
27336	708246	"This is called from the timer interrupt handler. The irq handler has
already updated our counts. We need to check if any timers fire now.
Interrupts are disabled."
27357	708738	Double-check with locks held.
27365	708993	"Here we take off tsk->cpu_timers[N] and tsk->signal->cpu_timers[N]
all the timers that are firing, and put them on the firing list."
27376	709453	"We must release these locks before taking any timer's lock.
There is a potential race with timer deletion here, as the
siglock now protects our private firing list. We have set
the firing flag in each timer, so that a deletion attempt
that gets the timer lock before we do will give it up and
spin until we've taken care of that timer below."
27386	709749	"Now that all the timers on our list have the firing flag,
noone will touch their list entries but us. We'll take
each timer's lock before clearing its firing flag, so no
timer call will interfere."
27397	710133	"The firing flag is -1 if we collided with a reset
of the timer, which already reported this
almost-firing as an overrun. So don't generate an event."
27411	710593	"Set one of the process-wide special case CPU timers.
The tasklist_lock and tsk->sighand->siglock must be held by the caller.
The oldval argument is null for the RLIMIT_CPU timer, where *newval is
absolute; non-null for ITIMER_*, where *newval is relative and we update
it to be absolute, *oldval is absolute and we update it to be relative."
27424	710989	Just about to fire.
27438	711304	"If the RLIMIT_CPU timer will expire before the
ITIMER_PROF timer, we have nothing else to do."
27447	711538	"Check whether there are any process timers already set to fire
before this one. If so, we don't have anything more to do."
27456	711848	"Rejigger each thread's expiry time so that one will
notice before we hit the process-cumulative expiry time."
27471	712215	Set up a temporary timer and then wait for it to go off.
27495	712807	Our timer fired and was reset.
27502	712934	Block until cpu_timer_fire (or a signal) wakes us.
27511	713117	We were interrupted by a signal.
27519	713372	It actually did fire already.
27539	713733	Diagnose required errors first.
27553	714113	Report back to the user the time still remaining.
27584	714975	Report back to the user the time still remaining.
27705	718667	"linux/kernel/posix-timers.c


2002-10-15 Posix Clocks & timers
by George Anzinger george@mvista.com



2004-06-01 Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.


This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

MontaVista Software | 1237 East Arques Avenue | Sunnyvale | CA 94085 | USA"
27709	718751	"These are all the functions necessary to implement
POSIX clocks & timers"
27744	720118	"Management arrays for POSIX timers. Timers are kept in slab memory
Timer ids are allocated by an external routine that keeps track of the
id and the timer. The external interface is:

void *idr_find(struct idr *idp, int id); to find timer_id <id>
int idr_get_new(struct idr *idp, void *ptr); to get a new id and
related it to <ptr>
void idr_remove(struct idr *idp, int id); to release <id>
void idr_init(struct idr *idp); to initialize <idp>
which we supply.
The idr_get_new *may* call slab for more memory so it must not be
called under a spin lock. Likewise idr_remore may release memory
(but it may be ok to do this under a lock...).
idr_find is just a memory look up and is quite fast. A -1 return
indicates that the requested id does not exist."
27748	720170	Lets keep our timers in a slab cache :-)
27756	720434	"we assume that the new SIGEV_THREAD_ID shares no bits with the other
SIGEV values. Here we put out an error if this assumption fails."
27770	720916	"The timer ID is turned into a timer address by idr_find().
Verifying a valid ID consists of:

a) checking that idr_find() returns other than -1.
b) checking that the timer id matches the one in the timer itself.
c) that the timer owner is in the callers thread group."
27810	723065	"CLOCKs: The POSIX standard calls for a couple of clocks and allows us
to implement others. This structure defines the various
clocks and allows the possibility of adding others. We
provide an interface to add clocks to the table and expect
the ""arch"" code to add at least one clock that is high
resolution. Here we define the standard CLOCK_REALTIME as a
1/HZ resolution clock.

RESOLUTION: Clock resolution is used to round up timer and interval
times, NOT to report clock times, which are reported with as
much resolution as the system can muster. In some cases this
resolution may depend on the underlying clock hardware and
may not be quantifiable until run time, and only then is the
necessary code is written. The standard says we should say
something about this issue in the documentation...

FUNCTIONS: The CLOCKs structure defines possible functions to handle
various clock functions. For clocks that use the standard
system timer code these entries should be NULL. This will
allow dispatch without the overhead of indirect function
calls. CLOCKS that depend on other sources (e.g. WWV or GPS)
must supply functions here, even if the function just returns
ENOSYS. The standard POSIX timer management code assumes the
following: 1.) The k_itimer struct (sched.h) is used for the
timer. 2.) The list, it_lock, it_clock, it_id and it_process
fields are not modified by timer code.

At this time all functions EXCEPT clock_nanosleep can be
redirected by the CLOCKS structure. Clock_nanosleep is in
there, but the code ignores it.

Permissions: It is assumed that the clock_settime() function defined
for each clock will take care of permission checks. Some
clocks may be set able by any user (i.e. local process
clocks) others not. Currently the only set able clock we
have is CLOCK_REALTIME and its high res counter part, both of
which we beg off on and pass to do_sys_settimeofday()."
27816	723155	These ones are defined below.
27835	723841	Call the k_clock hook function if non-null, or the default function.
27847	724284	"Default clock hook functions when the struct k_clock passed
to register_posix_clock leaves a function pointer null.

The function common_CALL is the default implementation for
the function pointer CALL in struct k_clock."
27859	724503	Get real time for posix timers
27880	724968	Return nonzero if we know a priori this clockid_t value is bogus.
27883	725098	CPU clock, posix_cpu_* will check it
27896	725343	Get monotonic time for posix timers
27905	725533	Initialize everything, well, just everything in Posix clocks/timers ;)
27955	727003	"This function is exported for use by the signal deliver code. It is
called just prior to the info block being released and passes that
block to us. It's function is to update the overrun entry AND to
restart the timer. It should only be called if the timer is to be
restarted (i.e. we have flagged this in the sys_private entry of the
info block).

To protect aginst the timer going away while the interrupt is queued,
we require that the it_requeue_pending flag be set."
27980	727614	Send signal to the process that owns this timer.
28013	728577	"This function gets called when a POSIX.1b interval timer expires. It
is used as a callback from the kernel internal timer. The
run_timer_list code ALWAYS calls with interrupts on.

This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers."
28032	729129	"signal was not sent because of sig_ignor
we will not get a call back to restart it AND
it should be restarted."
28057	730132	"FIXME: What we really want, is to stop this
timer completely and restart it in case the
SIG_IGN is removed. This is a non trivial
change which involves sighand locking
(sigh !), which we don't want to do late in
the release cycle.

For now we just let timers with an interval
less than a jiffie expire every jiffie to
avoid softirq starvation in case of SIG_IGN
and a very small interval, which would put
the timer right back on the softirq pending
list. By moving now ahead of time we trick
hrtimer_forward() to expire the timer
later, while we still maintain the overrun
accuracy, but have some inconsistency in
the timer_gettime() case. This is at least
better than a starved softirq. A more
complex fix which solves also another related
inconsistency is already in the pipeline."
28137	732124	Create a POSIX.1b interval timer.
28175	733036	"Wierd looking, but we return EAGAIN if the IDR is
full (proper POSIX return value for this)"
28191	733393	"return the timer_id now. The next step is hard to
back out if there is an error."
28222	734480	"We may be setting up this process for another
thread. It may be exiting. To catch this
case the we check the PF_EXITING flag. If
the flag is not set, the siglock will catch
him before it is too late (in exit_itimers).

The exec case is a bit more invloved but easy
to code. If the process is in our thread
group (and it must be or we would not allow
it here) and is doing an exec, it will cause
us to be killed. In this case it will wait
for us to die which means we can finish this
linkage with our last gasp. I.e. no code :)"
28257	735677	"In the case of the timer belonging to another task, after
the task is unlocked, the timer is owned by the other task
and may cease to exist at any time. Don't use or modify
new_timer after the unlock call."
28272	736098	"Locking issues: We need to protect the result of the id look up until
we get the timer locked down so it is not deleted under us. The
removal is done under the idr spinlock so we use that here to bridge
the find to the timer lock. To avoid a dead lock, the timer id MUST
be release with out holding the timer lock."
28280	736376	"Watch out here. We do a irqsave on the idr_lock and pass the
flags part over to the timer lock. Must not let interrupts in
while we are moving the lock."
28315	737528	"Get the time remaining on a POSIX.1b interval timer. This function
is ALWAYS called with spin_lock_irq on the timer, thus it must not
mess with irq.

We have a couple of messes to clean up here. First there is the case
of a timer that has a requeue pending. These timers should appear to
be in the timer list with an expiry as if we were to requeue them
now.

The second issue is the SIGEV_NONE timer which may be active but is
not really ever put in the timer list (to save system resources).
This timer may be expired, and if so, we will do it here. Otherwise
it is the same as a requeue pending timer WRT to what we should
report."
28326	737799	interval timer ?
28339	738146	"When a requeue is pending or this is a SIGEV_NONE
timer move the expiry time forward by intervals, so
expiry is > now."
28345	738438	Return 0 only, when the timer is expired and not pending
28350	738554	"A single shot SIGEV_NONE timer must return 0, when
it is expired !"
28357	738782	Get the time remaining on a POSIX.1b interval timer.
28387	739685	"Get the number of overruns of a POSIX.1b interval timer. This is to
be the overrun of the timer last delivered. At the same time we are
accumulating overruns on the next timer. The overrun is frozen when
the signal is delivered, either at the notify time (if the info block
is not queued) or at the actual delivery time (as we are informed by
the call back to do_schedule_next_timer(). So all we need to do is
to pick up the frozen overrun."
28406	739992	"Set a POSIX.1b interval timer.
timr->it_lock is taken."
28417	740309	disable the timer
28422	740492	"careful here. If smp we could be in the ""fire"" routine which will
be spinning as we hold the lock. But this is ONLY an SMP issue."
28430	740715	switch off the timer when it_value is zero
28440	741060	Convert interval
28443	741195	SIGEV_NONE timers are not queued ! See common_timer_get
28445	741316	Setup correct expiry time for relative timers
28456	741540	Set a POSIX.1b interval timer
28487	742357	We already got the old time...
28512	742859	Delete a POSIX.1b interval timer.
28535	743378	"This keeps any tasks waiting on the spin lock from thinking
they got something (see the lock code above)."
28548	743687	return timer owned by the process, used by exit_itimers
28564	744071	"This keeps any tasks waiting on the spin lock from thinking
they got something (see the lock code above)."
28577	744424	"This is called by do_exit or de_thread, only when there are no more
references to the shared signal_struct."
28588	744677	Not available / possible... functions
28600	745020	"aka ENOTSUP in userland for POSIX
parisc does define it separately."
28657	746280	nanosleep for monotonic and realtime clocks
28699	747187	nanosleep_restart for monotonic and realtime clocks
28708	747425	"This will restart clock_nanosleep. This is required only by
compat_clock_nanosleep_restart for now."
28734	748287	"linux/kernel/printk.c



Modified to make sys_syslog() more flexible: added commands to
return the last 4k of kernel messages, regardless of whether
they've been read or not. Added option to suppress kernel printk's
to the console. Added hook for sending the console messages
elsewhere, in preparation for a serial line console (someday).
Ted Ts'o, 2/11/93.
Modified for sysctl support, 1/8/97, Chris Horn.
Fixed SMP synchronization, 08/08/99, Manfred Spraul
manfred@colorfullife.com
Rewrote bits to get rid of console_lock
01Mar01 Andrew Morton <andrewm@uow.edu.au>"
28746	748603	For in_interrupt()
28759	748884	"printk's without a loglevel use this..
KERN_WARNING"
28763	748999	"We show everything that is MORE important than this..
Minimum loglevel we let people use
anything MORE serious than KERN_DEBUG"
28771	749267	"console_loglevel
default_message_loglevel
minimum_console_loglevel
default_console_loglevel"
28777	749575	"Low level drivers may need that to know if they can schedule in
their unblank() callback or not. So let's export it."
28785	749774	"console_sem protects the console_drivers list, and also
provides serialisation for access to the entire console
driver system."
28796	750249	"This is used for debugging the mess that is the VT code by
keeping track if we have the console semaphore held. It's
definitely not the perfect debug tool (we don't know if _WE_
hold it are racing, but it helps tracking those weird code
path in the console code where we end up in places I want
locked without the console sempahore held"
28803	750475	"logbuf_lock protects log_buf, log_start, log_end, con_start and logged_chars
It is also used in interesting ways to provide interlocking in
release_console_sem()."
28815	750719	"The indices into log_buf are not constrained to log_buf_len - they
must be masked before subscripting
Index into log_buf: next char to be read by syslog()
Index into log_buf: next char to be sent to consoles
Index into log_buf: most-recently-written-char + 1"
28819	751060	Array of consoles built from command line options (console=)
28824	751131	"Name of the driver
Minor dev. to use
Options for the driver"
28833	751441	Flag: console code may call schedule()
28841	751707	Number of chars produced since last read+clear operation
28887	752724	"msecs delay after each printk during bootup
per msec, based on boot_delay"
28894	752970	some guess
28927	753802	"use (volatile) jiffies to prevent
compiler reduction; loop termination via jiffies
is secondary and may or may not happen."
28941	754004	Return the number of unread characters in the log buffer.
28949	754115	Copy a range of characters from the log buffer.
28980	754617	Extract a single character from the log buffer.
29005	755254	"Commands to do_syslog:

0 -- Close the log. Currently a NOP.
1 -- Open the log. Currently a NOP.
2 -- Read from the log.
3 -- Read all messages remaining in the ring buffer.
4 -- Read and clear all messages remaining in the ring buffer
5 -- Clear ring buffer.
6 -- Disable printk's to console
7 -- Enable printk's to console
8 -- Set level of messages printed to console
9 -- Return number of unread characters in the log buffer
10 -- Return size of the log buffer"
29018	755491	Close log
29020	755525	Open log
29022	755564	Read from log
29053	756234	Read/clear last kernel messages
29056	756268	"FALL THRU
Read last kernel messages"
29081	756891	"__put_user() could sleep, and while we sleep
printk() could overwrite the messages
we try to copy to user space. Therefore
the messages are copied in reverse. <manfreds>"
29098	757323	buffer overflow during copy, correct user buffer.
29109	757541	Clear ring buffer
29112	757613	Disable logging to console
29115	757711	Enable logging to console
29118	757824	Set level of messages printed to console
29127	758053	Number of chars in the log buffer
29130	758132	Size of the log buffer
29148	758389	Call the console drivers on a range of log_buf
29175	759021	Write out chars from start to end - 1 inclusive
29182	759298	wrapped write
29196	759609	"Call the console drivers, asking them to write out
log_buf[start] to log_buf[end - 1].
The console_sem must be held."
29227	760478	"printk() has already given us loglevel tags in
the buffer. This code is here in case the
log buffer has wrapped right round and scribbled
on those tags"
29256	761146	"Zap console related locks when oopsing. Only zap at most once
every 10 seconds, to leave time for slow consoles to print a
full oops."
29267	761412	If a crash is occurring, make sure we can't deadlock
29269	761490	And make sure that we print immediately
29298	762161	Check if we have any console registered that can be called early in boot.
29331	763264	"printk - print a kernel message
@fmt: format string

This is printk(). It can be called from any context. We want it to work.
Be aware of the fact that if oops_in_progress is not set, we might try to
wake klogd up which could deadlock on runqueue lock if printk() is called
from scheduler code.

We try to grab the console_sem. If we succeed, it's easy - we log the output and
call the console drivers. If we fail to get the semaphore we place the output
into the log buffer and return. The current holder of the console_sem will
notice the new output in release_console_sem() and will send it to the
consoles before releasing the semaphore.

One effect of this deferred printing is that code which calls printk() and
then changes console_loglevel may break. This is because console_loglevel
is inspected when the actual printing occurs.

See also:
printf(3)"
29345	763452	cpu currently holding logbuf_lock
29361	763882	"If a crash is occurring during printk() on this CPU,
make sure we can't deadlock"
29364	763965	This stops the holder of console_sem just where we want him
29370	764119	Emit the output into the temporary buffer
29376	764315	"Copy the output into log_buf. If the caller didn't provide
appropriate log level tags, we insert them here"
29379	764455	log_level_unknown signals the start of a new line
29390	764684	"force the log level token to be
before the time output."
29434	765683	"We own the drivers. We can drop the spinlock and
let release_console_sem() print the text, maybe ..."
29444	765983	"Console drivers may assume that per-cpu resources have
been allocated. So unless they're explicitly marked as
being able to cope (CON_ANYTIME) don't call them until
this CPU is officially up."
29449	766171	Release by hand to avoid flushing the buffer.
29460	766474	"Someone else owns the drivers. We drop the spinlock, which
allows the semaphore holder to proceed and to call the
console drivers with the output which we just produced."
29488	766911	Set up a list of consoles. Called from init/main.c
29491	767022	4 for index
29497	767104	Decode str into name, index, options.
29536	768247	"add_preferred_console - add a device to the list of preferred consoles.
@name: device name
@idx: device index
@options: options for this console

The last preferred console added will be used for kernel messages
and stdin/out/err for init. Normally this is used by console_setup
above to handle user-supplied console arguments; however it can also
be used by arch-specific code either to override the user or more
commonly to provide a default console (ie from PROM variables) when
the user has not supplied one."
29545	768432	"See if this tty is not yet registered, and
if we have a slot free."
29578	769364	not found
29596	769728	"suspend_console - suspend the console subsystem

This disables printk() while we go into suspend states"
29621	770251	"acquire_console_sem - lock the console system for exclusive use.

Acquires a semaphore which guarantees that the caller has
exclusive access to the console system and the console_drivers list.

Can sleep, returns nothing."
29669	771322	"release_console_sem - unlock the console system

Releases the semaphore which the caller holds on the console system
and the console driver list.

While the semaphore was held, console output may have been buffered
by printk(). If this is the case, release_console_sem() emits
the output prior to releasing the semaphore.

If there is output waiting for klogd, we wake it up.

release_console_sem() may be called from any context."
29687	771701	Nothing to print
29690	771785	Flush
29711	772300	"console_conditional_schedule - yield the CPU if required

If the console code is currently allowed to sleep, and
if this CPU should yield the CPU to another task, do
so here.

Must be called within acquire_console_sem()."
29732	772707	"console_unblank can no longer be called in interrupt context unless
oops_in_progress is set to 1.."
29749	773080	Return the console tty driver structure and its associated index
29771	773589	"Prevent further output on the passed console device so that (for example)
serial drivers can disable console output before suspending a port, and can
re-enable output afterwards."
29793	774150	"The console driver calls this routine during kernel initialization
to register the console printing procedure with printk() and to
print any messages that were printed by the kernel before the
console driver was initialized."
29817	774709	"See if we want to use this console driver. If we
didn't select a console we take the first one
that registers here."
29831	775028	"See if this console matches one we selected on
the command line."
29870	776176	"Put this console in the list - keep the
preferred driver at the head of the list."
29885	776610	"release_console_sem() will print out the buffered messages
for us."
29917	777305	"If this isn't the last console and it has CON_CONSDEV set, we
need to set it on the next preferred console."
29947	778140	"tty_write_message - write a message to a certain tty, not just the console.
@tty: the destination tty_struct
@msg: the message to write

This is used for messages that need to be redirected to a specific tty.
We don't put it into the syslog queue right now maybe in the future if
really needed."
29961	778510	"printk rate limiting, lifted from the networking subsystem.

This enforces a rate limit: not more than one kernel message
every printk_ratelimit_jiffies to make a denial-of-service
attack impossible."
29992	779349	minimum time in jiffies between messages
29995	779442	number of messages we send before ratelimiting
30013	779960	"printk_timed_ratelimit - caller-controlled printk ratelimiting
@caller_jiffies: pointer to caller's state
@interval_msecs: minimum interval between prints

printk_timed_ratelimit() returns true if more than @interval_msecs
milliseconds have elapsed since the last time printk_timed_ratelimit()
returned true."
30038	780905	"linux/kernel/profile.c
Simple profiling. Manages a direct-mapped profile hit count buffer,
with configurable resolution, support for restricting the cpus on
which profiling is done, and switching between cpu time and
schedule() calls via kernel command line parameters passed at boot.

Scheduler profiling support, Arjan van de Ven and Ingo Molnar,
Red Hat, July 2004
Consolidation of architecture support code for profiling,
William Irwin, Oracle, July 2004
Amortized hit count accounting via per-cpu open-addressed hashtables
to resolve timer interrupt livelocks, William Irwin, Oracle, 2004"
30063	781532	Oprofile timer tick hook
30077	781963	CONFIG_SMP
30099	782586	CONFIG_SCHEDSTATS
30134	783468	only text is profiled
30139	783607	Profile event notifications
30224	785419	"make sure all CPUs see the NULL hook
Allow ongoing interrupts to complete."
30234	785776	CONFIG_PROFILING
30268	787479	"Each cpu has a pair of open-addressed hashtables for pending
profile hits. read_profile() IPI's all cpus to request them
to flip buffers and flushes their contents to prof_buffer itself.
Flip requests are serialized by the profile_flip_mutex. The sole
use of having a second hashtable is for avoiding cacheline
contention that would otherwise happen during flushes of pending
profile hits required for the accuracy of reported profile hits
and so resurrect the interrupt livelock issue.

The open-addressed hashtables are indexed by profile buffer slot
and hold the number of pending hits to that profile buffer slot on
a cpu in an entry. When the hashtable overflows, all pending hits
are accounted to their corresponding profile buffer slots with
atomic_add() and the hashtable emptied. As numerous pending hits
may be accounted to a profile buffer slot in a hashtable entry,
this amortizes a number of atomic profile buffer increments likely
to be far larger than the number of entries in the hashtable,
particularly given that the number of distinct profile buffer
positions to which hits are accounted during short intervals (e.g.
several seconds) is usually very small. Exclusion from buffer
flipping is provided by interrupt disablement (note that for
SCHED_PROFILING or SLEEP_PROFILING profile_hit() may be called from
process context).
The hash function is meant to be lightweight as opposed to strong,
and was vaguely inspired by ppc64 firmware-supported inverted
pagetable hash functions, but uses a full hashtable full of finite
collision chains, not just pairs of them.

-- wli"
30335	789324	"We buffer the global profiler buffer into a per-CPU
queue and thus reduce the number of global (and possibly
NUMA-alien) accesses. The write-queue is self-coalescing:"
30354	789751	"Add the current hit(s) and flush the write-queue out
to the global buffer:"
30421	791479	!CONFIG_SMP
30435	791905	!CONFIG_SMP
30483	793038	create /proc/irq/prof_cpu_mask
30496	793510	"This function accesses profiling information. The returned data is
binary: the sampling step and the actual contents of the profile
buffer. Use of the program readprofile is recommended in order to
get meaningful info out of these data."
30530	794406	"Writing to /proc/profile resets the counters

Writing a 'profiling multiplier' value into it also re-sets the profiling
interrupt frequency, on architectures that support this."
30626	796626	CONFIG_PROC_FS
30635	796814	"linux/kernel/ptrace.c

(C) Copyright 1999 Linus Torvalds

Common interfaces for ""ptrace()"" which we do not want
to continually duplicate across every architecture."
30659	797364	"ptrace a task: make the debugger its new parent and
move it to the ptrace list.

Must be called with the tasklist lock write-held."
30677	797966	"Turn a tracing stop into a normal stop now, since with no tracer there
would be no way to wake it up with SIGCONT or SIGKILL. If there was a
signal sent that would resume the child, but didn't because it was in
TASK_TRACED, resume it now.
Requires that irqs be disabled."
30696	798421	"unptrace a task: move it back to its original parent and
remove it from the ptrace list.

Must be called with the tasklist lock write-held."
30715	798805	Check that we have indeed attached to the thing..
30726	799211	"We take the read lock around doing both checks to close a
possible race where someone else was tracing our child and
detached between these two checks. After this locked check,
we are sure that this is our traced child and that can only
be changed by us so it's not changing right after this."
30746	799763	All systems go..
30759	800161	"May we inspect the given task?
This check is used both for attaching with ptrace
and for allowing access to sensitive information in /proc.

ptrace_attach denies several cases that /proc allows
because setting up the necessary parent/child relationship
or halting the specified task is impossible."
30761	800233	Don't let security modules deny introspection
30811	801328	"Nasty, nasty.

We want to hold both the task-lock and the
tasklist_lock for writing at the same time.
But that's against the rules (tasklist_lock
is taken for reading by interrupts on other
cpu's that may have task_lock)."
30823	801590	the same process cannot be attached many times
30830	801707	Go
30850	802162	.. re-parent ..
30852	802213	.. and wake it up.
30862	802438	Architecture-specific hardware disable ..
30867	802597	protect against de_thread()->release_task()
31017	806007	detach a process that was attached.
31032	806259	"ptrace_traceme -- helper for PTRACE_TRACEME

Performs checks and sets PT_PTRACED.
Should be used by all ptrace implementations for PTRACE_TRACEME."
31039	806348	Are we already being traced?
31045	806524	Set the ptrace bit in the process ptrace flags.
31062	806982	"ptrace_get_task_struct -- grab a task struct reference for ptrace
@pid: process id to grab a task_struct reference of

This function is a helper for ptrace implementations. It checks
permissions and then grabs a task struct for use of the actual
ptrace implementation.

Returns the task_struct for @pid or an ERR_PTR() on failure."
31069	807109	Tracing init is not allowed.
31096	807630	This lock_kernel fixes a subtle race with suid exec
31114	807972	"Some architectures need to do book-keeping after
a ptrace attach."
31134	808356	__ARCH_SYS_PTRACE
31186	810166	"Read-Copy Update mechanism for mutual exclusion

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.



Authors: Dipankar Sarma <dipankar@in.ibm.com>
Manfred Spraul <manfred@colorfullife.com>

Based on the original work by Paul McKenney <paulmck@us.ibm.com>
and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
Papers:
http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)

For detailed explanation of Read-Copy Update mechanism see -
http://lse.sourceforge.net/locking/rcupdate.html"
31213	810867	Definition for rcupdate control block.
31230	811341	Fake initialization required by compiler
31252	811923	"Don't send IPI to itself. With irqs disabled,
rdp->cpu is the current cpu."
31277	812691	"call_rcu - Queue an RCU callback for invocation after a grace period.
@head: structure to be used for queueing the RCU updates.
@func: actual update function to be invoked after the grace period

The update function will be invoked some time after a full grace
period elapses, in other words after all currently executing RCU
read-side critical sections have completed. RCU read-side critical
sections are delimited by rcu_read_lock() and rcu_read_unlock(),
and may be nested."
31312	814040	"call_rcu_bh - Queue an RCU for invocation after a quicker grace period.
@head: structure to be used for queueing the RCU updates.
@func: actual update function to be invoked after the grace period

The update function will be invoked some time after a full grace
period elapses, in other words after all currently executing RCU
read-side critical sections have completed. call_rcu_bh() assumes
that the read-side critical sections end on completion of a softirq
handler. This means that read-side critical sections in process
context must not be interrupted by softirqs. This interface is to be
used when most of the read-side critical sections are in softirq context.
RCU read-side critical sections are delimited by rcu_read_lock() and
rcu_read_unlock(), * if in interrupt context or rcu_read_lock_bh()
and rcu_read_unlock_bh(), if in process context. These may be nested."
31337	814576	"Return the number of RCU batches processed thus far. Useful
for debug and statistics."
31346	814745	"Return the number of RCU batches processed thus far. Useful
for debug and statistics."
31360	815046	Called with preemption disabled, and from cross-cpu IRQ context.
31374	815372	rcu_barrier - Wait until all the in-flight RCUs are complete.
31378	815482	Take cpucontrol mutex to protect against CPU hotplug
31391	815851	"Invoke the completed RCU callbacks. They are expected to be in
a per-cpu list."
31437	817300	"Grace period handling:
The grace period handling consists out of two steps:
- A new grace period is started.
This is done by rcu_start_batch. The start is not broadcasted to
all cpus, they must pick this up by comparing rcp->cur with
rdp->quiescbatch. All cpus are recorded in the
rcu_ctrlblk.cpumask bitmap.
- All cpus must go through a quiescent state.
Since the start of the grace period is not broadcasted, at least two
calls to rcu_check_quiescent_state are required:
The first call just notices that a new grace period is running. The
following calls check if there was a quiescent state since the beginning
of the grace period. If so, it updates rcu_ctrlblk.cpumask. If
the bitmap is empty, then the grace period is completed.
rcu_check_quiescent_state calls rcu_start_batch(0) to start the next grace
period (if necessary)."
31442	817497	"Register a new batch of callbacks, and start it up if there is currently no
active batch and the batch to be registered has not already occurred.
Caller must hold rcu_ctrlblk.lock."
31451	817756	"next_pending == 0 must be visible in
__rcu_process_callbacks() before it can see new value of cur."
31460	818005	"Accessing nohz_cpu_mask before incrementing rcp->cur needs a
Barrier Otherwise it can cause tickless idle CPUs to be
included in rcp->cpumask, which will extend graceperiods
unnecessarily."
31472	818344	"cpu went through a quiescent state since the beginning of the grace period.
Clear it from the cpu mask and complete the grace period if it was the last
cpu. Start another grace period if someone has further entries pending"
31477	818492	batch completed !
31487	818744	"Check if the cpu has gone through a quiescent state (say context
switch). If so and if it already hasn't done so in this RCU
quiescent cycle, then indicate that it has done so."
31492	818905	start new grace period:
31502	819141	"Grace period already completed for this cpu?
qs_pending is checked instead of the actual bitmap to avoid
cacheline trashing."
31509	819307	"Was there a quiescent state since the beginning of the grace
period? If no, then exit and wait for the next call."
31518	819521	"rdp->quiescbatch/rcp->cur and the cpu bitmap can come out of sync
during cpu startup. Ignore the quiescent state."
31531	819862	"warning! helper for rcu_offline_cpu. do not use elsewhere without reviewing
locking requirements, the list it's pulling from has to belong to a cpu
which is dead and hence not processing interrupts."
31548	820315	"if the cpu going offline owns the grace period
we can block indefinitely waiting for it, so flush
it here"
31582	821147	This does the RCU processing work from tasklet context.
31603	821682	start the next batch of callbacks
31605	821714	determine batch number
31609	821824	"see the comment and corresponding wmb() in
the rcu_start_batch()"
31613	821923	and start it/schedule start if it's a new batch
31636	822482	"This cpu has pending rcu entries and the grace period
for them has completed."
31640	822629	This cpu has no pending entries, but there are new entries
31644	822727	This cpu has finished callbacks to invoke
31648	822821	The rcu core waits for a quiescent state from the cpu
31652	822909	nothing to do
31660	823142	"Check to see if there is any immediate RCU-related work to be done
by the current CPU, returning 1 if so. This function is part of the
RCU implementation; it is -not- an exported member of the RCU API."
31672	823557	"Check to see if any future RCU-related work will need to be done
by the current CPU, even if none need be done immediately, returning
1 if so. This function is part of the RCU implementation; it is -not-
an exported member of the RCU API."
31744	825399	"Initializes rcu mechanism. Assumed to be called early.
That is before local timer(SMP) or jiffie timer (uniproc) is setup.
Note that rcu_qsctr and friends are implicitly
initialized due to the choice of ``0'' for RCU_CTR_INVALID."
31749	825550	Register notifier for non-boot CPUs
31758	825740	Because of FASTCALL declaration of complete, we use this wrapper
31778	826386	"synchronize_rcu - wait until a grace period has elapsed.

Control will return to the caller some time after a full grace
period has elapsed, in other words after all currently executing RCU
read-side critical sections have completed. RCU read-side critical
sections are delimited by rcu_read_lock() and rcu_read_unlock(),
and may be nested.

If your read-side code is not protected by rcu_read_lock(), do -not-
use synchronize_rcu()."
31784	826519	Will wake me after RCU finished
31787	826579	Wait for it
31823	827875	"Read-Copy Update module-based torture test facility

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.



Authors: Paul E. McKenney <paulmck@us.ibm.com>
Josh Triplett <josh@freedesktop.org>

See also: Documentation/RCU/torture.txt"
31859	828691	"# reader threads, defaults to 2*ncpus
# fake writer threads
Interval between stats, in seconds.
Defaults to ""only at end of test"".
Print more debug info.
Test RCU's support for tickless idle CPUs.
Interval between shuffles (in sec)
What RCU implementation to torture."
31902	830807	stop generating callbacks at test end.
31922	831581	Allocate an element from the rcu_tortures pool.
31943	832064	Free an element to the rcu_tortures pool.
31959	832399	"prime
prime"
31967	832682	"Crude but fast random-number generator. Uses a linear congruential
generator, with occasional help from cpu_clock()."
31982	833070	Operations vector for selecting different types of tests.
32000	833476	Definitions for rcu torture testing.
32013	833740	We want there to be long-running readers, but not all the time.
32038	834208	"Test is ending, just drop callbacks on the floor.
The next initialization will pick up the pieces."
32111	836003	Definitions for rcu_bh torture testing.
32160	837100	just reuse rcu's version.
32173	837507	just reuse rcu's version.
32184	837767	Definitions for srcu torture testing.
32211	838323	We want there to be long-running readers, but not all the time.
32265	839618	Definitions for sched torture testing.
32292	840090	just reuse rcu's version.
32305	840497	"RCU torture writer kthread. Repeatedly substitutes a new structure
for that pointed to by rcu_torture_current, freeing the old structure
after a series of grace periods (the ""pipeline"")."
32348	841634	"RCU torture fake writer kthread. Repeatedly calls sync, with a random
delay between calls."
32374	842383	"RCU torture reader kthread. Repeatedly dereferences rcu_torture_current,
incrementing the corresponding element of the pipeline array. The
counter in the element should never be greater than 1, otherwise, the
RCU implementation is broken."
32392	842796	Wait for rcu_torture_writer to get underway
32403	843112	Should not happen, but...
32409	843329	Should not happen, but...
32425	843750	Create an RCU-torture statistics message in the specified buffer.
32489	846065	"Print torture statistics. Caller must ensure that there is only
one call to this function at a given time!!! This is normally
accomplished by relying on the module system to only have one copy
of the module loaded, and then by giving the rcu_torture_stats
kthread full control (or the init/cleanup functions when rcu_torture_stats
thread is not running)."
32505	846457	"Periodically prints torture statistics, if periodic statistics printing
was specified via the stat_interval module parameter.

No need to worry about fullstop here, since this one doesn't reference
volatile state or register callbacks."
32518	846822	Force all torture tasks off this CPU
32522	846983	"Shuffle tasks such that we allow @rcu_idle_cpu to become idle. A special case
is when @rcu_idle_cpu = -1, when we allow the tasks to run on all CPUs."
32530	847166	No point in shuffling if there is only one online CPU (ex: UP)
32570	848071	"Shuffle tasks across CPUs, with the intent of allowing each CPU in the
system to become idle at a time and cut off its timer ticks. This is meant
to test the support for such tickless idle CPU in RCU."
32645	849840	Wait for all RCU callbacks to fire.
32648	849929	-After- the stats thread is stopped!
32668	850431	Process args and tell the world that the torturer is on the job.
32680	850804	"no ""goto unwind"" prior to this point!!!"
32689	850992	Set up the freelist.
32698	851273	Initialize the statistics so that each run gets its own numbers.
32716	851827	Start up the kthreads.
32776	853784	Create the shuffler thread
32809	854648	"Public API and common code for kernel->userspace relay file support.

See Documentation/filesystems/relay.txt for an overview.




Moved to kernel/relay.c by Paul Mundt, 2006.
November 2006 - CPU hotplug support by Mathieu Desnoyers
(mathieu.desnoyers@polymtl.ca)

This file is released under the GPL."
32821	854944	list of open channels, for cpu hotplug
32827	855085	close() vm_op implementation for relay file mapping.
32836	855312	nopage() vm_op implementation for relay file mapping.
32846	855622	Disallow mremap
32863	855849	vm_ops for relay file mappings.
32877	856225	"relay_mmap_buf: - mmap channel buffer to process address space
@buf: relay channel buffer
@vma: vm_area_struct describing memory to be mapped

Returns 0 if ok, negative on error

Caller should already have grabbed mmap_sem."
32903	856869	"relay_alloc_buf - allocate a channel buffer
@buf: the buffer struct
@size: total size of the buffer

Returns a pointer to the resulting buffer, %NULL if unsuccessful. The
passed in size will get page aligned, if it isn't already."
32942	857782	"relay_create_buf - allocate and initialize a channel buffer
@chan: the relay channel

Returns channel buffer if successful, %NULL otherwise."
32972	858441	"relay_destroy_channel - free the channel struct
@kref: target kernel reference that contains the relay channel

Should only be called from kref_put()."
32982	858683	"relay_destroy_buf - destroy an rchan_buf struct and associated buffer
@buf: the buffer struct"
33007	859327	"relay_remove_buf - remove a channel buffer
@kref: target kernel reference that contains the relay buffer

Removes the file from the fileystem, which also frees the
rchan_buf_struct and the channel buffer. Should only be called from
kref_put()."
33020	859664	"relay_buf_empty - boolean, is the channel buffer empty?
@buf: channel buffer

Returns 1 if the buffer is empty, 0 otherwise."
33031	859925	"relay_buf_full - boolean, is the channel buffer full?
@buf: channel buffer

Returns 1 if the buffer is full, 0 otherwise."
33041	860183	High-level relay kernel API and associated functions.
33046	860334	"rchan_callback implementations defining default channel behavior. Used
in place of corresponding NULL values in client callback struct."
33050	860393	subbuf_start() default callback. Does nothing.
33064	860644	buf_mapped() default callback. Does nothing.
33072	860794	buf_unmapped() default callback. Does nothing.
33080	860958	create_buf_file_create() default callback. Does nothing.
33092	861239	remove_buf_file() default callback. Does nothing.
33098	861366	relay channel default callbacks
33112	861843	"wakeup_readers - wake up readers waiting on a channel
@data: contains the channel buffer

This is the timer function used to defer reader waking."
33125	862167	"__relay_reset - reset a channel buffer
@buf: the channel buffer
@init: 1 if this is a first-time initialization

See relay_reset() for description of effect."
33160	863075	"relay_reset - reset the channel
@chan: the channel

This has the effect of erasing all data from all channel buffers
and restarting the channel in its initial state. The buffers
are not freed, so any mappings are still in effect.

NOTE. Care should be taken that the channel isn't actually
being used by anything when this call is made."
33185	863536	"relay_open_buf - create a new relay channel buffer

used by relay_open() and CPU hotplug."
33207	864009	Create file in fs
33238	864628	"relay_close_buf - close a channel buffer
@buf: channel buffer

Marks the buffer finalized and restores the default callbacks.
The channel buffer and channel buffer data structure are then freed
automatically when the last reference is given up."
33274	865557	"relay_hotcpu_callback - CPU hotplug callback
@nb: notifier block
@action: hotplug action to take
@hcpu: CPU number

Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)"
33303	866364	"No need to flush the cpu : will be flushed upon
final relay_flush() call."
33324	866982	"relay_open - create a new relay channel
@base_filename: base name of files to create
@parent: dentry of parent directory, %NULL for root directory
@subbuf_size: size of sub-buffers
@n_subbufs: number of sub-buffers
@cb: client callback functions
@private_data: user-defined data

Returns channel pointer if successful, %NULL otherwise.

Creates a channel buffer for each cpu using the sizes and
attributes specified. The created channel buffer files
will be named base_filename0...base_filenameN-1. File
permissions will be %S_IRUSR."
33387	868473	"relay_switch_subbuf - switch to a new sub-buffer
@buf: channel buffer
@length: size of current event

Returns either the length passed in or 0 if full.

Performs sub-buffer-switch tasks such as invoking callbacks,
updating padding counts, waking up readers, etc."
33410	869224	"Calling wake_up_interruptible() from here
will deadlock if we happen to be logging
from the scheduler (trying to re-grab
rq->lock), so defer it."
33448	870295	"relay_subbufs_consumed - update the buffer's sub-buffers-consumed count
@chan: the channel
@cpu: the cpu associated with the channel buffer to update
@subbufs_consumed: number of sub-buffers to add to current buf's count

Adds to the channel buffer's consumed sub-buffer count.
subbufs_consumed should be the number of sub-buffers newly consumed,
not the total consumed.

NOTE. Kernel clients don't need to call this function if the channel
mode is 'overwrite'."
33473	870836	"relay_close - close the channel
@chan: the channel

Closes all channel buffers and frees the channel."
33505	871574	"relay_flush - close the channel
@chan: the channel

Flushes all channel buffers, i.e. forces buffer switch."
33532	872088	"relay_file_open - open file op for relay files
@inode: the inode
@filp: the file

Increments the channel buffer refcount."
33548	872449	"relay_file_mmap - mmap file op for relay files
@filp: the file
@vma: the vma describing what to map

Calls upon relay_mmap_buf() to map the file into user space."
33561	872730	"relay_file_poll - poll file op for relay files
@filp: the file
@wait: poll table

Poll implemention."
33586	873260	"relay_file_release - release file op for relay files
@inode: the inode
@filp: the file

Decrements the channel refcount, as the filesystem is
no longer using it."
33597	873510	relay_file_read_consume - update the consumed count for the buffer
33627	874383	relay_file_read_avail - boolean, are there unconsumed bytes available?
33665	875333	"relay_file_read_subbuf_avail - return bytes available in sub-buffer
@read_pos: file read position
@buf: relay channel buffer"
33696	876307	"relay_file_read_start_pos - find the first available byte to read
@read_pos: file read position
@buf: relay channel buffer

If the @read_pos is in the middle of padding, return the
position of the first actually available byte, otherwise
return the original value."
33724	877194	"relay_file_read_end_pos - return the new read position
@read_pos: file read position
@buf: relay channel buffer
@count: number of bytes to be read"
33747	877801	subbuf_read_actor - read up to one subbuf's worth of data
33778	878468	relay_file_read_subbufs - read count bytes, bridging subbuf boundaries
33861	880548	subbuf_splice_actor - splice up to one subbuf's worth of data
33893	881560	Adjust read len, if longer than what is available
34014	884010	"linux/kernel/resource.c




Arbitrary resource management."
34152	886918	CONFIG_PROC_FS
34154	886975	Return the conflict entry if you can't request it
34208	888008	"request_resource - request and reserve an I/O or memory resource
@root: root resource descriptor
@new: resource descriptor desired by caller

Returns 0 for success, negative error code on error."
34224	888371	"release_resource - release a previously reserved resource
@old: resource pointer"
34242	888817	"Finds the lowest memory reosurce exists within [res->start.res->end)
the caller must specify res->start, res->end, res->flags.
If found, returns 0, res is overwritten, if not found, returns -1."
34256	889136	system ram is just marked as IORESOURCE_MEM
34269	889357	copy data
34304	890247	Find empty slot in the resource tree given range and alignment.
34318	890740	"Skip past an allocated resource that starts at 0, since the assignment
of this->start - 1 to new->end below would cause an underflow."
34357	891773	"allocate_resource - allocate empty slot in the resource tree given range & alignment
@root: root resource descriptor
@new: resource descriptor desired by caller
@size: requested resource region size
@min: minimum size to allocate
@max: maximum size to allocate
@align: alignment requested, in bytes
@alignf: alignment function, optional, called if not NULL
@alignf_data: arbitrary data to pass to the @alignf function"
34389	892836	"insert_resource - Inserts a resource in the resource tree
@parent: parent of the new resource
@new: new resource to insert

Returns 0 on success, -EBUSY if the resource can't be inserted.

This function is equivalent to request_resource when no conflict
happens. If a conflict happens, and the conflicting resources
entirely fit within the range of the new resource, then the new
resource is inserted and the conflicting resources become children of
the new resource."
34414	893390	Partial overlap? Bad, and unfixable
34456	894295	"adjust_resource - modify a resource's start and size
@res: resource to modify
@start: new start value
@size: new size

Given an existing resource, change its start and size to match the
arguments. Returns 0 on success, -EBUSY if it can't fit.
Existing children of the resource are assumed to be immutable."
34506	895357	"This is compatibility stuff for IO resources.

Note how this, unlike the above, knows about
the IO flag meanings (busy etc).

request_region creates a new busy region.

check_region returns non-zero if the area is already busy.

release_region releases a matching busy region."
34514	895562	"__request_region - create a new busy resource region
@parent: parent resource descriptor
@start: resource start address
@n: resource region size
@name: reserving caller's ID string"
34541	896182	Uhhuh, that didn't work out..
34566	896844	"__check_region - check if a resource region is busy or free
@parent: parent resource descriptor
@start: resource start address
@n: resource region size

Returns 0 if the region is free at the moment it is checked,
returns %-EBUSY if the region is busy.

NOTE:
This function is deprecated because its use is racy.
Even if it returns 0, a subsequent call to request_region()
may fail because another driver etc. just allocated the region.
Do NOT use it. It will be removed from the kernel."
34589	897379	"__release_region - release a previously reserved resource region
@parent: parent resource descriptor
@start: resource start address
@n: resource region size

The described resource region must match a currently busy region."
34631	898196	Managed region resource
34692	899638	Called from init/main.c to reserve IO ports.
34734	900752	"RT-Mutexes: simple blocking mutual exclusion locks with PI support

started by Ingo Molnar and Thomas Gleixner.






See Documentation/rt-mutex-design.txt for details."
34773	902208	"lock->owner state tracking:

lock->owner holds the task_struct pointer of the owner. Bit 0 and 1
are used to keep track of the ""owner is pending"" and ""lock has
waiters"" state.

owner bit1 bit0
NULL 0 0 lock is free (fast acquire possible)
NULL 0 1 invalid state
NULL 1 0 Transitional State
NULL 1 1 invalid state
taskpointer 0 0 lock is held (fast release possible)
taskpointer 0 1 task is pending owner
taskpointer 1 0 lock is held and has waiters
taskpointer 1 1 task is pending owner and lock has more waiters

Pending ownership is assigned to the top (highest priority)
waiter of the lock, when the lock is released. The thread is woken
up and can now take the lock. Until the lock is taken (bit 0
cleared) a competing higher priority thread can steal the lock
which puts the woken up thread back on the waiters list.

The fast atomic compare exchange based acquire and release is only
possible when bit 0 and 1 of lock->owner are 0.

(*) There's a small time where the owner can be NULL and the
""lock has waiters"" bit is set. This can happen when grabbing the lock.
To prevent a cmpxchg of the owner releasing the lock, we need to set this
bit before looking at the lock, hence the reason this is a transitional
state."
34802	902906	"We can speed up the acquire/release, if the architecture
supports cmpxchg and if there's no debugging state to be set up"
34827	903657	"Calculate task priority from the waiter list priority

Return task->normal_prio when the waiter list is empty or when
the waiter is not allowed to do priority boosting"
34841	904014	"Adjust the priority of a task, after its pi_waiters got modified.

This can be both boosting and unboosting. task->pi_lock must be held."
34858	904545	"Adjust task priority (undo boosting). Called from the exit path of
rt_mutex_slowunlock() and rt_mutex_slowlock().

(Note: We do this outside of the protection of lock->wait_lock to
allow the lock to be taken while or before we readjust the priority
of task. We do not use the spin_xx_mutex() variants here as we are
outside of the debug path.)"
34870	904815	Max number of times we'll walk the boosting chain:
34877	904999	"Adjust the priority chain. Also used for deadlock detection.
Decreases task's usage by one - may thus free the task.
Returns 0 or -EDEADLK."
34897	905696	"The (de)boosting is a step by step approach with a lot of
pitfalls. We want this to be preemptible and we want hold a
maximum of two locks per step. So we have to check
carefully whether things change under us."
34905	905887	"Print this only once. If the admin changes the limit,
print a new message when reaching the limit again."
34919	906252	Task can not go away as we did a get_task() before !
34927	906475	"Check whether the end of the boosting chain has been
reached or the state of the chain has changed while we
dropped the locks."
34935	906698	"Check the orig_waiter state. After we dropped the locks,
the previous owner of the lock might have released the lock
and made us the pending owner:"
34943	906888	"Drop out, when the task has no waiters. Note,
top_waiter can be NULL, when we are in the deboosting
mode!"
34951	907116	"When deadlock detection is off then we check, if further
priority adjustment is necessary."
34962	907373	Deadlock detection
34972	907665	Requeue the waiter
34977	907831	Release the task
34981	907930	Grab the next task
34987	908096	Boost the owner
34994	908363	Deboost the owner
35024	909067	"Optimization: check if we can steal the lock from the
assigned pending owner [which might not have taken the
lock yet]:"
35047	909623	"Check if a waiter is enqueued on the pending owners
pi_waiters list. Remove it and readjust pending owners
priority."
35053	909805	No chain handling, pending owner is not blocked on anything:
35072	910536	"We are going to steal the lock and a waiter was
enqueued on the pending owners pi_waiters queue. So
we have to enqueue this waiter into
current->pi_waiters list. This covers the case,
where current is boosted because it holds another
lock and gets unboosted because the booster is
interrupted, so we would delay a waiter with higher
priority as current->normal_prio.

Note: in the rare case of a SCHED_OTHER task changing
its priority and thus stealing the lock, next->task
might be current:"
35090	910997	"Try to take an rt-mutex

This fails
- when the lock has a real owner
- when a different pending owner exists and has higher priority than current

Must be called with lock->wait_lock held."
35111	911859	"We have to be careful here if the atomic speedups are
enabled, such that, when
- no other waiter is on the lock
- the lock has been released since we did the cmpxchg
the lock can be released or taken while we are doing the
checks and marking the lock with RT_MUTEX_HAS_WAITERS.

The atomic acquire/release aware variant of
mark_rt_mutex_waiters uses a cmpxchg loop. After setting
the WAITERS bit, the atomic release / acquire can not
happen anymore and lock->wait_lock protects us from the
non-atomic case.

Note, that this might set lock->owner =
RT_MUTEX_HAS_WAITERS in the case the lock is not contended
any more. This is fixed up when we take the ownership.
This is the transitional state explained at the top of this file."
35117	911982	We got the lock.
35133	912242	"Task blocks on lock.

Prepare waiter and propagate pi chain

This must be called with lock->wait_lock held."
35150	912801	Get the top priority waiter on the lock
35179	913635	"The owner can't disappear while holding a lock,
so the owner struct is protected by wait_lock.
Gets dropped in rt_mutex_adjust_prio_chain()!"
35199	914056	"Wake up the next waiter on the lock.

Remove the top waiter from the current tasks waiter list and from
the lock waiter list. Set it as pending owner. Then wake it up.

Called with lock->wait_lock held."
35216	914544	"Remove it from current->pi_waiters. We do not adjust a
possible priority boost right now. We execute wakeup in the
boosted mode and go back to normal after releasing
lock->wait_lock."
35231	915073	"Clear the pi_blocked_on variable and enqueue a possible
waiter into the pi_waiters list of the pending owner. This
prevents that in case the pending owner gets unboosted a
waiter with higher priority than pending-owner->normal_prio
is blocked on the unboosted (pending) owner."
35255	915628	"Remove a waiter from a lock

Must be called with lock->wait_lock held"
35295	916644	gets dropped in rt_mutex_adjust_prio_chain()!
35309	916900	"Recheck the pi chain, in case we got a priority setting

Called from sched_setscheduler"
35325	917305	gets dropped in rt_mutex_adjust_prio_chain()!
35332	917423	Slow path lock function:
35346	917734	Try to acquire the lock again:
35354	917891	Setup the timer, when timeout != NULL
35360	918046	Try to acquire the lock:
35367	918182	"TASK_INTERRUPTIBLE checks for signals and
timeout. Ignored otherwise."
35369	918254	Signal pending?
35382	918562	"waiter.task is NULL the first time we come here and
when we have been woken up by the previous owner
but the lock got stolen by a higher prio task."
35390	918806	"If we got woken up by the owner then start loop
all over without going into schedule to try
to get the lock now:"
35397	919011	"Reset the return value. We might
have returned with -EDEADLK and the
owner released the lock while we
were walking the pi chain."
35424	919472	"try_to_take_rt_mutex() sets the waiter bit
unconditionally. We might have to fix that up."
35429	919566	Remove pending timer:
35437	919802	"Readjust priority, when we did not get the lock. We might
have been the pending owner and boosted. Since we did not
take the lock, the PI boost has to go."
35448	919950	Slow path try-lock function:
35462	920247	"try_to_take_rt_mutex() sets the lock waiters
bit unconditionally. Clean this up."
35473	920375	Slow path to release a rt-mutex:
35493	920751	Undo pi boosting if necessary:
35502	920987	"debug aware fast / slowpath lock,trylock,unlock

The atomic acquire/release ops are compiled away, when either the
architecture does not support cmpxchg or when debugging is enabled."
35556	922426	"rt_mutex_lock - lock a rt_mutex

@lock: the rt_mutex to be locked"
35575	922908	"rt_mutex_lock_interruptible - lock a rt_mutex interruptible

@lock: the rt_mutex to be locked
@detect_deadlock: deadlock detection on/off

Returns:
0 on success
-EINTR when interrupted by a signal
-EDEADLK when the lock would deadlock (when deadlock detection is on)"
35600	923640	"rt_mutex_lock_interruptible_ktime - lock a rt_mutex interruptible
the timeout structure is provided
by the caller

@lock: the rt_mutex to be locked
@timeout: timeout structure or NULL (no timeout)
@detect_deadlock: deadlock detection on/off

Returns:
0 on success
-EINTR when interrupted by a signal
-ETIMEOUT when the timeout expired
-EDEADLK when the lock would deadlock (when deadlock detection is on)"
35618	924063	"rt_mutex_trylock - try to lock a rt_mutex

@lock: the rt_mutex to be locked

Returns 1 on success and 0 on contention"
35629	924303	"rt_mutex_unlock - unlock a rt_mutex

@lock: the rt_mutex to be unlocked"
35643	924700	"rt_mutex_destroy - mark a mutex unusable
@lock: the mutex to be destroyed

This function marks the mutex uninitialized, and any subsequent
use of the mutex is forbidden. The mutex must not be locked when
this function is called."
35662	925080	"__rt_mutex_init - initialize the rt lock

@lock: the rt lock to be initialized

Initialize the rt lock to unlocked state.

Initializing of a locked rt lock is not allowed"
35682	925608	"rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a
proxy owner

@lock: the rt_mutex to be locked
@proxy_owner:the task to set as owner

No locking. Caller has to do serializing itself
Special API call for PI-futex support"
35699	926081	"rt_mutex_proxy_unlock - release a lock on behalf of owner

@lock: the rt_mutex to be locked

No locking. Caller has to do serializing itself
Special API call for PI-futex support"
35719	926563	"rt_mutex_next_owner - return the next owner of the lock

@lock: the rt lock query

Returns the next owner of the lock or NULL

Caller has to serialize against other accessors to the lock
itself.

Special API call for PI-futex support"
35748	927491	"Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)



Interactivity improvements by Mike Galbraith
(C) 2007 Mike Galbraith <efault@gmx.de>

Various enhancements by Dmitry Adamushko.
(C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>

Group scheduling enhancements by Srivatsa Vaddagiri

Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>

Scaled math optimizations by Thomas Gleixner


Adaptive scheduling granularity, math enhancements by Peter Zijlstra"
35761	927966	"Targeted preemption latency for CPU-bound tasks:
(default: 20ms * (1 + ilog(ncpus)), units: nanoseconds)

NOTE: this latency value is not the same as the concept of
'timeslice length' - timeslices in CFS are of variable length
and have no persistent notion like in traditional, time-slice
based scheduling concepts.

(to see the precise effective timeslice length of your workload,
run vmstat and monitor the context-switches (cs) field)"
35767	928139	"Minimal preemption granularity for CPU-bound tasks:
(default: 4 msec * (1 + ilog(ncpus)), units: nanoseconds)"
35772	928269	is kept at sysctl_sched_latency / sysctl_sched_min_granularity
35778	928414	"After fork, child runs first. (default) If set to 0 then
parent will (try to) run first."
35786	928610	"sys_sched_yield() compat mode

This option switches the agressive yield implementation of the
old scheduler back on."
35796	928954	"SCHED_BATCH wake-up granularity.
(default: 10 msec * (1 + ilog(ncpus)), units: nanoseconds)

This option delays the preemption effects of decoupled workloads
and reduces their over-scheduling. Synchronous workloads will still
have immediate wakeup/sleep latencies."
35806	929309	"SCHED_OTHER wake-up granularity.
(default: 10 msec * (1 + ilog(ncpus)), units: nanoseconds)

This option delays the preemption effects of decoupled workloads
and reduces their over-scheduling. Synchronous workloads will still
have immediate wakeup/sleep latencies."
35813	929554	"***********************************************************
CFS operations on generic schedulable entities:"
35817	929639	cpu runqueue to which this cfs_rq is attached
35823	929775	"An entity is a task if it doesn't ""own"" a runqueue"
35826	929851	CONFIG_FAIR_GROUP_SCHED
35835	930024	CONFIG_FAIR_GROUP_SCHED
35845	930278	"***********************************************************
Scheduling class tree data structure manipulation methods:"
35872	930810	Enqueue an entity into the rb-tree:
35883	931112	Find the right place in the rbtree:
35890	931297	"We dont care about collisions. Nodes with
the same key stay together."
35902	931515	"Maintain a cache of leftmost tree entries (it is frequently
used):"
35945	932584	"***********************************************************
Scheduling class statistics methods:"
35971	933215	"The idea is to set a period in which each task runs once.

When there are too many tasks (sysctl_sched_nr_latency) we have to stretch
this period because otherwise the slices get too small.

p = (nr <= nl) ? l : l*nr/nl"
35990	933595	"We calculate the wall-time slice from the period by taking a part
proportional to the weight.

s = p*w/rw"
36005	933866	"We calculate the vruntime slice.

vs = s/w = p/rw"
36030	934464	"Update the current task's runtime statistics. Skip current tasks that
are not in our scheduling class."
36052	935130	"maintain cfs_rq->min_vruntime to be a monotonic increasing
value tracking the leftmost vruntime in the tree."
36076	935685	"Get the amount of time the current task was running
since the last time we changed load (this cannot
overflow on 32 bits):"
36097	936130	Task is being enqueued - update stats:
36103	936317	"Are we enqueueing a waiting task? (for current tasks
a dequeue/enqueue event is a NOP)"
36122	936766	"Mark the end of the wait period if dequeueing a
waiting task:"
36129	936895	We are picking a new current task - update its stats:
36135	937035	We are starting a new run period:
36141	937172	"***********************************************
Scheduling class queueing methods:"
36190	938283	"Blocking time is in units of nanosecs, so shift by 20 to
get a milliseconds-range estimation of the amount of
time that the task spent sleeping:"
36235	939349	"The 'current' period is already promised to the current tasks,
however the extra weight of the new task will slow them down a
little, place the new task so that it fits in the slot that
stays open at the end."
36240	939502	sleeps upto a single latency don't count.
36244	939660	ensure we never gain time by being placed backwards.
36256	939891	Update run-time statistics of the 'current'.
36276	940300	Update run-time statistics of the 'current'.
36300	940820	Preempt the current task with a newly woken task if needed:
36315	941255	'current' is not kept within the tree.
36321	941420	"Any task has to be enqueued before it get to execute on
a CPU. So account for the time it spent waiting on the
runqueue."
36333	941757	"Track our maximum slice length, if the CPU's load is at
least twice that of our own weight (i.e. dont track it
when there are only lesser-weight tasks around):"
36359	942378	"If still on the runqueue then deactivate_task()
was not called and update_curr() has to be done:"
36366	942552	Put 'current' back into the tree.
36376	942748	Update run-time statistics of the 'current'.
36385	942954	"***********************************************
CFS operations on tasks:"
36389	943031	Walk up scheduling entities hierarchy
36398	943243	runqueue on which this entity is (to be) queued
36404	943369	"runqueue ""owned"" by this group"
36412	943566	"Given a group's cfs_rq on one cpu, return its corresponding cfs_rq on
another cpu ('this_cpu')"
36418	943737	Iterate thr' all leaf cfs_rq's on a runqueue
36422	943915	Do the two (enqueued) entities belong to the same group ?
36437	944194	CONFIG_FAIR_GROUP_SCHED
36455	944543	"runqueue ""owned"" by this group"
36480	945066	CONFIG_FAIR_GROUP_SCHED
36486	945229	"The enqueue_task method is called before nr_running is
increased. Here we update the fair scheduling stats and
then put the task into the rbtree:"
36505	945666	"The dequeue_task method is called before nr_running is
decreased. We remove the task from the rbtree and
update the fair scheduling stats:"
36514	945964	Don't dequeue parent if it has other entities besides us
36525	946168	"sched_yield() support is very simple - we dequeue and enqueue.

If compat_yield is turned on then we requeue to the end of the tree."
36534	946393	Are we the only task in the tree?
36542	946604	Update run-time statistics of the 'current'.
36549	946694	Find the rightmost entry in the rbtree:
36553	946783	Already in the rightmost position?
36561	947033	"Minimally necessary key value to be last in the tree:
Upon rescheduling, sched_class::put_prev_task() will place
'current' within the tree based on its new key value."
36567	947147	Preempt the current task with a newly woken task if needed:
36584	947579	"Batch tasks do not preempt (their preemption is driven by
the tick):"
36622	948300	Account for a descheduled task:
36637	948652	"***********************************************
Fair scheduling class load-balancing methods:"
36645	948921	"Load-balancing iterator. Note: while the runqueue stays locked
during the whole iteration, the current task might be
dequeued so the iterator has to be dequeue-safe. Here we
achieve that by always pre-iterating before returning
the current task:"
36715	950581	Don't pull if this_cfs_rq has more load than busy_cfs_rq
36719	950658	Don't pull more than imbalance/2
36730	950908	"pass busy_cfs_rq argument into
load_balance_[start|next]_fair iterators"
36758	951637	"pass busy_cfs_rq argument into
load_balance_[start|next]_fair iterators"
36771	951873	scheduler tick hitting a task of our scheduling class:
36791	952462	"Share the fairness runtime between parent and child, thus the
total amount of pressure for CPU stays equal - new tasks
get a chance to run but frequent forkers are not allowed to
monopolize the CPU. Note: the parent runqueue is locked,
the child is not running yet."
36803	952809	'curr' will be NULL if the child belongs to a different group
36809	953052	"Upon rescheduling, sched_class::put_prev_task() will place
'current' within the tree based on its new key value."
36821	953318	"Account for a task changing its policy or group.

This routine is mostly called to set cfs_rq->curr field when a task
migrates between groups/classes."
36832	953521	All the scheduling class methods:
36891	955397	"kernel/sched.c

Kernel scheduler and related syscalls



1996-12-23 Modified by Dave Grothe to fix bugs in semaphores and
make semaphores SMP safe
1998-11-19 Implemented schedule_timeout() and related stuff
by Andrea Arcangeli
2002-01-04 New ultra-scalable O(1) scheduler by Ingo Molnar:
hybrid priority-list and round-robin design with
an array-switch method of distributing timeslices
and per-CPU runqueues. Cleanups and useful suggestions
by Davide Libenzi, preemptible kernel bits by Robert Love.
2003-09-03 Interactivity tuning by Con Kolivas.
2004-04-02 Scheduler domains code by Nick Piggin
2007-04-15 Work begun on replacing all interactivity tuning with a
fair scheduling design by Con Kolivas.
2007-05-05 Load balancing (smp-nice) and other improvements
by Peter Williams
2007-05-06 Interactivity improvements to CFS by Mike Galbraith
2007-07-01 Group scheduling enhancements by Srivatsa Vaddagiri"
36940	956671	"Scheduler clock - returns current time in nanosec units.
This is default implementation.
Architectures and sub-architectures can override this."
36950	956914	"Convert user-nice values [ -20 ... 0 ... 19 ]
to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
and back."
36959	957245	"'User priority' is the nice value converted to something we
can work with better when scaling various scheduler parameters,
it's a [ 0 ... 39 ] range."
36966	957461	Some helpers for converting nanosecond timing to jiffy resolution
36978	957845	"These are the 'tuning knobs' of the scheduler:

default timeslice is 100 msecs (used only for SCHED_RR tasks).
Timeslices get refilled after they expire."
36985	958051	"Divide a load by a sched group cpu_power : (load / sg->__cpu_power)
Since cpu_power is a 'constant', we can use a reciprocal divide."
36994	958286	"Each time a sched group cpu_power is changed,
we must compute its reciprocal value"
37016	958767	This is the priority-queue data structure of the RT scheduling class:
37018	958864	include 1 bit for delimiter
37028	959018	task group related information
37033	959164	schedulable entities of this group on each cpu
37035	959241	"runqueue ""owned"" by this group on each cpu"
37038	959341	spinlock to serialize modification to shares
37043	959437	Default task group's sched entity on each cpu
37045	959546	Default task group's cfs_rq on each cpu
37053	959815	"Default task group.
Every task in system belong to this group at bootup."
37067	960139	return group to which a task belongs
37083	960544	Change a task's cfs_rq and parent entity if it moves across CPUs/groups
37094	960834	CONFIG_FAIR_GROUP_SCHED
37096	960874	CFS-related fields in a runqueue
37109	961220	"'curr' points to currently running entity on this cfs_rq.
It is set to NULL otherwise (i.e when none are currently running)."
37115	961380	cpu runqueue to which this cfs_rq is attached
37124	961682	"leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
a hierarchy). Non-leaf lrqs hold other higher schedulable entities
(like users, containers etc.)

leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
list is used during load balance."
37126	961780	"group that ""owns"" this runqueue"
37130	961845	Real-Time classes' related field in a runqueue:
37143	962244	"This is the main, per-CPU runqueue data structure.

Locking rule: those places that want to lock multiple runqueues
(such as the load balancing or the thread migration code), lock
acquire operations must be ordered by ascending &runqueue."
37145	962278	runqueue lock:
37151	962441	"nr_running and cpu_load should be in the same cacheline because
remote CPUs use both these fields when doing load calculation."
37159	962678	capture load from *all* tasks on this cpu:
37166	962846	list of leaf cfs_rq on this cpu:
37176	963167	"This is part of a global counter where only the total sum
over all CPUs matters. A task can increase this counter on
one CPU and if it got migrated afterwards it may decrease
it on another CPU. Always updated under the runqueue lock:"
37196	963562	For active balancing
37199	963627	cpu of this runqueue:
37207	963766	latency stats
37210	963832	sys_sched_yield() stats
37216	963970	schedule() stats
37221	964084	try_to_wake_up() stats
37225	964154	BKL stats
37251	964693	"Update the per-runqueue clock, as finegrained as the platform can give
us, but without assuming monotonicity, etc.:"
37264	965011	Protect against sched_clock() occasionally going backwards:
37271	965132	Catch too large forward jumps too:
37301	965866	"The domain tree (rq->sd) is protected by RCU's quiescent state transition.
See detach_destroy_domains: synchronize_sched for details.

The domain tree of any CPU may only be accessed from within
preempt-disabled sections."
37312	966232	Tunables that become constants when CONFIG_SCHED_DEBUG is off:
37321	966383	Debugging: various feature bits
37342	966938	"Number of tasks to iterate in a single balance run.
Limited because this is done with IRQs disabled."
37348	967114	"For kernel-internal use: high-speed (but slightly incorrect) per-cpu
clock constructed from sched_clock():"
37360	967395	"Only call sched_clock() if the scheduler has already been
initialized (some code might call cpu_clock() very early):"
37395	968196	this is a valid case when another task releases the spinlock
37402	968385	"If we are tracking spinlock dependencies then we have to
fix up the runqueue lock - which gets 'carried over' from
prev into current:"
37408	968508	__ARCH_WANT_UNLOCKED_CTXSW
37425	968905	"We can optimise this out completely for !SMP, because the
SMP rebalancing from interrupt is the only thing that cares
here."
37442	969303	"After ->oncpu is cleared, the task can be moved to a different CPU.
We must ensure this doesn't happen until the switch is completely
finished."
37450	969450	__ARCH_WANT_UNLOCKED_CTXSW
37455	969560	"__task_rq_lock - lock the runqueue a given task resides on.
Must be called interrupts disabled."
37472	969981	"task_rq_lock - lock the runqueue a given task resides on and disable
interrupts. Note the ordering: we can safely lookup the task_rq without
explicitly disabling preemption."
37502	970582	this_rq_lock - lock this runqueue and disable interrupts.
37517	970792	We are going deep-idle (irqs are disabled):
37531	971106	We just idled delta nanoseconds (called with irqs disabled):
37544	971468	"Override the previous timestamp and ignore all
sched_clock() deltas that occured while we idled,
and use the PM-provided delta_ns to advance the
rq clock:"
37558	971839	"resched_task - mark a task 'to be rescheduled now'.

On UP this means the setting of the need_resched flag, on SMP it
might also involve a cross-CPU call to trigger the scheduler on
the target CPU."
37580	972296	NEED_RESCHED must be visible before we test polling
37614	972877	Shift right and round:
37629	973256	Check whether we'd overflow the 64-bit multiplication:
37662	974248	"To aid in avoiding the subversion of ""niceness"" due to uneven distribution
of tasks with abnormal ""nice"" values across CPUs the contribution that
each task makes to its run queue's load is weighted according to its
scheduling class and ""nice"" value. For SCHED_NORMAL tasks this is just a
scaled version of the new time slice allocation that they receive on time
slice expiry etc."
37678	974886	"Nice levels are multiplicative, with a gentle 10% change for every
nice level changed. I.e. when a CPU-bound task goes from nice 0 to
nice 1, it will get ~10% less CPU time than another CPU-bound task
that remained on nice 0.

The ""10% effect"" is relative and cumulative: from _any_ nice level,
if you go up 1 level, it's -10% CPU usage, if you go down 1 level
it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
If a task goes up by ~10% and another task goes down by ~10% then
the relative distance between them is ~25%.)"
37687	974937	"-20
-15
-10
-5
0
5
10
15"
37696	975703	"Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.

In cases where the weight does not change often, we can use the
precalculated inverse to speed up arithmetics by turning divisions
into multiplications:"
37705	975753	"-20
-15
-10
-5
0
5
10
15"
37714	976541	"runqueue iterator, to support SMP load-balancing between different
scheduling classes, without having to expose their internal data
structures to the load-balancing proper:"
37764	978022	"Update delta_exec, delta_fair fields for rq.

delta_fair clock advances at a rate inversely proportional to
total load (rq->load.weight) on the runqueue, while
delta_exec advances at the same rate as wall-clock (provided
cpu is not idle).

delta_exec / delta_fair is a measure of the (smoothened) load on this
runqueue over any given interval. This (smoothened) load is used
during load balance.

This function is called /before/ updating rq->load
and when switching tasks."
37797	978727	SCHED_IDLE tasks get minimal weight:
37823	979382	__normal_prio - return the priority that is based on the static prio
37835	979723	"Calculate the expected normal priority: i.e. priority
without taking RT-inheritance into account. Might be
boosted by interactivity modifiers. Changes upon fork,
setprio syscalls, and whenever the interactivity
estimator recalculates."
37853	980185	"Calculate the current priority, i.e. the priority
taken into account by the scheduler. This value might
be boosted by RT tasks, or might be boosted by
interactivity modifiers. Will be RT if the task got
RT-boosted. If not then it returns p->normal_prio."
37861	980425	"If we are RT tasks or we were boosted to RT priority,
keep the priority unchanged. Otherwise, update priority
to the normal priority:"
37869	980549	activate_task - move a task to the runqueue.
37881	980813	deactivate_task - remove a task from the runqueue.
37894	981111	"task_curr - is this task currently executing on a CPU?
@p: the task in question."
37900	981263	Used instead of source_load when we know the type == 0
37914	981674	"After ->cpu is set up to a new value, task_rq_lock(p, ...) can be
successfuly executed on another CPU. We must ensure that updates of
per-task data have been completed by this moment."
37924	981789	Is this task likely cache-hot:
37985	983162	"The task's runqueue lock must be held.
Returns true if you have to wait for migration thread."
37994	983412	"If the task is not on a runqueue (and not running), then
it is sufficient to simply update the task's cpu field."
38016	984014	"wait_task_inactive - wait for a thread to unschedule.

The caller must ensure that the task *will* unschedule sometime soon,
else this function might spin for a *long* time. This function can't
be called with interrupts off, or it may introduce deadlock with
smp_call_function() if an IPI is sent by the same process we are
waiting to become inactive."
38029	984328	"We do the initial early heuristics without holding
any task-queue locks at all. We'll only try to get
the runqueue lock when things look like they will
work out!"
38042	984739	"If the task is actively running on another CPU
still, just relax and busy-wait without holding
any locks.

NOTE! Since we don't hold any locks, it's not
even sure that ""rq"" stays as the right runqueue!
But we don't care, since ""task_running()"" will
return false if the runqueue has changed and p
is actually now running somewhere else!"
38050	984929	"Ok, time to look more closely! We need the rq
lock now, to be *sure*. If we're wrong, we'll
just go back and repeat."
38061	985198	"Was it really running after all now that we
checked with the proper locks actually held?

Oops. Go back and try again.."
38075	985541	"It's not enough that it's not actively running,
it must be off the runqueue _entirely_, and not
preempted!

So if it wa still runnable (but just not actively
running right now), it's preempted, and we should
yield - it could be a while."
38085	985792	"Ahh, all good. It wasn't running, and it wasn't
runnable, which means that it will never become
running in the future either. We're all done!"
38102	986314	"kick_process - kick a running thread to enter/exit the kernel
@p: the to-be-kicked thread

Cause a process which is running on another CPU to enter
kernel-mode, without any delay. (to get signals handled.)

NOTE: this function doesnt have to take the runqueue lock,
because all it wants to ensure is that the remote task enters
the kernel. If the IPI races and the task has been migrated
to another CPU then no harm is done and the purpose has been
achieved as well."
38120	986732	"Return a low guess at the load of a migration-source cpu weighted
according to the scheduling class and ""nice"" value.

We want to under-estimate the load of migration sources, to
balance conservatively."
38135	987073	"Return a high guess at the load of a migration-target cpu weighted
according to the scheduling class and ""nice"" value."
38149	987348	Return the average load per task on the cpu's run queue
38162	987659	"find_idlest_group finds and returns the least busy CPU group within the
domain."
38176	988099	Skip over this group if it has no CPUs allowed
38182	988275	Tally up the load of all CPUs in the group
38186	988383	Bias balancing toward cpus of our domain
38195	988562	Adjust by relative CPU power of the group
38215	988993	find_idlest_cpu - find the idlest cpu among the cpus in group.
38224	989210	Traverse only the allowed CPUs
38249	989786	"sched_balance_self: balance the current task (running on cpu) in domains
that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
SD_BALANCE_EXEC.

Balance, ie. select the least loaded group.

Returns the target CPU number, or the same CPU if no balancing is needed.

preempt must be disabled."
38258	990016	If power savings logic is enabled for a domain, stop there.
38284	990519	Now try balancing at a lower domain level of cpu
38289	990617	Now try balancing at a lower domain level of new_cpu
38299	990852	while loop will break here if sd == NULL
38305	990896	CONFIG_SMP
38314	991178	"wake_idle() will wake a task on an idle cpu if task->cpu is
not idle and an idle cpu is available. The span of cpus to
search starts with cpus closest then further out as needed,
so we always favor a closer, idle cpu.

Returns the CPU we should wake onto."
38330	991708	"If it is idle, then it is the best cpu to run this task.

This cpu is also the best, if it has more than one task already.
Siblings must be also busy(in most cases) as they didn't already
pickup the extra load from this cpu and hence we need not check
sibling runqueue info. This will avoid the checks and cache miss
penalities associated with that."
38372	992710	"try_to_wake_up - wake up a thread
@p: the to-be-woken-up thread
@state: the mask of task states that can be woken
@sync: do a synchronous wakeup?

Put it on the run-queue if it's not already there. The ""current""
thread is always on the run-queue (except when the actual
re-schedule is in progress), and as such you're allowed to do
the simpler ""current->state = TASK_RUNNING"" to mark yourself
runnable without the overhead of this.

returns failure only if the task is already active."
38422	993704	Check for affine wakeup and passive balancing possibilities.
38432	993965	Wake to this CPU if we can
38440	994132	Attract cache-cold tasks on sync wakeups:
38451	994461	"If sync wakeup then subtract the (maximum possible)
effect of the currently running task from the load
of the current CPU:"
38462	994776	"This domain has SD_WAKE_AFFINE and
p is cache cold in this domain, and
there is no bad imbalance."
38472	994989	"Start passive balancing when half the imbalance_pct
limit is reached."
38482	995270	Could not wake to this_cpu. Wake to cpu instead
38488	995434	might preempt at this point
38501	995668	CONFIG_SMP
38541	996582	"Perform scheduler related setup for a newly forked process p.
p is forked by current.

__sched_fork() is basic setup used by init_idle() too:"
38572	997370	"We mark the process as running here, but have not actually
inserted it onto the runqueue yet. This guarantees that
nobody will actually run it, and a signal or other external
event cannot wake it up and insert it on the runqueue either."
38578	997436	fork()/clone()-time setup:
38592	997707	Make sure we do not leak PI boosting priority to the child:
38605	998119	Want to start with kernel preemption disabled.
38617	998441	"wake_up_new_task - wake up a newly created task for the first time.

This function will do some initial scheduler statistics housekeeping
that must be done for every newly created context, then puts the task
on the runqueue and wakes it."
38635	998863	"Let the scheduling class do new task startup
management (if any):"
38648	999160	"preempt_notifier_register - tell me when current is being being preempted & rescheduled
@notifier: notifier struct to register"
38660	999536	"preempt_notifier_unregister - no longer interested in preemption notifications
@notifier: notifier struct to unregister

This is safe to call from within a preemption notifier."
38713	1000868	"prepare_task_switch - prepare to switch tasks
@rq: the runqueue preparing to switch
@prev: the current task that is being switched out
@next: the task we are going to switch to.

This is called with the rq lock held and interrupts off. It must
be paired with a subsequent finish_task_switch after the context
switch.

prepare_task_switch sets up locking and calls architecture specific
hooks."
38737	1001734	"finish_task_switch - clean up after a task-switch
@rq: runqueue associated with task-switch
@prev: the thread we just switched away from.

finish_task_switch must be called after the context switch, paired
with a prepare_task_switch call before the context switch.
finish_task_switch will reconcile locking set up by prepare_task_switch,
and do any other architecture-specific cleanup actions.

Note that we may have delayed dropping an mm in context_switch(). If
so, we finish that here outside of the runqueue lock. (Doing it
with the lock held can cause deadlocks; see schedule() for
details.)"
38756	1002440	"A task struct has one reference for the use as ""current"".
If a task dies, then it sets TASK_DEAD in tsk->state and calls
schedule one last time. The schedule call will never return, and
the scheduled task must drop that reference.
The test for TASK_DEAD must occur while the runqueue locks are
still held, otherwise prev could be scheduled on another cpu, die
there before we look at prev->state, and then the reference would
be dropped twice.
Manfred Spraul <manfred@colorfullife.com>"
38767	1002755	"Remove function-return probe instances associated with this
task and put them back on the free list."
38776	1002936	"schedule_tail - first thing a freshly forked thread must call.
@prev: the thread we just switched away from."
38784	1003179	In this case, finish_task_switch does not reenable preemption
38794	1003384	"context_switch - switch to the new MM and the new
thread's register state."
38808	1003760	"For paravirt, this is coupled with an exit in switch_to to
combine the page table reload and the switch backend into
one hypercall."
38827	1004240	"Since the runqueue lock will be released by the next
task (which is an invalid locking op but in the case
of the scheduler it's an obvious special-case), so we
do an early lockdep release here:"
38832	1004392	Here we just switch the register state and the stack.
38840	1004599	"this_rq must be evaluated again because prev may have moved
CPUs since it called schedule(), thus the 'rq' on its stack
frame will be invalid."
38850	1004904	"nr_running, nr_uninterruptible and nr_context_switches:

externally visible scheduler statistics: current number of runnable
threads, current number of uninterruptible-sleeping threads, total
number of context switches performed since bootup."
38871	1005302	"Since we read the counters lockless, it might be slightly
inaccurate. Do not allow it to go below zero though:"
38917	1006089	"Update rq->cpu_load[] statistics. This function is usually called every
scheduler tick (TICK_NSEC)."
38925	1006258	Update our load:
38929	1006431	scale is effectively 1 << i now, and >> i divides by scale
38937	1006645	"Round up the averaging division if load is increasing. This
prevents us from getting stuck on 9 if the load is 10, for
example."
38951	1006946	"double_rq_lock - safely lock two runqueues

Note this does not disable interrupts like task_rq_lock,
you need to do so manually before calling."
38959	1007169	Fake it out ;)
38978	1007536	"double_rq_unlock - safely unlock two runqueues

Note this does not restore interrupts like task_rq_unlock,
you need to do so manually after calling."
38992	1007834	double_lock_balance - lock the busiest runqueue, this_rq is locked already.
38999	1008074	printk() doesn't work good under rq->lock
39018	1008573	"If dest_cpu is allowed for this process, migrate the task to it.
This is accomplished by forcing the cpu_allowed mask to only
allow dest_cpu, which will force the cpu onto dest_cpu. Then
the cpu_allowed mask is restored."
39030	1008888	force the process onto the specified CPU
39032	1008994	Need to wait for migration thread (might exit: take ref).
39050	1009386	"sched_exec - execve() is a valuable balancing opportunity, because at
this point the task has the smallest effective memory and cache footprint."
39063	1009699	"pull_task - move a task from a remote runqueue to the local runqueue.
Both runqueues must be locked."
39073	1010005	"Note that idle threads have a prio of MAX_PRIO, for this test
to be always true for them."
39079	1010123	can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
39090	1010469	"We do not migrate tasks that are:
1) running (obviously), or
2) cannot be migrated to this CPU due to cpus_allowed, or
3) are cache-hot on their current CPU."
39106	1010812	"Aggressive migration if:
1) task is cache cold, or
2) too many balance attempts have failed."
39143	1011666	Start the load-balancing iterator:
39152	1011974	"To help distribute high priority tasks across CPUs we don't
skip a task if it will be the highest priority task (i.e. smallest
prio value) on its new queue regardless of its load weight"
39167	1012413	We only want to steal up to the prescribed amount of weighted load.
39179	1012726	"Right now, this is one of only two places pull_task() is called,
so we can safely collect pull_task() stats here rather than
inside pull_task()."
39194	1013091	"move_tasks tries to move up to max_load_move weighted load from busiest to
this_rq, as part of a balancing operation within domain ""sd"".
Returns 1 if successful and 0 otherwise.

Called with both runqueues locked."
39230	1014249	"Right now, this is only the second place pull_task()
is called, so we can safely collect pull_task()
stats here rather than inside pull_task()."
39247	1014585	"move_one_task tries to move exactly one task from busiest to this_rq, as
part of active balancing operations within ""domain"".
Returns 1 if successful and 0 otherwise.

Called with both runqueues locked."
39264	1015130	"find_busiest_group finds and returns the busiest CPU group within the
domain. It calculates and returns the amount of weighted load which
should be moved to restore balance via the imbalance parameter."
39306	1016586	Tally up the load of all CPUs in the group
39322	1016911	Bias balancing toward cpus of our domain
39348	1017582	"First idle cpu or the first cpu(busiest) in this sched group
is eligible for doing load balancing at this and above
domains. In the newly idle case, we will allow all the cpu's
to do the newly idle load balance."
39358	1017820	Adjust by relative CPU power of the group
39385	1018570	"Busy processors will not participate in power savings
balance."
39393	1018785	"If the local group is idle or completely loaded
no need to do power savings balance at this domain"
39401	1019034	"If a group is already running at full capacity or idle,
don't include that group in power savings calculations"
39410	1019301	"Calculate the group which has the least non-idle load.
This is the group from where we need to pick up the load
for saving power"
39425	1019754	"Calculate the group which is almost near its
capacity but still has some space to pick up some load
from other group and save more power"
39463	1021120	"We're trying to get all the cpus to the average_load, so we don't
want to push ourselves above the average load, nor do we wish to
reduce the max loaded cpu below the average load, as either of these
actions would just result in more rebalancing later, and ping-pong
tasks around. Thus we look for the minimum possible imbalance.
Negative imbalances (*we* are more loaded than anyone else) will
be counted as no imbalance for these purposes -- we can't fix that
by pulling tasks to us. Be careful of negative numbers as they'll
appear as very large values with unsigned longs."
39471	1021376	"In the presence of smp nice balancing, certain scenarios can have
max load less than avg load(as we skip the groups at or below
its cpu_power, while calculating max_load..)"
39477	1021517	Don't want to pull so many tasks that a group would go idle
39480	1021654	How much load to actually move to equalise the imbalance
39490	1022000	"if *imbalance is less than the average load per runnable task
there is no gaurantee that any tasks will be moved so we'll have
a think about bumping its value to force at least one task to be
moved"
39515	1022656	"OK, we don't have enough imbalance to justify moving tasks,
however we may be able to increase total CPU power used by
moving them."
39523	1022877	Amount of load we'd subtract
39530	1023099	Amount of load we'd add
39542	1023490	Move if we gain throughput
39566	1023974	find_busiest_queue - find the busiest runqueue among the cpus in group.
39599	1024593	"Max backoff if we encounter pinned tasks. Pretty arbitrary value, but
so long as it is large enough."
39605	1024742	"Check this_cpu to ensure it is balanced within domain. Attempt to move
tasks if there is an imbalance."
39622	1025324	"When power savings policy is enabled for the parent domain, idle
sibling can pick up load irrespective of busy siblings. In this case,
let the state of idle sibling percolate up as CPU_IDLE, instead of
portraying it as CPU_NOT_IDLE."
39658	1026216	"Attempt to move tasks. If find_busiest_group has found
an imbalance but busiest->nr_running <= 1, the group is
still unbalanced. ld_moved simply stays zero, so it is
correctly treated as an imbalance."
39668	1026501	some other cpu did the load balance for us.
39672	1026641	All tasks on this runqueue were pinned by CPU affinity
39691	1027087	"don't kick the migration_thread, if the curr
task on busiest cpu can't be moved to this_cpu"
39710	1027585	"We've kicked active balancing, reset the failure
counter."
39717	1027770	We were unbalanced, so reset the balancing interval
39725	1028040	"If we've begun active balancing, start to back off. This
case may not be covered by the all_pinned logic if there
is only 1 task on the busy runqueue (because we don't call
move_tasks)."
39741	1028404	tune up the balancing interval
39758	1028895	"Check this_cpu to ensure it is balanced within domain. Attempt to move
tasks if there is an imbalance.

Called from schedule when this_rq is about to become idle (CPU_NEWLY_IDLE).
this_rq is locked."
39775	1029415	"When power savings policy is enabled for the parent domain, idle
sibling can pick up load irrespective of busy siblings. In this case,
let the state of idle sibling percolate up as IDLE, instead of
portraying it as CPU_NOT_IDLE."
39802	1030098	Attempt to move tasks
39804	1030181	this_rq->clock is already updated
39841	1031051	"idle_balance is called by schedule() if this_cpu is about to become
idle. Attempts to pull tasks from other CPUs."
39855	1031408	If we've pulled tasks over stop searching:
39869	1031852	"We are going idle. next_balance may be set based on
a busy processor. So reset next_balance."
39881	1032184	"active_load_balance is run by migration threads. It pushes running tasks
off the busiest CPU onto idle CPUs. It requires at least 1 task to be
running on each physical CPU where possible, and avoids physical /
logical imbalances.

Called with busiest_rq locked."
39888	1032382	Is there any task to move?
39898	1032603	"This condition is ""impossible"", if it occurs
we need to fix it. Originally reported by
Bjorn Helgaas on a 128-cpu setup."
39901	1032686	move a task from busiest_rq to target_rq
39906	1032847	Search for an sd spanning us and the target CPU.
39953	1034296	"This routine will try to nominate the ilb (idle load balancing)
owner among the cpus whose ticks are stopped. ilb owner will do the idle
load balancing on behalf of all those cpus. If all the cpus in the system
go into this tickless mode, then there will be no ilb owner (as there is
no need for one) and all the cpus will sleep till the next wakeup event
arrives...

For the ilb owner, tick is not stopped. And this tick will be used
for idle load balancing. ilb owner will still be part of
nohz.cpu_mask..

While stopping the tick, this cpu will become the ilb owner if there
is no other owner. And will be the owner till that cpu becomes busy
or if all cpus in the system stop their ticks at which point
there is no need for ilb owner.

When the ilb owner becomes busy, it nominates another owner, during the
next busy scheduler_tick()"
39964	1034533	If we are going offline and still the leader, give up!
39972	1034741	time for ilb owner also to sleep
39980	1034984	make me the ilb owner
40006	1035589	"It checks each scheduling domain to see if it is due to be balanced,
and initiates a balancing operation if so.

Balancing parameters are set up in arch_init_sched_domains."
40013	1035810	Earliest time when we have to do rebalance again
40025	1036088	scale ms to jiffies
40044	1036568	"We've pulled tasks over so either we're no
longer idle, or one of our SMT siblings is
not idle."
40061	1036989	"Stop the load balance at this level. There is another
CPU in our sched group which is doing load balancing more
actively."
40070	1037168	"next_balance will be updated only when there is a need.
When the cpu is attached to null domain for ex, it will not be
updated."
40079	1037457	"run_rebalance_domains is triggered when needed from the scheduler tick.
In CONFIG_NO_HZ case, the idle load balance owner will do the
rebalancing for all the cpus for whom scheduler ticks are stopped."
40094	1037886	"If this cpu is the owner for idle load balancing, then do the
balancing on behalf of the other idle cpus whose ticks are
stopped."
40107	1038269	"If this cpu gets work to do, stop the load balancing
work being done for other cpus. Next load
balancing owner will pick it up."
40127	1038764	"Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.

In case of CONFIG_NO_HZ, this is the place where we nominate a new
idle load balancing owner or decide to stop the periodic load balancing,
if the whole system is idle."
40135	1039006	"If we were in the nohz mode recently and busy at the current
scheduler tick, then check if we need to nominate new idle
load balancer."
40152	1039485	"simple selection for now: Nominate the
first cpu in the nohz list to be the next
ilb owner.

TBD: Traverse the sched domains and nominate
the nearest cpu in the nohz.cpu_mask."
40163	1039711	"If this cpu is idle and doing idle load balancing for all the
cpus with ticks stopped, is it time for that to stop?"
40173	1039996	"If this cpu is idle and the idle load balancing is done by
someone else, then no need raise the SCHED_SOFTIRQ"
40182	1040222	CONFIG_SMP
40186	1040279	on UP we do not need to balance between CPUs:
40200	1040568	"Return p->sum_exec_runtime plus any more ns on the sched_clock
that have not yet been banked in case the task is currently running."
40224	1041113	"Account user cpu time to a process.
@p: the process that the cpu time gets accounted to
@cputime: the cpu time spent in user space since the last update"
40232	1041336	Add user time to cpustat.
40244	1041687	"Account guest cpu time to a process.
@p: the process that the cpu time gets accounted to
@cputime: the cpu time spent in virtual machine since the last update"
40263	1042253	"Account scaled user cpu time to a process.
@p: the process that the cpu time gets accounted to
@cputime: the cpu time spent in user space since the last update"
40274	1042623	"Account system cpu time to a process.
@p: the process that the cpu time gets accounted to
@hardirq_offset: the offset to subtract from hardirq_count()
@cputime: the cpu time spent in kernel space since the last update"
40287	1043010	Add system time to cpustat.
40299	1043501	Account for system time used
40308	1043775	"Account scaled system cpu time to a process.
@p: the process that the cpu time gets accounted to
@hardirq_offset: the offset to subtract from hardirq_count()
@cputime: the cpu time spent in kernel space since the last update"
40318	1044064	"Account for involuntary wait time.
@p: the process from which the cpu time has been stolen
@steal: the cpu time spent in involuntary wait"
40341	1044755	"This function gets called by the timer code, with HZ frequency.
We call it with interrupts disabled.

It also gets called by the fork code, when changing the parent's
timeslices."
40353	1045037	Let rq->clock advance by at least TICK_NSEC:
40358	1045199	FIXME: needed?
40374	1045491	Underflow?
40380	1045621	Spinlock count overflowing soon?
40390	1045806	Underflow?
40395	1045916	Is the spinlock portion underflowing?
40408	1046130	Print scheduling while atomic bug:
40428	1046550	Various schedule()-time debugging checks and statistics:
40435	1046804	"Test if we are atomic. Since do_exit() needs to call into
schedule() atomically, we ignore that path for now.
Otherwise, whine if we are scheduling when we should not be."
40452	1047204	Pick up the highest-prio task:
40462	1047471	"Optimization: we know that if all tasks are in
the fair class we can call that function directly:"
40477	1047788	"Will never be NULL as the idle class always
returns a non-NULL p:"
40484	1047870	schedule() is the main scheduler function.
40507	1048283	Do the rq-clock update outside the rq lock:
40536	1048964	unlocks the rq
40556	1049483	"this is the entry point to schedule() from in-kernel preemption
off of preempt_enable. Kernel preemptions off return from interrupt
occur there and call schedule directly."
40567	1049814	"If there is a non-zero preempt_count or interrupts are disabled,
we do not want to preempt the current task. Just return.."
40578	1050067	"We keep the big kernel semaphore locked, but we
clear ->lock_depth so that schedule() doesnt
auto-release the semaphore:"
40592	1050390	"Check again in case we missed a preemption opportunity
between schedule and now."
40603	1050710	"this is the entry point to schedule() from kernel preemption
off of irq context.
Note, that this is called and return with irqs disabled. This will
protect us against recursive calling from irq."
40611	1050949	Catch callers which need to be fixed
40621	1051189	"We keep the big kernel semaphore locked, but we
clear ->lock_depth so that schedule() doesnt
auto-release the semaphore:"
40637	1051557	"Check again in case we missed a preemption opportunity
between schedule and now."
40642	1051658	CONFIG_PREEMPT
40659	1052316	"The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just
wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve
number) then we wake all the non-exclusive tasks and one exclusive task.

There are circumstances in which we can try to wake a task which has already
started to run but is not in state TASK_RUNNING. try_to_wake_up() returns
zero in this (rare) case, and we handle it by continuing to scan the queue."
40680	1052901	"__wake_up - wake up threads blocked on a waitqueue.
@q: the waitqueue
@mode: which threads
@nr_exclusive: how many wake-one or wake-many threads to wake up
@key: is directly passed to the wakeup function"
40694	1053265	Same as __wake_up but called with the spinlock in wait_queue_head_t held.
40712	1053892	"__wake_up_sync - wake up threads blocked on a waitqueue.
@q: the waitqueue
@mode: which threads
@nr_exclusive: how many wake-one or wake-many threads to wake up

The sync wakeup differs that the waker knows that it will schedule
away soon, so while the target thread will be woken up, it will not
be migrated to another CPU - ie. the two threads are 'synchronized'
with each other. This can prevent needless bouncing between CPUs.

On UP it can prevent extra preemption."
40729	1054291	For internal use only
40882	1058089	"rt_mutex_setprio - set the current priority of a task
@p: task
@prio: prio value (kernel-internal form)

This function changes the 'effective' priority of a task. It does
not touch ->normal_prio like __setscheduler().

Used by the rt_mutex code to implement priority inheritance logic."
40918	1058902	"Reschedule if we are currently running on this runqueue and
our priority decreased, or if we are not currently running on
this runqueue and our priority is higher than the current's"
40942	1059384	"We have to be careful, if called from sys_setpriority(),
the task might be in the middle of scheduling on another CPU."
40950	1059662	"The RT priorities are set via sched_setscheduler(), but we still
allow the 'normal' nice value to be set - but as expected
it wont have any effect on scheduling until the task is
SCHED_FIFO/SCHED_RR:"
40973	1060158	"If the task increased its priority or is running and
lowered its priority, then reschedule its CPU:"
40986	1060413	"can_nice - check if a task can reduce its nice value
@p: task
@nice: nice value"
40989	1060537	convert nice value [19,-20] to rlimit style value [1,40]
41004	1060881	"sys_nice - change the priority of the current process.
@increment: priority increment

sys_setpriority is a more generic, but much slower function that
does similar things."
41013	1061111	"Setpriority might change our priority at the same moment.
We don't have to worry. Conceptually one call occurs first
and we have a single winner."
41045	1061761	"task_prio - return the priority value of a given task.
@p: the task in question.

This is the priority value as seen by users in /proc.
RT tasks are offset by -200. Normal tasks are centered
around 0, value goes from -16 to +15."
41054	1061931	"task_nice - return the nice value of a given task.
@p: the task in question."
41064	1062120	"idle_cpu - is a given cpu idle currently?
@cpu: the processor in question."
41073	1062288	"idle_task - return the idle task for a given cpu.
@cpu: the processor in question."
41082	1062464	"find_process_by_pid - find a process with a matching PID value.
@pid: the pid in question."
41088	1062629	Actually do priority change: must hold rq lock.
41109	1063070	we are holding p->pi_lock already
41121	1063371	"sched_setscheduler - change the scheduling policy and/or RT priority of a thread.
@p: the task in question.
@policy: new policy.
@param: structure containing the new RT priority.

NOTE that the task may be already dead."
41129	1063605	may grab non-irq protected spin_locks
41132	1063684	double check policy once rq lock held
41143	1064047	"Valid priorities for SCHED_FIFO and SCHED_RR are
1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,
SCHED_BATCH and SCHED_IDLE is 0."
41153	1064355	Allow unprivileged RT tasks to decrease priority:
41163	1064635	can't set/change the rt policy
41167	1064732	can't increase priority
41175	1064947	"Like positive nice levels, dont allow tasks to
move out of SCHED_IDLE either:"
41179	1065066	can't change other user's priorities
41191	1065350	"make sure no PI-waiters arrive (or leave) while we are
changing the priority of the task:"
41196	1065489	"To be able to change p->policy safely, the apropriate
runqueue lock must be held."
41198	1065558	recheck policy now with rq lock held
41225	1066291	"Reschedule if we are currently running on this runqueue and
our priority decreased, or if we are not currently running on
this runqueue and our priority is higher than the current's"
41269	1067206	"sys_sched_setscheduler - set/change the scheduler policy and RT priority
@pid: the pid in question.
@policy: new policy.
@param: structure containing the new RT priority."
41273	1067352	negative values for policy are not valid
41284	1067596	"sys_sched_setparam - set/change the RT priority of a thread
@pid: the pid in question.
@param: structure containing the new RT priority."
41293	1067840	"sys_sched_getscheduler - get the policy (scheduling class) of a thread
@pid: the pid in question."
41318	1068329	"sys_sched_getscheduler - get the RT priority of a thread
@pid: the pid in question.
@param: structure containing the RT priority."
41343	1068840	This one might sleep, we cannot do it with a spinlock held ...
41373	1069460	"It is not safe to call set_cpus_allowed with the
tasklist_lock held. We will bump the task_struct's
usage count and then drop tasklist_lock."
41398	1070102	"We must have raced with a concurrent cpuset
update. Just reset the cpus_allowed to the
cpuset's cpus_allowed"
41425	1070788	"sys_sched_setaffinity - set the cpu affinity of a process
@pid: pid of the process
@len: length in bytes of the bitmask pointed to by user_mask_ptr
@user_mask_ptr: user-space pointer to the new cpu mask"
41444	1071296	"Represents all cpu's present in the system
In systems capable of hotplug, this map could dynamically grow
as new cpu's are detected in the system via any platform specific
method, such as ACPI for e.g."
41488	1072263	"sys_sched_getaffinity - get the cpu affinity of a process
@pid: pid of the process
@len: length in bytes of the bitmask pointed to by user_mask_ptr
@user_mask_ptr: user-space pointer to hold the current cpu mask"
41513	1072851	"sys_sched_yield - yield the current processor to other threads.

This function yields the current CPU to other tasks. If there are no
other threads running on this CPU then this function will return."
41524	1073108	"Since we are going to call schedule() anyway, there's
no need to preempt or enable interrupts:"
41544	1073518	"The BKS might be reacquired before we have dropped
PREEMPT_ACTIVE, which could trigger a second
cond_resched() call."
41570	1074177	"cond_resched_lock() - if a reschedule is pending, drop the given lock,
call schedule, and on return reacquire the lock.

This works OK both with and without CONFIG_PREEMPT. We do strange low-level
operations here to prevent schedule() from being called twice (once via
spin_unlock(), once by hand)."
41612	1075026	"yield - yield the current processor to other threads.

This is a shortcut for kernel-space yielding - it marks the
thread runnable and calls sys_sched_yield()."
41626	1075435	"This task is about to go to sleep on IO. Increment rq->nr_iowait so
that process accounting knows that this is a task in IO wait state.

But don't do that if it is a deliberate, throttling IO wait (this task
has set its backing_dev_info: the queue against which it should throttle)"
41658	1076134	"sys_sched_get_priority_max - return maximum RT priority.
@policy: scheduling class.

this syscall returns the maximum rt_priority that can be used
by a given scheduling class."
41683	1076596	"sys_sched_get_priority_min - return minimum RT priority.
@policy: scheduling class.

this syscall returns the minimum rt_priority that can be used
by a given scheduling class."
41708	1077145	"sys_sched_rr_get_interval - return the default timeslice of a process.
@pid: pid of the process.
@interval: userspace pointer to the timeslice value.

this syscall writes the default timeslice value of a given process
into the user-space timespec buffer. A value of '0' means infinity."
41733	1077651	"Time slice is 0 for SCHED_FIFO tasks and for SCHED_OTHER
tasks that are on an otherwise idle runqueue:"
41809	1079469	"reset the NMI-timeout, listing all files on a slow
console might take alot of time:"
41823	1079769	Only show locks if all tasks are dumped:
41840	1080165	"init_idle - set up an idle thread for a given CPU
@idle: task in question
@cpu: cpu the idle task belongs to

NOTE: this function does not set the idle thread's NEED_RESCHED
flag, to make booting more robust."
41860	1080709	Set the preempt count _outside_ the spinlocks!
41868	1080961	The idle tasks have their own, simple scheduling class:
41878	1081279	"In a system that switches off the HZ timer nohz_cpu_mask
indicates which cpus entered this state. This is used
in the rcu update to wait only for active cpus. For system
which do not switch off the HZ timer nohz_cpu_mask should
always be CPU_MASK_NONE."
41889	1081648	"Increase the granularity value when there are more CPUs,
because with more CPUs the 'effective latency' as visible
to users decreases. But the relationship is not linear,
so pick a second-best guess by going with the log2 of the
number of CPUs.

This idea comes from the SD scheduler of Con Kolivas:"
41922	1082759	"This is how migration works:

1) we queue a struct migration_req structure in the source CPU's
runqueue and wake up that CPU's migration thread.
2) we down() the locked semaphore => thread blocks.
3) migration thread wakes up (implicitly it forces the migrated
thread off the CPU)
4) it gets the migration request and checks whether the migrated
task is still in the wrong runqueue.
5) if it's in the wrong runqueue then the migration thread removes
it and puts it into the right queue.
6) migration thread up()s the semaphore.
7) we wake up and the migration is done."
41932	1083112	"Change a given task's CPU affinity. Migrate the thread to a
proper CPU and schedule it away if the CPU it's executing on
is removed from the allowed bitmask.

NOTE: the caller must have a valid reference to the task, the
task must not exit() & deallocate itself prematurely. The
call is not atomic; no spinlocks may be held."
41947	1083471	Can the task run on the task's current CPU? If so, we're done
41952	1083640	Need help from migration thread: drop lock and wait.
41976	1084313	"Move (not current) task off this cpu, onto dest cpu. We're doing
this because either it can't run here any more (set_cpus_allowed()
away from this CPU, or CPU going down), or because we're
attempting to rebalance this task on exec (sched_exec).

So we race with normal scheduler movements, but that's OK, as long
as the task is no longer on this CPU.

Returns non-zero if task was successfully migrated."
41989	1084612	Already moved.
41992	1084686	Affinity changed (again).
42015	1085152	"migration_thread - this is a highprio system thread that performs
thread migration by bumping thread off CPU then 'pushing' onto
another runqueue."
42062	1086102	Wait for kthread_stop
42087	1086640	"Figure out where task on dead CPU should go, use force if necessary.
NOTE: interrupts should be disabled by the caller"
42096	1086813	On same node?
42101	1086967	On any allowed CPU?
42105	1087072	No more Mr. Nice Guy.
42114	1087432	"Try to stay on the same cpuset, where the
current cpuset may be a subset of all cpus.
The cpuset_cpus_allowed_locked() variant of
cpuset_cpus_allowed() will not block. It must be
called within calls to cpuset_lock/cpuset_unlock."
42124	1087718	"Don't tell them about moving exiting tasks or
kernel threads (both mm NULL), since they never
leave kernel."
42140	1088291	"While a dead CPU has no uninterruptible tasks queued at this point,
it might still have a nonzero ->nr_uninterruptible counter, because
for performance reasons the counter is not stricly tracking tasks to
their home CPUs. So we just add the counter to another CPU's counter,
to keep the global sum constant after CPU-down:"
42154	1088716	Run through task list and migrate tasks from the dead cpu.
42176	1089173	"Schedules idle task to be the next runnable task on current CPU.
It does so by boosting its priority to highest possible.
Used by CPU offline code."
42184	1089360	cpu has to be offline
42190	1089518	"Strictly not necessary since rest of the CPUs are stopped by now
and interrupts disabled on the current cpu."
42204	1089796	"Ensures that the idle task is using init_mm right before its cpu goes
offline."
42216	1090037	called under rq->lock with disabled interrupts
42221	1090202	Must be exiting, otherwise would be on tasklist.
42224	1090293	Cannot have done final schedule yet: would have vanished.
42233	1090490	"Drop lock around migration; if someone else moves it,
that's OK. No task can be added to this CPU, so iteration is
fine."
42241	1090687	release_task() removes task from tasklist, so we won't find dead tasks.
42258	1091009	CONFIG_HOTPLUG_CPU
42297	1091814	"In the intermediate directories, both the child directory and
procname are dynamically allocated and could fail but the mode
will always be set. In the lowest directory the names are
static strings and all have proc handlers."
42352	1093656	&table[11] is terminator
42407	1094811	may be called multiple times per register
42428	1095294	"migration_call - callback that gets triggered when a CPU is added.
Here we can start up the necessary migration thread for the new CPU."
42448	1095813	Must be high prio: stop_machine expects to yield to it.
42457	1096075	Strictly unnecessary, as first user will wake it.
42466	1096323	Unbind it from offline cpu so it can run. Fall thru.
42475	1096612	around calls to cpuset_cpus_allowed_lock()
42480	1096786	Idle task back to normal (off runqueue, low prio)
42497	1097289	"No need to migrate the tasks: it was best-effort if
they didn't take sched_hotcpu_mutex. Just wake up
the requestors."
42519	1097789	"Register at highest priority so that task migration (migrate_all_tasks)
happens before everything else."
42530	1098037	Start one for the boot CPU:
42540	1098298	Number of possible processor ids
42650	1100748	Following flags need at least 2 groups
42661	1101003	Following flags don't use groups
42682	1101397	"Does parent contain flags not in child?
WAKE_BALANCE is a subset of WAKE_AFFINE"
42685	1101570	Flags needing groups don't count if only 1 group in parent
42703	1101926	"Attach the domain 'sd' to 'cpu' as its base domain. Callers must
hold the hotplug lock."
42709	1102121	Remove the sched domains which do not contribute to scheduling.
42732	1102559	cpus with isolated domains
42735	1102672	Setup the mask of cpus configured for isolated domains
42759	1103466	"init_sched_build_groups takes the cpumask we wish to span, and a pointer
to a function which identifies what group(along with sched group) a CPU
belongs to. The return value of group_fn must be a >= 0 and < NR_CPUS
(due to the fact that we keep track of groups covered with a cpumask_t).

init_sched_build_groups will build a circular linked list of the groups
covered by the given span, and will set each group's ->cpumask correctly,
and ->cpu_power to 0."
42809	1104573	"find_next_best_node - find the next node to include in a sched_domain
@node: node whose sched_domain we're building
@used_nodes: nodes already in the sched_domain

Find the next node to include in a given scheduling domain. Simply
finds the closest node not already in the @used_nodes map.

Should use nodemask_t."
42817	1104766	Start at @node
42823	1104870	Skip already used nodes
42827	1104950	Simple min distance search
42848	1105452	"sched_domain_node_span - get a cpumask for a node's sched_domain
@node: node whose cpumask we're constructing
@size: number of nodes to include in this span

Given a node, construct a good cpumask for its sched_domain to span. It
should be one that prevents unnecessary balancing, but also spreads tasks
out optimally."
42877	1106035	SMT sched-domains:
42893	1106377	multi-core sched-domains:
42949	1107849	"The init_sched_build_groups can't handle what we want to do with node
groups, so roll our own. Now each node has its own list of groups which
gets dynamically allocated."
42986	1108788	"Only add ""power"" once for each
physical package."
42998	1109000	Free memory allocated for various sched_group structures
43051	1110429	"Initialize sched groups cpu_power.

cpu_power indicates the capacity of sched group, which is used while
distributing the load between different sched groups in a sched domain.
Typically cpu_power for all the groups in a sched domain will be same unless
there are asymmetries in the topology. If there are asymmetries, group
having more cpu_power will pickup more load compared to the group having
less cpu_power.

cpu_power will be a multiple of SCHED_LOAD_SCALE. This multiple represents
the maximum number of tasks a group can handle in the presence of other idle
or lightly loaded groups in the same sched domain."
43072	1111006	"For perf policy, if the groups in child domain share resources
(for example cores sharing some portions of the cache hierarchy
or SMT), then set this domain groups cpu_power such that each group
can handle only one task, when there are other idle groups in the
same sched domain."
43082	1111278	add cpu_power of each child group to this groups cpu_power
43093	1111530	"Build sched domains for a given set of cpus and attach the sched domains
to the individual cpus"
43103	1111743	Allocate the per-node list of sched groups
43115	1112087	Set up domains for cpus specified by the cpu_map.
43176	1113515	Set up CPU (sibling) groups
43189	1113865	Set up multi-core groups
43200	1114168	Set up physical groups
43212	1114449	Set up node groups
43218	1114608	Set up node groups
43285	1116172	Calculate CPU power for physical packages and nodes
43319	1116905	Attach the domains
43342	1117317	"current sched domains
number of sched domains in 'doms_cur'"
43348	1117573	"Special case: If a kmalloc of a doms_cur partition (array of
cpumask_t) fails, then fallback to a single sched domain,
as determined by the single cpumask_t fallback_doms."
43355	1117801	"Set up scheduler domains and groups. Callers must hold the hotplug lock.
For now this just excludes isolated cpus, but could be used to
exclude other special cases in the future."
43379	1118355	"Detach sched domains from a group of cpus specified in cpu_map
These cpus will now be attached to the NULL domain"
43412	1119532	"Partition sched domains as specified by the 'ndoms_new'
cpumasks in the array doms_new[] of cpumasks. This compares
doms_new[] to the current sched domain partitioning, doms_cur[].
It destroys each deleted domain and builds each new domain.

'doms_new' is an array of cpumask_t's of length 'ndoms_new'.
The masks don't intersect (don't overlap.) We should setup one
sched domain for each mask. CPUs not in any of the cpumasks will
not be load balanced. If the same cpumask appears both in the
current 'doms_cur' domains and in the new 'doms_new', we can leave
it as it is.

The passed in 'doms_new' should be kmalloc'd. This routine takes
ownership of it and will kfree it when done with it. If the caller
failed the kmalloc call, then it can pass in doms_new == NULL,
and partition_sched_domains() will fallback to the single partition
'fallback_doms'.

Call with hotplug lock held"
43417	1119673	always unregister in case we don't destroy any domains
43426	1119877	Destroy deleted domains
43432	1120078	no match - a current sched domain not in new doms_new[]
43438	1120159	Build new domains
43444	1120334	no match - add a new doms_new
43450	1120425	Remember the new sched domains
43540	1122761	"Force a reinitialization of the sched domains hierarchy. The domains
and groups cannot be updated in place without racing with the balancing
code, so we temporarily attach all running cpus to the NULL domain
which will prevent rebalancing while the sched domains are recalculated."
43562	1123298	Fall through and re-initialise the domains.
43568	1123402	The hotplug lock is already held by cpu_up/cpu_down
43584	1123864	XXX: Theoretical race here - CPU may be hotplugged now
43587	1123952	Move init over to a non-isolated CPU
43597	1124139	CONFIG_SMP
43674	1125998	delimiter for bitsearch:
43695	1126439	The boot idle thread does lazy MMU switching as well:
43704	1126759	"Make us the idle thread. Technically, schedule() should not be
called from this thread, however somewhere below it might be,
but because we are the idle thread, we just pick up running again
when this runqueue becomes ""idle""."
43708	1126865	During early bootup we pretend to be a normal task:
43716	1127059	ratelimiting
43762	1128101	Only normalize user tasks:
43778	1128390	"Renice negative nice level userspace
tasks back to 0:"
43796	1128729	CONFIG_MAGIC_SYSRQ
43807	1129103	"These functions are only useful for the IA64 MCA handling.

They can only be called when the whole system has been
stopped - every CPU needs to be quiescent, and no scheduling
activity can take place. Using them for anything else would
be a serious bug, and as a result, they aren't even visible
under any other configuration."
43814	1129255	"curr_task - return the current task for a given cpu.
@cpu: the processor in question.

ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!"
43834	1129992	"set_curr_task - set the current task for a given cpu.
@cpu: the processor in question.
@p: the task pointer to set.

Description: This function must only be used when non-maskable interrupts
are serviced on a separate stack. It allows the architecture to switch the
notion of the current task on a cpu in a non-blocking manner. This function
must be called with all CPU's synchronized, and interrupts disabled, the
and caller must save the original value of the current task (see
curr_task() above) and restore that value before reenabling interrupts and
re-starting the system.

ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!"
43844	1130157	allocate runqueue etc for a new task group
43917	1131642	rcu callback to free various structures associated with a task group
43925	1131873	now it should be safe to free those cfs_rqs
43939	1132087	Destroy runqueue etc associated with a task group
43952	1132364	wait for possible concurrent references to cfs_rqs complete
43960	1132640	"change task's runqueue when it moves between groups.
The caller of this function should have put the task in its new group
by now. This function just updates tsk->se.cfs_rq and tsk->se.parent to
reflect its new group."
44026	1133910	"A weight of 0 or 1 can cause arithmetics problems.
(The default weight is 1024 - so there's no practical
limitation from this.)"
44048	1134254	CONFIG_FAIR_GROUP_SCHED
44052	1134345	return corresponding task_group object of a cgroup
44065	1134722	This is early initialization for the top cgroup
44070	1134857	we support only 1-level deep hierarchical scheduler atm
44078	1135045	Bind the cgroup to task_group object we just created
44096	1135416	We don't support RT-tasks being in separate groups
44147	1136576	CONFIG_FAIR_CGROUP_SCHED
44156	1136754	"CPU accounting code for task groups.

Based on the work by Paul Menage (menage@google.com) and Balbir Singh
(balbir@in.ibm.com)."
44158	1136797	track cpu usage of a group of tasks
44161	1136911	cpuusage holds pointer to a u64-type object on every cpu
44167	1137035	return cpu accounting group corresponding to this container
44174	1137258	return cpu accounting group to which this task belongs
44181	1137459	create a new cpu accounting group
44199	1137824	destroy an existing cpu accounting group
44209	1138041	return total cpu usage (in nanoseconds) of a group
44222	1138342	"Take rq->lock to make 64-bit addition safe on 32-bit
platforms."
44247	1138811	"charge this task's execution time to its accounting group.

called with rq->lock held."
44270	1139279	CONFIG_CGROUP_CPUACCT
44282	1139584	"linux/kernel/signal.c



1997-11-02 Modified for POSIX.1b signals by Richard Henderson

2003-06-02 Jim Houston - Concurrent Computer Corp.
Changes to use preallocated sigqueue structures
to allow signals to be sent reliably."
44305	1140152	audit_signal_info()
44309	1140192	SLAB caches for signal bits.
44320	1140377	Tracers always want to know about signals..
44328	1140540	"Blocked signals are never ignored, since the
signal handler may change by the time it is
unblocked."
44332	1140674	Is it explicitly or implicitly ignored?
44341	1140939	"Re-calculate pending state from the set of locally pending
signals, globally pending signals, and blocked signals."
44382	1142106	"We must never clear the flag in another thread, or in current
when it's possible the current syscall is returning -ERESTART*.
So we don't clear it here, and only callers who know they should do."
44389	1142285	"After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.
This is superfluous when called on current, the wakeup is a harmless no-op."
44403	1142611	Given the mask, find the first available signal that should be serviced.
44447	1143468	"In order to avoid problems with ""switch_user()"", we want to make
sure that the compiler doesn't re-load ""t->user"""
44488	1144339	Flush all pending signals for a task.
44512	1144825	Flush all handlers for a task.
44545	1145907	"Notify the system that a driver wants to block all signals for this
process, and wants to be notified if any signals at all were to be
sent/acted upon. If the notifier routine returns non-zero, then the
signal will be acted upon after all. If the notifier routine returns 0,
then then signal will be blocked. Only one block per process is
allowed. priv is a pointer to private data that the notifier routine
can use to determine if the signal should be blocked or not."
44559	1146275	Notify the system that blocking has ended.
44584	1146853	"Collect the siginfo appropriate to this signal. Check if
there is another siginfo for the same signal."
44605	1147329	"Ok, it wasn't in the queue. This must be
a fast-pathed signal or we must have been
out of queue space. So zero out the info."
44643	1148055	"Dequeue a signal and return the element to the caller, which is
expected to free it.

All callers have to hold the siglock."
44650	1148245	"We only dequeue private signals from ourselves, we don't let
signalfd steal them"
44667	1148881	"itimer signal ?

itimers are process shared and we restart periodic
itimers in the signal delivery path to prevent DoS
attacks in the high resolution timer case. This is
compliant with the old way of self restarting
itimers, as the SIGALRM is a legacy signal and only
queued once. Changing the restart behaviour to
restart the timer in the signal dequeue path is
reducing the timer noise on heavy loaded !highres
systems too."
44692	1149844	"Set a marker that we have dequeued a stop signal. Our
caller might release the siglock and then the pending
stop signal it is about to process is no longer in the
pending bitmasks, but must still be cleared by a SIGCONT
(and overruled by a SIGKILL). So those cases clear this
shared flag after we've set it. Note that this flag may
remain set after the signal we return is ignored or
handled. That doesn't matter because its only purpose
is to alert stop-signal processing code when another
processor has come along and cleared the flag."
44704	1150260	"Release the siglock to ensure proper locking order
of timer locks outside of siglocks. Note, we leave
irqs disabled here, since the posix-timers code is
about to disable them again anyway."
44722	1150732	"Tell a process that it has a new active signal..

NOTE! we rely on the previous spin_lock to
lock interrupts for us! We can only be called with
""siglock"" held, and the local interrupt must
have been disabled when that got acquired!

No need to set need_resched since signal event passing
goes through ->blocked"
44735	1151158	"For SIGKILL, we want to wake it up in the stopped/traced case.
We don't check t->state here because there is a race with it
executing another processor and just now entering stopped state.
By using wake_up_state, we ensure the process will wake up and
handle its death signal."
44751	1151548	"Remove signals in mask from the pending set and queue.
Returns 1 if any signals were found.

All callers must be holding the siglock.

This version takes a sigset mask and looks at all signals,
not just those in the first mask word."
44775	1152091	"Remove signals in mask from the pending set and queue.
Returns 1 if any signals were found.

All callers must be holding the siglock."
44796	1152525	Bad permissions for sending the signal
44805	1152837	Let audit system see the signal
44820	1153205	forward decl
44829	1153635	"Handle magic process-wide effects of stop/continue signals.
Unlike the signal actions, these happen immediately at signal-generation
time regardless of blocking, ignoring, or handling. This does the
actual continuing for SIGCONT, but not the actual stopping for stop
signals. The process stop is done as a signal action for SIG_DFL."
44837	1153831	The process is in the middle of dying already.
44843	1153943	This is a stop signal. Remove SIGCONT from all queues.
44854	1154228	"Remove all stop signals from all queues,
and wake all threads."
44867	1154817	"There was a group stop in progress. We'll
pretend it finished before we got here. We are
obliged to report it to the parent: if the
SIGSTOP happened ""after"" this SIGCONT, then it
would have cleared this pending SIGCONT. If it
happened ""before"" this SIGCONT, then the parent
got the SIGCHLD about the stop finishing before
the continue happened. We do the notification
now, and it's as if the stop had finished and
the SIGCHLD was pending on entry to this kill."
44893	1155723	"If there is a handler for SIGCONT, we must make
sure that no thread returns to user mode before
we post the signal, in case it was the only
thread eligible to run the signal handler--then
it must not do anything between resuming and
running the handler. With the TIF_SIGPENDING
flag set, the thread will pause and acquire the
siglock that we hold now and until we've queued
the pending signal.

Wake up the stopped thread _after_ setting
TIF_SIGPENDING"
44908	1156139	"We were in fact stopped, and are now continued.
Notify the parent with CLD_CONTINUED."
44919	1156521	"We are not stopped, but there could be a stop
signal in the middle of being processed after
being removed from the queue. Clear that too."
44926	1156694	"Make sure that any pending stop signal already dequeued
is undone by the wakeup for SIGKILL."
44940	1156986	"Deliver the signal to listening signalfds. This must be called
with the sighand lock held."
44946	1157102	"fast-pathed signals for kernel-internal things like SIGSTOP
or SIGKILL."
44956	1157539	"Real-time signals must be queued if sent by sigqueue, or
some other real-time mechanism. It is implementation
defined whether kill() does so. We attempt to do so, on
the principle of least surprise, but since kill is not
allowed to fail with EAGAIN when low on memory we just
make sure at least one signal gets delivered and don't
pass on the info struct."
44987	1158414	"Queue overflow, abort. We may abort if the signal was rt
and sent by user using something other than kill()."
45039	1159469	Short-circuit ignored signals.
45045	1159643	"Support queueing exactly one non-rt signal, so that we
can get more detailed information about the cause of
the signal."
45065	1160230	"Force a signal that the process can't ignore: if necessary
we unblock the signal and change any SIG_IGN to SIG_DFL.

Note: If we unblock the signal, we always reset it to SIG_DFL,
since we do not want to have a signal handler that was blocked
be invoked when user space had explicitly blocked it.

We don't want to have recursive SIGSEGV's etc, for example."
45103	1161370	"Test if P wants to take SIG. After we've checked all threads with this,
it's equivalent to finding no threads not blocking SIG. Any threads not
blocking SIG were ruled out because they are not running and already
have pending signals. Such threads will dequeue from the shared queue
as soon as they're available, so putting the signal on the shared queue
will be equivalent to sending it to one such thread."
45127	1161957	"Now find a thread we can wake up to take the signal off the queue.

If the main thread wants the signal, it gets first crack.
Probably the least surprising to the average bear."
45134	1162162	"There is just one thread and it does not need to be woken.
It will dequeue unblocked signals before it runs again."
45139	1162237	Otherwise try to find a suitable thread.
45142	1162326	restart balancing at this thread
45152	1162580	"No thread needs to be woken.
Any eligible threads will see
the signal in the queue soon."
45161	1162754	"Found a killable thread. If the signal will be fatal,
then start taking the whole group down immediately."
45167	1162983	This signal will be fatal to the whole group.
45174	1163211	"Start a group exit and wake everybody up.
This way we don't have other threads
running and doing things after a slower
thread has the fatal signal pending."
45195	1163924	"There will be a core dump. We make all threads other
than the chosen one go into a group stop so that nothing
happens until it gets scheduled, takes the signal off
the shared queue, and does the core dump. This is a
little more complicated than strictly necessary, but it
keeps the signal state that winds up in the core dump
unchanged from the death state, e.g. which thread had
the core-dump signal unblocked."
45211	1164353	"The signal is already in the shared-pending queue.
Tell the chosen thread to wake up and dequeue it."
45224	1164610	Short-circuit ignored signals.
45229	1164768	This is a non-RT signal and we already have one queued.
45236	1164951	"Put this signal on the shared-pending queue, or fail with EAGAIN.
We always use the shared queue for process-wide signals,
to avoid several races."
45247	1165143	Nuke all other threads in the group.
45258	1165401	Don't bother with already dead threads
45262	1165495	SIGKILL will be handled before any pending SIGSTOP
45270	1165647	Must be called under rcu_read_lock() or with tasklist_lock read-held.
45310	1166502	"kill_pgrp_info() sends a signal to a process group: this is what the tty
control characters do (^C, ^Z etc)"
45368	1167727	"like kill_pid_info(), but doesn't use uid/euid of ""current"""
45410	1168841	"kill_something_info() interprets pid in interesting ways just like kill(2).

POSIX specifies that kill(-1,sig) is unspecified, but what we have
is probably wrong. Should make it like BSD or SYSV."
45444	1169634	These are for backward compatibility with the rest of the kernel source.
45449	1169741	"These two are the most common entry points. They send a signal
just to the specific thread."
45459	1169972	"Make sure legacy kernel users don't send in bad values
(normal paths check this in check_kill_permission)."
45468	1170228	"We need the tasklist lock even for the specific
thread case (when we don't need to follow the group
lists) in order to avoid races with ""p->sighand""
going away or changing from under us."
45489	1170761	"This is the entry point for ""process-wide"" signals.
They will go to an appropriate thread in the thread group."
45511	1171279	"When things go south during signal handling, we
will force a SIGSEGV. And if the signal that caused
the problem was already a SIGSEGV, we'll want to
make sure we don't even try to deliver the signal.."
45556	1172466	"These functions support sending signals using preallocated sigqueue
structures. This is needed ""because realtime applications cannot
afford to lose notifications of asynchronous events, like timer
expirations or I/O completions"". In the case of Posix Timers
we allocate the sigqueue structure from the timer_create. If this
allocation fails we are able to report the failure to the application
with an EAGAIN error."
45577	1172952	"If the signal is still pending remove it from the
pending queue. We must hold ->siglock while testing
q->list to serialize with collect_signal()."
45601	1173598	"The rcu based delayed sighand destroy makes it possible to
run this without tasklist lock held. The task struct itself
cannot go away as create_timer did get_task_struct().

We return -1, when the task is marked exiting, so
posix_timer_event can redirect it to the group leader"
45613	1173828	"If an SI_TIMER entry is already queue just increment
the overrun count."
45618	1173945	Short-circuit ignored signals.
45626	1174107	"Deliver the signal to listening signalfds. This must be called
with the sighand lock held."
45651	1174617	Since it_lock is held, p->sighand cannot be NULL.
45655	1174735	Short-circuit ignored signals.
45666	1174990	"If an SI_TIMER entry is already queue just increment
the overrun count. Other uses should not try to
send the signal multiple times."
45674	1175177	"Deliver the signal to listening signalfds. This must be called
with the sighand lock held."
45681	1175351	"Put this signal on the shared-pending queue.
We always use the shared queue for process-wide signals,
to avoid several races."
45694	1175673	Wake up any threads in the parent blocked in wait* syscalls.
45704	1175970	"Let a parent know about the death of a child.
For a stopped/continued status change, use do_notify_parent_cldstop instead."
45714	1176192	do_notify_parent_cldstop should have been called instead.
45733	1176814	"we are under tasklist_lock here so our parent is tied to
us and cannot exit and release its namespace.

the only it can is to switch its nsproxy with sys_unshare,
bu uncharing pid namespaces is not allowed, so we'll always
see relevant namespace

write_lock() currently calls preempt_disable() which is the
same as rcu_read_lock(), but according to Oleg, this is not
correct to rely on this"
45740	1177014	FIXME: find out whether or not this is supposed to be c*time.
45775	1178373	"We are exiting and our parent doesn't care. POSIX.1
defines special semantics for setting SIGCHLD to SIG_IGN
or setting the SA_NOCLDWAIT flag: we should be reaped
automatically and not left for our parent's wait4 call.
Rather than having the parent do it as a magic kind of
signal handler, we just set this to tell do_exit that we
can be cleaned up without becoming a zombie. Note that
we still call __wake_up_parent in this case, because a
blocked sys_wait4 might now return -ECHILD.

Whether we send SIGCHLD or not for SA_NOCLDWAIT
is implementation-defined: we do (if you don't want
it, just use SIG_IGN instead)."
45804	1179065	see comment in do_notify_parent() abot the following 3 lines
45811	1179265	FIXME: find out whether or not this is supposed to be c*time.
45837	1179972	Even if SIGCHLD is not generated, we must wake up wait4 calls.
45859	1180633	"Are we in the middle of do_coredump?
If so and our tracer is also part of the coredump stopping
is a deadlock situation, and pointless because our tracer
is dead so don't allow us to stop.
If SIGKILL was already sent before the caller unlocked
->siglock we must see ->core_waiters != 0. Otherwise it
is safe to enter schedule()."
45877	1181162	"This must be called with current->sighand->siglock held.

This should be the path for all ptrace stops.
We always set current->last_siginfo while stopped here.
That makes it a way to test a stopped process for
being ptrace-stopped vs being job-control-stopped.

If we actually decide not to stop at all because the tracer is gone,
we leave nostop_code in current->exit_code."
45883	1181332	"If there is a group stop in progress,
we must participate in the bookkeeping."
45890	1181511	Let the debugger run.
45903	1181856	"By the time we got the lock, our tracer went away.
Don't stop here."
45913	1182159	"We are back. Now reacquire the siglock before touching
last_siginfo, so that we are sure to have synchronized with
any signal-sending on another CPU that wants to examine it."
45921	1182428	"Queued signals ignored us while we were stopped for tracing.
So check for any that we should take before resuming user mode.
This sets TIF_SIGPENDING, but never clears it."
45937	1182754	Let the debugger run.
45950	1183124	"If there are no other threads in the group, or if there is
a group stop in progress and we are the last to stop,
report to the parent. When ptraced, every thread reports itself."
45962	1183397	Now we don't run again until continued.
45971	1183683	"This performs the stopping for SIGSTOP and other stop signals.
We have to stop all threads in the thread group.
Returns nonzero if we've actually stopped and released the siglock.
Returns zero if we didn't stop and still hold the siglock."
45984	1183975	"There is a group stop in progress. We don't need to
start another one."
45990	1184116	"There is no group stop already in progress.
We must initiate one now."
46001	1184404	"Setting state to TASK_STOPPED for a group
stop is always done with the siglock held,
so this check has no races."
46025	1185045	"Do appropriate magic when group_stop_count > 0.
We return nonzero if we stopped, after releasing the siglock.
We return zero if we still hold the siglock and should look
for another signal without checking group_stop_count again."
46034	1185262	"Group stop is so we can do a core dump,
We are the initiating thread, so get on with it."
46044	1185544	"Group stop is so another thread can do a core dump,
or else we are racing against a death signal.
Just punt the stop so we can get the next signal."
46050	1185668	"There is a group stop in progress. We stop
without any associated signal being in our queue."
46081	1186439	will return 0
46086	1186573	Let the debugger run.
46089	1186666	We're back. Did the debugger cancel the sig?
46099	1186968	"Update the siginfo structure if the signal has
changed. If the debugger wanted something
specific in the siginfo structure then it should
have updated *info via PTRACE_SETSIGINFO."
46108	1187238	If the (new) signal is now blocked, requeue it.
46116	1187458	Do nothing.
46119	1187536	Run the handler.
46125	1187680	"will return non-zero ""signr"" value"
46131	1187754	"Now we are doing the default action for this signal.
Default is nothing."
46136	1187887	Global init gets no signals it doesn't want.
46150	1188395	"The default action is to stop all threads in
the thread group. The job control signals
do nothing in an orphaned pgrp, but SIGSTOP
always works. Note that siglock needs to be
dropped during the call to is_orphaned_pgrp()
because of lock ordering with tasklist_lock.
This allows an intervening SIGCONT to be posted.
We need to check for that and bail out if necessary."
46154	1188523	signals can be posted during this window
46163	1188708	It released the siglock.
46170	1188832	"We didn't actually stop, due to a race
with SIGCONT or something like that."
46178	1188962	Anything else is fatal, maybe with a core dump.
46190	1189440	"If it was able to dump core, this kills all
other threads in the group and synchronizes with
their demise. If we lost the race with another
thread getting here, it set group_exit_code
first and our do_group_exit call below will use
that value and ignore the one we pass it."
46196	1189532	Death signals, no core dump.
46198	1189575	NOTREACHED
46219	1190014	System call entry points.
46236	1190412	"We don't need to get the kernel lock - this is all local to this
particular thread.. (and that's good, because this is _heavily_
used by various programs)"
46245	1190672	"This is also useful for kernel threads that want to temporarily
(or permanently) block certain signals.

NOTE! Unlike the user-mode sys_sigprocmask(), the kernel
interface happily blocks ""unblockable"" signals like SIGKILL
and friends."
46280	1191429	XXX: Don't preclude handling different sized sigset_t's.
46323	1192417	Outside the lock because only this thread touches it.
46359	1193355	"If you change siginfo_t structure, please be sure
this code is fixed accordingly.
Please remember to update the signalfd_copyinfo() function
inside fs/signalfd.c too, in case siginfo_t changes.
It should never copy any pad contained in the structure
to avoid security leaks, but must copy the generic
3 ints plus the relevant union member."
46391	1194486	"This is not generated by the kernel as of now.
But this is"
46396	1194724	this is just in case for now ...
46418	1195177	XXX: Don't preclude handling different sized sigset_t's.
46428	1195388	"Invert the set of allowed signals to get those we
want to block."
46451	1196031	"None ready -- temporarily unblock those we're
interested while we are sleeping in so that we'll
be awakened when they arrive."
46517	1197501	"The null signal is a permissions and process existence
probe. No signal is actually delivered."
46539	1198143	"sys_tgkill - send signal to one specific thread
@tgid: the thread group ID of the thread
@pid: the PID of the thread
@sig: signal to be sent

This syscall also checks the @tgid and returns -ESRCH even if the PID
exists but it's not belonging to the target process anymore. This
method solves the problem of threads exiting and PIDs getting reused."
46542	1198243	This is only valid for single tasks
46551	1198404	Send a signal to only one task, even if it's a CLONE_THREAD task.
46555	1198493	This is only valid for single tasks
46571	1198863	"Not even root can pretend to send signals from the kernel.
Nor can they impersonate a kill(), which adds source info."
46576	1198976	POSIX.1b doesn't mention process groups.
46608	1199887	"POSIX 3.3.1.3:
""Setting a signal action to SIG_IGN for a signal that is
pending shall cause the pending signal to be discarded,
whether or not it is blocked.""

""Setting a signal action to SIG_DFL for a signal that is
pending and whose default action is to ignore the signal
(for example, SIGCHLD), shall cause the pending signal to
be discarded, whether or not it is blocked"""
46662	1201181	"Note - this code used to test ss_flags incorrectly
old code may have been written using ss_flags==0
to mean ss_flags==SS_ONSTACK (as this was the only
way that worked) - this fix preserves that older
mechanism"
46702	1201919	"Some platforms have their own version with special arguments others
support only sys_rt_sigprocmask."
46752	1202940	__ARCH_WANT_SYS_SIGPROCMASK
46764	1203238	XXX: Don't preclude handling different sized sigset_t's.
46782	1203615	__ARCH_WANT_SYS_RT_SIGACTION
46788	1203730	For backwards compatibility. Functionality superseded by sigprocmask.
46792	1203783	SMP safe
46811	1204158	__ARCH_WANT_SGETMASK
46816	1204268	For backwards compatibility. Functionality superseded by sigaction.
46831	1204640	__ARCH_WANT_SYS_SIGNAL
46850	1204994	XXX: Don't preclude handling different sized sigset_t's.
46869	1205529	__ARCH_WANT_SYS_RT_SIGSUSPEND
46887	1205871	"linux/kernel/softirq.c



Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)"
46920	1206910	"- No shared variables, all the data are CPU local.
- If a softirq needs serialization, let it serialize itself
by its own spinlocks.
- Even if softirq is serialized, only local cpu is marked for
execution. Hence, we get something sort of weak cpu binding.
Though it is still not clear, will it result in better locality
or will not.

Examples:
- NET RX softirq. It is multithreaded and does not require
any global serialization.
- NET TX softirq. It kicks software netdevice queues, hence
it is logically serialized per device, but this serialization
is invisible to common code.
- Tasklets: serialized wrt itself."
46936	1207380	"we cannot loop indefinitely here to avoid userspace starvation,
but we also don't want to introduce a worst case 1/HZ latency
to the pending events, so lets the scheduler to balance
the softirq load for us."
46939	1207482	Interrupts are disabled: no need to stop preemption
46949	1207698	"This one is for softirq.c-internal use,
where hardirqs are disabled legitimately:"
46961	1207936	Were softirqs turned off above:
46966	1208070	!CONFIG_TRACE_IRQFLAGS
46972	1208213	CONFIG_TRACE_IRQFLAGS
46988	1208528	"softirqs should never be enabled by __local_bh_enable(),
it always nests inside local_bh_enable() sections:"
46999	1208810	"Special-case - softirqs can safely be enabled in
cond_resched_softirq(), or by __do_softirq(),
without processing still-pending softirqs:"
47026	1209336	Are softirqs going to be turned on now:
47032	1209527	"Keep preemption disabled until we are done with
softirq processing:"
47057	1209997	Are softirqs going to be turned on now:
47063	1210148	"Keep preemption disabled until we are done with
softirq processing:"
47085	1210747	"We restart softirq processing MAX_SOFTIRQ_RESTART times,
and we fall back to softirqd after that.

This number has been established via experimentation.
The two things to balance is latency against fairness -
we want to handle softirqs as soon as possible, but they
should not be able to lock up the box."
47103	1211159	Reset the pending bitmask before enabling irqs
47158	1211876	Enter an interrupt context.
47176	1212219	Exit an interrupt context. Process softirqs if needed and possible:
47186	1212480	Make sure that timer wheel updates are propagated
47195	1212677	This function must run with irqs disabled!
47208	1213043	"If we're in an interrupt or softirq, we're done
(this also catches softirq-disabled code). We will
actually run the softirq once we return from
the irq or softirq.

Otherwise we wake up ksoftirqd to make sure we
schedule the softirq soon."
47228	1213402	Tasklets
47235	1213546	"Some compilers disobey section attribute on statics when not
initialized -- RR"
47383	1216838	"Preempt disable stops cpu going offline.
If already offline, we'll be on wrong CPU:
don't process"
47399	1217168	Wait for kthread_stop
47418	1217700	"tasklet_kill_immediate is called to remove a tasklet which can already be
scheduled for execution on @cpu.

Unlike tasklet_kill, this function removes the tasklet
_immediately_, even if the tasklet is in TASKLET_STATE_SCHED state.

When this function is called, @cpu must be in the CPU_DEAD state."
47429	1217977	CPU is dead, so no lock needed.
47443	1218224	CPU is dead, so no lock needed.
47446	1218290	Find end, append list for that CPU.
47459	1218719	CONFIG_HOTPLUG_CPU
47488	1219454	Unbind so it can run. Fall thru.
47502	1219854	CONFIG_HOTPLUG_CPU
47525	1220289	Call a function on all processors
47555	1221087	"Author: Zwane Mwaikambo <zwane@fsmlabs.com>



This file contains the spinlock/rwlock implementations for the
SMP and the DEBUG_SPINLOCK cases. (UP-nondebug inlines them)

Note that some architectures have special knowledge about the
stack frames of these functions in their profile_pc. If you
change anything significant here that could change the stack
frame contact the architecture maintainers."
47607	1222164	"If lockdep is enabled then we use the non-preemption spin-ops
even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
not re-enabled during lock-acquire (which the preempt-spin-ops do):"
47630	1222825	"On lockdep we dont want the hand-coded irq-enable of
_raw_spin_lock_flags() code, because lockdep assumes
that interrupts are not re-enabled during lock-acquire:"
47736	1225434	CONFIG_PREEMPT:
47744	1225719	"This could be a long-held lock. We both prepare to spin for a long
time (making _this_ CPU preemptable if possible), and we also signal
towards that other CPU that it should break the lock ASAP.

(We do this in a function because inlining it would be excessive.)"
47803	1227182	"
Careful: we must exclude softirqs too, hence the
irq-disabling. We use the generic preemption-aware
function:"
47819	1227755	"Build preemption-friendly versions of the following
lock-spinning functions:

_[spin|read|write]_lock()
_[spin|read|write]_lock_irq()
_[spin|read|write]_lock_irqsave()
_[spin|read|write]_lock_bh()"
47824	1227877	CONFIG_PREEMPT
47847	1228536	"On lockdep we dont want the hand-coded irq-enable of
_raw_spin_lock_flags() code, because lockdep assumes
that interrupts are not re-enabled during lock-acquire:"
47982	1231757	Linker adds these: start and end of __lockfunc functions
47994	1232018	linux/kernel/sys.c
48063	1233662	"this is where the system-wide overflow UID and GID are defined, for
architectures that now have 32-bit UID/GID but didn't in the past"
48076	1233966	"the same as above, but for filesystems which can only store a 16-bit
UID and GID. as such, this is needed on all architectures"
48086	1234207	this indicates whether you can reboot with ctrl-alt-del: the default is yes
48094	1234339	If set, this is used for preparing the system to power off.
48133	1235143	normalize: avoid signed division (rounding problems)
48165	1235881	No processes for this user
48172	1236074	For find_user()
48186	1236372	"Ugh. To avoid negative return values, ""getpriority()"" will
not return the normal nice-value, but a negated value that
has been offset by 20 (ie it returns 40..1 instead of -20..19)
to stay compatible."
48227	1237329	No processes for this user
48237	1237569	for find_user()
48253	1237911	"emergency_restart - reboot the system

Without shutting down any hardware or taking any locks
reboot the system. This is called when we know we are in
trouble so this is our best effort to reboot. This is
safe to call in interrupt context."
48275	1238440	"kernel_restart - reboot the system
@cmd: pointer to buffer containing command to execute for restart
or %NULL

Shutdown everything and perform a clean reboot.
This is not safe to call in interrupt context."
48292	1238861	"kernel_kexec - reboot the system

Move into place and start executing a preloaded standalone
executable. If nothing was preloaded return an error."
48318	1239436	"kernel_halt - halt the system

Shutdown everything and perform a clean system halt."
48333	1239728	"kernel_power_off - power_off the system

Shutdown everything and perform a clean system power_off."
48352	1240333	"Reboot system call: for obvious reasons only root may call it,
and even root needs to set up some magic numbers in the registers
so that some mistake won't make this reboot the whole machine.
You can also set the meaning of the ctrl-alt-del-key here.

reboot doesn't sync: do that yourself before calling this."
48357	1240505	We only trust the superuser with rebooting the system.
48361	1240601	"For safety, we require ""magic"" arguments."
48371	1240957	"Instead of trying to make the power_off code look like
halt when pm_power_off is not set do it the easy way."
48442	1242266	"This function gets called by ctrl-alt-del - ie the keyboard interrupt.
As it's called within an interrupt, it may NOT sync: the only choice
is whether to reboot at once, or just ignore the ctrl-alt-del."
48470	1243197	"Unprivileged users may change the real gid to the effective gid
or vice versa. (BSD-style)

If you set the real gid at all, or set the effective gid to a value not
equal to the real gid, then the saved gid is set to the new effective gid.

This makes it possible for a setgid program to completely drop its
privileges, which is often a useful assertion to make when you are doing
a security audit over a program.

The general idea is that a program which uses just setregid() will be
100% compatible with BSD. A program which uses just setgid() will be
100% compatible with POSIX with saved IDs.

SMP: There are not races, the GIDs are checked only by filesystem
operations (as far as semantic preservation is concerned)."
48519	1244295	"setgid() is implemented like SysV w/ SAVED_IDS

SMP: Same implicit races as above."
48589	1246136	"Unprivileged users may change the real uid to the effective uid
or vice versa. (BSD-style)

If you set the real uid at all, or set the effective uid to a value not
equal to the real uid, then the saved uid is set to the new effective uid.

This makes it possible for a setuid program to completely drop its
privileges, which is often a useful assertion to make when you are doing
a security audit over a program.

The general idea is that a program which uses just setreuid() will be
100% compatible with BSD. A program which uses just setuid() will be
100% compatible with POSIX with saved IDs."
48651	1247881	"setuid() is implemented like SysV with SAVED_IDS

Note that SAVED_ID's is deficient in that a setuid root program
like sendmail, for example, cannot set its uid to be a normal
user and then switch back, because if you're root, setuid() sets
the saved uid too. If you don't like this, blame the bright people
in the POSIX committee and/or USG. Note that the BSD-style setreuid()
will allow a root program to temporarily drop privileges and be able to
regain them by swapping the real and effective uid."
48690	1248839	"This function implements a generic ability to update ruid, euid,
and suid. This allows you to implement the 4.4 compatible seteuid()."
48747	1250364	Same as above, but for rgid, egid, sgid.
48802	1251873	"""setfsuid()"" sets the fsuid - the uid used for filesystem checks. This
is used for ""access()"" and for the NFS daemon (letting nfsd stay at
whatever uid it wants to). It normally shadows ""euid"", except when
explicitly set by setfsuid() or for access.."
48831	1252504	Samma p svenska..
48861	1253331	"In the SMP world we might just be unlucky and have one of
the times increment as we use it. Since the value is an
atomically safe type this is just fine. Conceptually its
as if the syscall took an instant longer to occur."
48903	1254615	"This needs some heavy checking ...
I just haven't the stomach for it. I also don't fully
understand sessions/pgrp etc. Let somebody who does explain it.

OK, I think I have the protection semantics right.... this is really
only important on a multi-user system anyway, to make sure one user
can't send a signal to a process owned by another. -TYT, 12/12/91

Auch. Had to add the 'did_exec' flag to conform completely to POSIX.
LBT 04.03.94"
48920	1255039	"From this point forward we keep holding onto the tasklist lock
so that our parent does not change from under us. -DaveM"
48974	1256038	All paths lead to here, thus we are safe. -DaveM
49007	1256654	SMP - assuming writes are word atomic this is fine
49045	1257349	Fail if I am already a session leader
49056	1257696	"Fail if a process group id already exists that equals the
proposed session id.

Don't check if session id == 1 because kernel threads use this
session id and so the check will always fail and make it so
init cannot successfully call setsid."
49076	1258125	Supplementary group IDs
49078	1258195	init to 2 - one for init_task, one to ensure it is never freed
49088	1258503	Make sure we always allocate at least one indirect block pointer
49132	1259512	export the group_info to a user-space array
49152	1260015	fill a group_info from a user-space array - it must be allocated already
49172	1260469	a simple Shell sort
49179	1260672	nothing
49201	1261126	a simple bsearch
49224	1261544	validate and set current->group_info
49256	1262133	"SMP: Nobody else can change our grouplist. Thus we are
safe."
49261	1262230	no need to grab task_lock here; it cannot change
49280	1262557	"SMP: Our groups are copy-on-write. We can set them safely
without another task interfering."
49309	1263138	Check whether we're fsgid/egid or in the supplemental group..
49389	1264598	"Only setdomainname; getdomainname can be implemented by calling
uname()"
49428	1265473	Back compatibility for getrlimit. Needed for some apps.
49477	1266822	"The caller is asking for an immediate RLIMIT_CPU
expiry. But we use the zero value to mean ""it was
never set"". So let's cheat and make it one second
instead"
49493	1267247	"RLIMIT_CPU handling. Note that the kernel fails to return an error
code if it rejected the user's attempt to set RLIMIT_CPU. This is a
very long-standing error, and fixing it now risks breakage of
applications, so we live with it"
49544	1269557	"It would make sense to put struct rusage in the task_struct,
except that would make the task_struct be *really big*. After
task_struct gets moved into malloc'ed memory, it would
make sense to do this. It will make moving the rest of the information
a lot simpler! (Which we're not doing right now because we're not
measuring them yet).

When sampling multiple threads for RUSAGE_SELF, under SMP we might have
races with threads incrementing their own counters. But since word
reads are atomic, we either get new values or old values and we don't
care which for the sums. We always take the siglock to protect reading
the c* fields from p->signal from races with exit.c updating those
fields when reaping, so a sample either gets all the additions of a
given child after it's reaped, or none so this sample is before reaping.

Locking:
We need to take the siglock for CHILDEREN, SELF and BOTH
for the cases current multithreaded, non-current single threaded
non-current multithreaded. Thread traversal is now safe with
the siglock held.
Strictly speaking, we donot need to take the siglock if we are current and
single threaded, as no one else can take our signal_struct away, no one
else can reap the children to update signal->c* counters, and no one else
can race with the signal-> fields. If we do not take any lock, the
signal-> fields could be read out of order while another thread was just
exiting. So we should place a read memory barrier when we avoid the lock.
On the writer side, write memory barrier is implied in __exit_signal
as __exit_signal releases the siglock spinlock after updating the signal->
fields. But we don't do this yet to keep things simple."
49766	1274702	"orderly_poweroff - Trigger an orderly system poweroff
@force: force poweroff if command execution fails

This may be called from any context to trigger a system shutdown.
If the orderly shutdown fails, it will force an immediate shutdown."
49802	1275585	"I guess this should try to kick off some daemon to
sync and poweroff asap. Or not even bother syncing
if we're doing an emergency shutdown?"
49844	1276555	KERN_OSREV not used
49847	1276619	"KERN_SECUREMASK not used
KERN_PROF not used"
49853	1276832	def CONFIG_SECURITY_CAPABILITIES
49864	1277042	"KERN_NAMETRANS not used
KERN_PPC_HTABRECLAIM not used
KERN_PPC_ZEROPAGED not used"
49873	1277318	"KERN_RTSIGNR not used
KERN_RTSIGMAX not used"
49878	1277467	KERN_MSGPOOL not used
49934	1279461	VM_PAGEBUF unused
49945	1279926	VM_SWAP_TOKEN_TIMEOUT unused
49962	1280503	NET_CORE_DESTROY_DELAY unused
49964	1280586	NET_CORE_FASTROUTE unused
49973	1280750	"NET_CORE_HOT_LIST_LENGTH unused
NET_CORE_DIVERT_VERSION unused
NET_CORE_NO_CONG_THRESH unused
NET_CORE_NO_CONG unused
NET_CORE_LO_CONG unused
NET_CORE_MOD_CONG unused"
49985	1281280	"NET_UNIX_DESTROY_DELAY unused
NET_UNIX_DELETE_DELAY unused"
50119	1287554	NET_IPV4_FIB_HASH unused
50127	1287911	NET_IPV4_AUTOCONFIG unused
50133	1288207	NET_IPV4_TCP_MAX_KA_PROBES unused
50139	1288502	NET_IPV4_IP_MASQ_DEBUG unused
50143	1288658	NET_TCP_SYN_TAILDROP unused
50152	1288948	"NET_IPV4_ICMP_SOURCEQUENCH_RATE unused
NET_IPV4_ICMP_DESTUNREACH_RATE unused
NET_IPV4_ICMP_TIMEEXCEED_RATE unused
NET_IPV4_ICMP_PARAMPROB_RATE unused
NET_IPV4_ICMP_ECHOREPLY_RATE unused"
50156	1289351	NET_IPV4_ALWAYS_DEFRAG unused
50186	1290762	NET_TCP_DEFAULT_WIN_SCALE unused
50189	1290901	NET_TCP_BIC_BETA unused
50207	1291893	NET_IPQ_QMAX
50213	1292072	NET_IPX_FORWARDING unused
50352	1297936	IPQ_QMAX
50511	1304483	"NET_ETHER not used
NET_802 not used"
50524	1305115	NET_ECONET not used
50590	1307048	"FS_MAXINODE unused
FS_NRDQUOT unused
FS_MAXDQUOT unused"
50595	1307215	"FS_NRSUPER unused
FS_MAXUPSER unused"
50632	1308303	"DEV_MAC_HID_KEYBOARD_SENDS_LINUX_KEYCODES unused
DEV_MAC_HID_KEYBOARD_LOCK_KEYCODES unused"
50636	1308600	DEV_MAC_HID_ADB_MOUSE_SENDS_KEYCODES unused
50690	1310154	DEV_HWMON unused
51005	1317728	"CTL_S390DBF_STOPPABLE
CTL_S390DBF_ACTIVE"
51025	1318261	"CTL_PM_SUSPEND
CTL_PM_CMODE
CTL_PM_P0
CTL_PM_CM"
51039	1318680	CTL_PROC not used
51045	1318883	CTL_CPU not used
51148	1320898	"Don't complain about the classic default
sysctl strategy routine. Maybe later we
can get the tables fixed and complain about
this."
51311	1325304	def CONFIG_SECURITY_CAPABILITIES
51379	1327755	"sysctl.c: General linux system control interface

Begun 24 March 1995, Stephen Tweedie
Added /proc support, Dec 1995
Added bdflush entry and intvec min/max checking, 2/23/96, Tom Dyas.
Added hooks for /proc/sys/net (minor, minor patch), 96/4/1, Mike Shaver.
Added kernel/java-{interpreter,appletviewer}, 96/5/10, Mike Shaver.
Dynamic registration fixes, Stephen Tweedie.
Added kswapd-interval, ctrl-alt-del, printk stuff, 1/8/97, Chris Horn.
Made sysctl support optional via CONFIG_SYSCTL, 1/10/97, Chris
Horn.
Added proc_doulongvec_ms_jiffies_minmax, 09/08/99, Carlos H. Bauer.
Added proc_doulongvec_minmax, 09/08/99, Carlos H. Bauer.
Changed linked lists to use list.h instead of lists.h, 02/24/00, Bill
Wendling.
The list_for_each() macro wasn't appropriate for the sysctl loop.
Removed it and replaced it with older style, 03/23/00, Bill Wendling"
51422	1328749	External variables not in a header file.
51445	1329431	Constants used for minimum and maximum
51458	1329688	this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID
51539	1331574	The default sysctl tables:
51583	1332309	"NOTE: do not add new entries to this table unless you have read
Documentation/sysctl/ctl_unnumbered.txt"
51591	1332421	"100 usecs
1 second
0 usecs
1 second"
51753	1336547	def CONFIG_SECURITY_CAPABILITIES
52141	1345222	"NOTE: do not add new entries to this table unless you have read
Documentation/sysctl/ctl_unnumbered.txt"
52227	1347443	read-only
52447	1353067	"NOTE: do not add new entries to this table unless you have read
Documentation/sysctl/ctl_unnumbered.txt"
52591	1356163	"NOTE: do not add new entries to this table unless you have read
Documentation/sysctl/ctl_unnumbered.txt"
52615	1356618	called under sysctl_lock
52624	1356771	called under sysctl_lock
52632	1356972	called under sysctl_lock, will reacquire if has to wait
52638	1357169	"if p->used is 0, nobody will ever touch that entry again;
we'll eliminate all paths to it before dropping sysctl_lock"
52650	1357480	"do not remove from the list until nobody holds it; walking the
list in do_sysctl() relies on that."
52736	1359281	CONFIG_SYSCTL_SYSCALL
52741	1359419	"sysctl_perm does NOT grant the superuser all rights automatically, because
some sysctl variables are readonly even to root."
52797	1360562	Perform the actual read/write of a sysctl table entry.
52822	1361121	"If there is no strategy routine, or if the strategy returns
zero, proceed with automatic r/w"
52831	1361314	CONFIG_SYSCTL_SYSCALL
52919	1364343	"register_sysctl_table - register a sysctl hierarchy
@table: the top-level table structure

Register a sysctl table hierarchy. @table should be a filled in ctl_table
array. An entry with a ctl_name of 0 terminates the table.

The members of the &struct ctl_table structure are used as follows:

ctl_name - This is the numeric sysctl value used by sysctl(2). The number
must be unique within that level of sysctl

procname - the name of the sysctl file under /proc/sys. Set to %NULL to not
enter a sysctl file

data - a pointer to data for use by proc_handler

maxlen - the maximum size in bytes of the data

mode - the file permissions for the /proc/sys file, and for sysctl(2)

child - a pointer to the child sysctl table if this entry is a directory, or
%NULL.

proc_handler - the text handler routine (described below)

strategy - the strategy routine (described below)

de - for internal use by the sysctl routines

extra1, extra2 - extra pointers usable by the proc handler routines

Leaf nodes in the sysctl tree will be represented by a single file
under /proc; non-leaf nodes will be represented by directories.

sysctl(2) can automatically manage read and write requests through
the sysctl table. The data and maxlen fields of the ctl_table
struct enable minimal validation of the values being written to be
performed, and the mode field allows minimal authentication.

More sophisticated management can be enabled by the provision of a
strategy routine with the table entry. This will be called before
any automatic read or write of the data is performed.

The strategy routine may return

< 0 - Error occurred (error is passed to user process)

0 - OK - proceed with automatic read or write.

> 0 - OK - read or write has been done by the strategy routine, so
return immediately.

There must be a proc_handler routine for any terminal nodes
mirrored under /proc/sys (non-terminals are handled by a built-in
directory handler). Several default handlers are available to
cover common cases -

proc_dostring(), proc_dointvec(), proc_dointvec_jiffies(),
proc_dointvec_userhz_jiffies(), proc_dointvec_minmax(),
proc_doulongvec_ms_jiffies_minmax(), proc_doulongvec_minmax()

It is the handler's job to read the input buffer from user memory
and process it. The handler should return 0 on success.

This routine returns %NULL on a failure to register, and a pointer
to the table header on success."
52947	1365150	"unregister_sysctl_table - unregister a sysctl table hierarchy
@header: the header returned from register_sysctl_table

Unregisters the sysctl table and all children. proc entries may not
actually be removed until they are no longer used by anyone."
52961	1365394	!CONFIG_SYSCTL
52971	1365580	CONFIG_SYSCTL
52975	1365609	/proc/sys support
53054	1367233	"proc_dostring - read a string sysctl
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position

Reads/writes a string from/to the user buffer. If the kernel
buffer provided is not large enough to hold the string, the
string is truncated. The copied string is %NULL-terminated.
If the string is being read by the user process, it is copied
and a newline '\n' is added. It is truncated if the buffer is
not large enough.

Returns 0 on success."
53215	1370531	"proc_dointvec - read a vector of integers
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position

Reads/writes up to table->maxlen/sizeof(unsigned int) integer
values from/to the user buffer, treated as an ASCII string.

Returns 0 on success."
53255	1371372	init may raise the set.
53270	1371772	def CONFIG_SECURITY_CAPABILITIES
53274	1371818	Taint values can only be increased
53333	1373301	"proc_dointvec_minmax - read a vector of integers with min/max values
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position

Reads/writes up to table->maxlen/sizeof(unsigned int) integer
values from/to the user buffer, treated as an ASCII string.

This routine will ensure the values are within the range specified by
table->extra1 (min) and table->extra2 (max).

Returns 0 on success."
53477	1376630	"proc_doulongvec_minmax - read a vector of long integers with min/max values
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position

Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long
values from/to the user buffer, treated as an ASCII string.

This routine will ensure the values are within the range specified by
table->extra1 (min) and table->extra2 (max).

Returns 0 on success."
53501	1377543	"proc_doulongvec_ms_jiffies_minmax - read a vector of millisecond values with min/max values
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position

Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long
values from/to the user buffer, treated as an ASCII string. The values
are treated as milliseconds, and converted to jiffies when they are stored.

This routine will ensure the values are within the range specified by
table->extra1 (min) and table->extra2 (max).

Returns 0 on success."
53594	1379695	"proc_dointvec_jiffies - read a vector of integers as seconds
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position

Reads/writes up to table->maxlen/sizeof(unsigned int) integer
values from/to the user buffer, treated as an ASCII string.
The values read are assumed to be in seconds, and are converted into
jiffies.

Returns 0 on success."
53617	1380507	"proc_dointvec_userhz_jiffies - read a vector of integers as 1/USER_HZ seconds
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: pointer to the file position

Reads/writes up to table->maxlen/sizeof(unsigned int) integer
values from/to the user buffer, treated as an ASCII string.
The values read are assumed to be in 1/USER_HZ seconds, and
are converted into jiffies.

Returns 0 on success."
53641	1381351	"proc_dointvec_ms_jiffies - read a vector of integers as 1 milliseconds
@table: the sysctl table
@write: %TRUE if this is a write to the sysctl file
@filp: the file structure
@buffer: the user buffer
@lenp: the size of the user buffer
@ppos: file position
@ppos: the current position in the file

Reads/writes up to table->maxlen/sizeof(unsigned int) integer
values from/to the user buffer, treated as an ASCII string.
The values read are assumed to be in 1/1000 seconds, and
are converted into jiffies.

Returns 0 on success."
53671	1382106	CONFIG_PROC_FS
53730	1383583	CONFIG_PROC_FS
53736	1383657	General sysctl support routines
53738	1383735	The generic sysctl data routine (used if no strategy routine supplied)
53745	1383947	Get out of I don't have a variable
53772	1384484	The generic string strategy routine:
53787	1384922	This shouldn't trigger for a well-formed sysctl
53791	1385035	Copy up to a max of bufsize-1 bytes of the string
53818	1385715	"This function makes sure that all of the integers in the vector
are between the minimum and maximum values given in the arrays
table->extra1 and table->extra2, respectively."
53854	1386514	Strategy function to convert jiffies to seconds
53888	1387262	Strategy function to convert jiffies to seconds
53924	1388022	CONFIG_SYSCTL_SYSCALL
53937	1388300	If no error reading the parameters then just -ENOSYS ...
53979	1389287	CONFIG_SYSCTL_SYSCALL
53987	1389436	Check args->nlen.
53991	1389569	Read in the sysctl name for better debug message logging
53996	1389705	Ignore accesses to kernel.version
54015	1390136	"No sense putting this after each symbol definition, twice,
exception granted :-)"
54048	1391347	"taskstats.c - Export per-task statistics to userland


(C) Balbir Singh, IBM Corp. 2006

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details."
54066	1391803	"Maximum length of a cpumask that can be specified in
the TASKSTATS_CMD_ATTR_REGISTER/DEREGISTER_CPUMASK attribute"
54118	1393103	If new attributes are added, please revisit this allocation
54141	1393540	Send taskstats data in @skb to listener with nl_pid @pid
54159	1393905	Send taskstats data in @skb to listeners registered for @cpu's exit data
54199	1394761	Delete invalidated entries
54232	1395424	"Each accounting subsystem adds calls to its functions to
fill in relevant parts of struct taskstsats as follows

per-task-foo(stats, tsk);"
54236	1395491	fill in basic acct fields
54242	1395651	fill in extended acct fields
54245	1395720	Define err: label here if needed
54261	1396048	"Add additional stats from live tasks except zombie thread group
leaders who are already counted with the dead tasks"
54283	1396522	"Accounting subsystem can call its functions here to
fill in relevant parts of struct taskstsats as follows

per-task-foo(stats, tsk);"
54299	1396856	"Accounting subsystems can also add calls here to modify
fields of taskstats."
54317	1397224	"Each accounting subsystem calls its functions here to
accumalate its per-task stats for tsk, into the per-tgid structure

per-task-foo(tsk->signal->stats, tsk);"
54352	1398013	Deregister or cleanup
54482	1400682	Size includes space for nested attributes
54526	1401756	No problem if kmem_cache_zalloc() fails
54542	1402073	Send pid data out on exit
54557	1402359	Size includes space for nested attributes
54563	1402569	PID + STATS + TGID + STATS
54565	1402634	fill the tsk->signal->stats structure
54587	1403072	Doesn't matter if tsk is the leader or the last group member leaving
54616	1403676	Needed early in initialization
54657	1404561	"late initcall ensures initialization of statistics collection
mechanisms precedes initialization of the taskstats interface"
54668	1404830	"linux/kernel/time.c



This file contains the interface functions for the various
time related system calls: time, stime, gettimeofday, settimeofday,
adjtime"
54687	1405629	"Modification history kernel/time.c

1993-09-02 Philip Gladstone
Created file with time related functions from sched.c and adjtimex()
1993-10-08 Torsten Duwe
adjtime interface update and CMOS clock write code
1995-08-13 Torsten Duwe
kernel PLL updated to 1994-12-13 specs (rfc-1589)
1999-01-16 Ulrich Windl
Introduced error checking for many cases in adjtimex().
Updated NTP code according to technical memorandum Jan '96
""A Kernel Model for Precision Timekeeping"" by Dave Mills
Allow time_constant larger than MAXTC(6) for NTP v4 (MAXTC == 10)
(Even though the technical memorandum forbids it)
2004-07-14 Christoph Lameter
Added getnstimeofday to allow the posix timer functions to return
with nanosecond accuracy"
54704	1406038	"The timezone where the local system is located. Used as a default by some
programs who obtain this value by using gettimeofday."
54716	1406343	"sys_time() can be implemented in user-level using
sys_gettimeofday(). Is this for backwards compatibility? If so,
why not move it into the appropriate arch directory (for those
architectures that need it)."
54733	1406718	"sys_stime() can be implemented in user-level using
sys_settimeofday(). Is this for backwards compatibility? If so,
why not move it into the appropriate arch directory (for those
architectures that need it)."
54753	1407001	__ARCH_WANT_SYS_TIME
54785	1408022	"Adjust the time obtained from the CMOS to be UTC time instead of
local time.

This is ugly, but preferable to the alternatives. Otherwise we
would either need to write a program to do it in /etc/rc (and risk
confusion if the program gets run more than once; it would also be
hard to make the program warp the clock precisely n hours) or
compile in the timezone information into the kernel. Bad, bad....

- TYT, 1992-01-01

The best thing to do is to keep the CMOS clock in universal time (UTC)
as real UNIX machines always do it. This avoids all headaches about
daylight saving times and warping kernel clocks."
54804	1408814	"In case for some reason the CMOS clock has not already been running
in UTC, but in some local time: The first time we set the timezone,
we will warp the clock so that it is ticking UTC time instead of
local time. Presumably, if someone is setting the timezone then we
are running in an environment where the programs understand about
timezones. This should be done at boot time in the /etc/rc script,
as soon as possible, so that the clock can be set right. Otherwise,
various programs will get confused when the clock gets warped."
54819	1409105	SMP safe, global irq locking makes it work.
54832	1409342	"SMP safe, again the code in arch/foo/time.c should
globally block out interrupts when it runs."
54861	1409992	Local copy of parameter
54867	1410125	"Copy the user data space into the kernel copy
structure. But bear in mind that the structures
may change"
54880	1410452	"current_fs_time - Return FS time
@sb: Superblock.

Return the current time truncated to the time granularity supported by
the fs."
54893	1410775	"Convert jiffies to milliseconds and back.

Avoid unnecessary multiplications/divisions in the
two most common HZ cases:"
54929	1411849	"timespec_trunc - Truncate timespec to a granularity
@t: Timespec
@gran: Granularity in ns.

Truncate a timespec to a granularity. gran must be smaller than a second.
Always rounds down.

This function should be only used for timestamps returned by
current_kernel_time() or CURRENT_TIME, not with do_gettimeofday() because
it doesn't handle the better resolution of the later."
54936	1412083	"Division is pretty slow so avoid it for common cases.
Currently current_kernel_time() never returns better than
jiffies resolution. Exploit that."
54938	1412142	nothing
54952	1412437	"Simulate gettimeofday using do_gettimeofday which only allows a timeval
and therefore only yields usec accuracy"
54978	1413323	"Converts Gregorian date to seconds since 1970-01-01 00:00:00.
Assumes input in normal date format, i.e. 1980-12-31 23:59:59
=> year=1980, mon=12, day=31, hour=23, min=59, sec=59.

[For the Julian calendar (which was used in Russia before 1917,
Britain & colonies before 1752, anywhere else before 1582,
and is still in use by some communities) leave out the
-year/100+year/400 terms, and add 10.]

This algorithm was first published by Gauss (I think).

WARNING: this function will overflow on 2106-02-07 06:28:16 on
machines were long is 32-bit! (However, as time_t is signed, we
will already get problems at other places on 2038-01-19 03:14:08)"
54986	1413577	1..12 -> 11,12,1..10
54988	1413662	Puts Feb last since it has leap day
54997	1413822	"now have hours
now have minutes
finally seconds"
55015	1414365	"set_normalized_timespec - set timespec sec and nsec parts and normalize

@ts: pointer to timespec variable to be set
@sec: seconds to set
@nsec: nanoseconds to set

Set seconds and nanoseconds field of a timespec variable and
normalize to the timespec storage format

Note: The tv_nsec part is always in the range of
0 <= tv_nsec < NSEC_PER_SEC
For negative values only the tv_sec field is negative !"
55035	1414786	"ns_to_timespec - Convert nanoseconds to timespec
@nsec: the nanoseconds value to be converted

Returns the timespec representation of the nsec parameter."
55056	1415280	"ns_to_timeval - Convert nanoseconds to timeval
@nsec: the nanoseconds value to be converted

Returns the timeval representation of the nsec parameter."
55082	1415977	"When we convert to jiffies then we interpret incoming values
the following way:

- negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET)

- 'too large' values [that would result in larger than
MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.

- all other values are converted to jiffies by either multiplying
the input value by a factor or dividing it with a factor

We must also be careful about 32-bit overflows."
55087	1416085	Negative value, means infinite timeout:
55096	1416332	"HZ is equal to or smaller than 1000, and 1000 is a nice
round multiple of HZ, divide with the factor between them,
but round upwards:"
55105	1416645	"HZ is larger than 1000, and HZ is a nice round multiple of
1000 - simply multiply with the factor between them.

But first make sure the multiplication result cannot
overflow:"
55115	1416906	"Generic case - multiply, round and divide. But first
check that if we are doing a net multiplication, that
we wouldnt overflow:"
55148	1417954	"The TICK_NSEC - 1 rounds up the value to the next resolution. Note
that a remainder subtract here would not do the right thing as the
resolution values don't fall on second boundries. I.e. the line:
nsec -= nsec % TICK_NSEC; is NOT a correct resolution rounding.

Rather, we just shift the bits off the right.

The >> (NSEC_JIFFIE_SC - SEC_JIFFIE_SC) converts the scaled nsec
value to a scaled second value."
55172	1418507	"Convert jiffies to nanoseconds and separate with
one divide."
55189	1419215	"Same for ""timeval""

Well, almost. The problem here is that the real system resolution is
in nanoseconds and the value being converted is in micro seconds.
Also for some machines (those that use HZ = 1024, in-particular),
there is a LARGE error in the tick size in microseconds.

The solution we use is to do the rounding AFTER we convert the
microsecond part. Thus the USEC_ROUND, the bits to be shifted off.
Instruction wise, this should cost only an additional add with carry
instruction above the way it was done above."
55211	1419759	"Convert jiffies to nanoseconds and separate with
one divide."
55223	1420029	Convert jiffies/jiffies_64 to clock_t and back.
55245	1420501	Don't worry about loss of precision here ..
55249	1420588	.. but do try to contain it here
55266	1420944	"There are better ways that don't overflow early,
but even this doesn't overflow in hundreds of years
in 64 bits, so.."
55287	1421445	"max relative error 5.7e-8 (1.8s per year) for USER_HZ <= 1024,
overflow after 64.99 years.
exact for HZ=60, 72, 90, 120, 144, 180, 300, 600, 900, ..."
55332	1422752	"linux/kernel/timer.c

Kernel internal timers, basic process system calls



1997-01-28 Modified by Finn Arne Gangstad to make timers scale better.

1997-09-10 Updated NTP code according to technical memorandum Jan '96
""A Kernel Model for Precision Timekeeping"" by Dave Mills
1998-12-24 Fixed a xtime SMP race (we need the xtime_lock rw spinlock to
serialize accesses to xtime/lost_ticks).

1999-03-10 Improved NTP compatibility by Ulrich Windl
2002-05-31 Move sys_sysinfo here and make its locking sane, Robert Love
2000-10-05 Implemented scalable SMP per-CPU timer handling.

Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar"
55365	1423489	per-CPU timer vector definitions:
55402	1424426	"Note that all tvec_bases is 2 byte aligned and lower bit of
base in timer_list is guaranteed to be zero. Use the LSB for
the new flag to indicate whether the timer is deferrable"
55405	1424519	Functions below help us manage 'deferrable' flag
55448	1426085	"__round_jiffies - function to round jiffies to a full second
@j: the time in (absolute) jiffies that should be rounded
@cpu: the processor number on which the timeout will happen

__round_jiffies() rounds an absolute time in the future (in jiffies)
up or down to (approximately) full seconds. This is useful for timers
for which the exact time they fire does not matter too much, as long as
they fire approximately every X seconds.

By rounding these timers to whole seconds, all such timers will fire
at the same time, rather than at various times spread out. The goal
of this is to have the CPU wake up less, which saves power.

The exact rounding is skewed for each processor to avoid all
processors firing at the exact same time, which could lead
to lock contention or spurious cache line bouncing.

The return value is the rounded version of the @j parameter."
55461	1426515	"We don't want all cpus firing their timers at once hitting the
same lock or cachelines, so we skew each extra cpu with an extra
3 jiffies. This 3 jiffies came originally from the mm/ code which
already did this.
The skew is done by adding 3*cpunr, then round, then subtract this
extra offset again."
55472	1426836	"If the target jiffie is just after a whole second (which can happen
due to delays of the timer irq, long irq off times etc etc) then
we should round down to the whole second, not up. Use 1/4th second
as cutoff for this rounding as an extreme upper bound for this.
round down"
55474	1426906	round up
55477	1426990	now that we have rounded, subtract the extra skew again
55480	1427066	rounding ate our timeout entirely;
55505	1428073	"__round_jiffies_relative - function to round jiffies to a full second
@j: the time in (relative) jiffies that should be rounded
@cpu: the processor number on which the timeout will happen

__round_jiffies_relative() rounds a time delta in the future (in jiffies)
up or down to (approximately) full seconds. This is useful for timers
for which the exact time they fire does not matter too much, as long as
they fire approximately every X seconds.

By rounding these timers to whole seconds, all such timers will fire
at the same time, rather than at various times spread out. The goal
of this is to have the CPU wake up less, which saves power.

The exact rounding is skewed for each processor to avoid all
processors firing at the exact same time, which could lead
to lock contention or spurious cache line bouncing.

The return value is the rounded version of the @j parameter."
55513	1428409	"In theory the following code can skip a jiffy in case jiffies
increments right between the addition and the later subtraction.
However since the entire point of this function is to use approximate
timeouts, it's entirely ok to not handle that."
55532	1429183	"round_jiffies - function to round jiffies to a full second
@j: the time in (absolute) jiffies that should be rounded

round_jiffies() rounds an absolute time in the future (in jiffies)
up or down to (approximately) full seconds. This is useful for timers
for which the exact time they fire does not matter too much, as long as
they fire approximately every X seconds.

By rounding these timers to whole seconds, all such timers will fire
at the same time, rather than at various times spread out. The goal
of this is to have the CPU wake up less, which saves power.

The return value is the rounded version of the @j parameter."
55553	1430006	"round_jiffies_relative - function to round jiffies to a full second
@j: the time in (relative) jiffies that should be rounded

round_jiffies_relative() rounds a time delta in the future (in jiffies)
up or down to (approximately) full seconds. This is useful for timers
for which the exact time they fire does not matter too much, as long as
they fire approximately every X seconds.

By rounding these timers to whole seconds, all such timers will fire
at the same time, rather than at various times spread out. The goal
of this is to have the CPU wake up less, which saves power.

The return value is the rounded version of the @j parameter."
55591	1431142	"Can happen if you add a timer with expires == jiffies,
or you set a timer to go off in the past"
55597	1431333	"If the timeout is larger than 0xffffffff on 64-bit
architectures then we use the maximum timeout:"
55607	1431545	Timers are FIFO:
55643	1432409	"init_timer - initialize a timer.
@timer: the timer to be initialized

init_timer() must be done to a timer prior calling *any* of the
other timer functions."
55685	1433550	"We are using hashed locking: holding per_cpu(tvec_bases).lock
means that all timers which are tied to this base via timer->base are
locked, and the base itself is locked too.

So __run_timers/migrate_timers can safely modify all timers which could
be found on ->tvX lists.

When the timer's base is locked, and the timer removed from list, it is
possible to set timer->base = NULL and drop the lock: the timer remains
locked."
55699	1433968	The timer has migrated to another CPU
55731	1434719	"We are trying to schedule the timer on the local CPU.
However we can't change timer's base while it is running,
otherwise del_timer_sync() can't detect that the timer's
handler yet has not finished. This also guarantees that
the timer is serialized wrt itself."
55733	1434811	See the comment in lock_timer_base()
55757	1435306	"add_timer_on - start a timer on a particular CPU
@timer: the timer to be added
@cpu: the CPU to start it on

This is not very scalable on SMP. Double adds are not possible."
55791	1436443	"mod_timer - modify a timer's timeout
@timer: the timer to be modified
@expires: new timeout in jiffies

mod_timer() is a more efficient way to update the expire field of an
active timer (if the timer is inactive it will be activated)

mod_timer(timer, expires) is equivalent to:

del_timer(timer); timer->expires = expires; add_timer(timer);

Note that if there are multiple unserialized concurrent users of the
same timer, then mod_timer() is the only safe way to modify the timeout,
since add_timer() cannot modify an already running timer.

The function returns whether it has modified a pending timer or not.
(ie. mod_timer() of an inactive timer returns 0, mod_timer() of an
active timer returns 1.)"
55801	1436731	"This is a common optimization triggered by the
networking code - if the timer is re-modified
to be the same thing then just return:"
55820	1437213	"del_timer - deactive a timer.
@timer: the timer to be deactivated

del_timer() deactivates a timer - this works on both active and inactive
timers.

The function returns whether it has deactivated a pending timer or not.
(ie. del_timer() of an inactive timer returns 0, del_timer() of an
active timer returns 1.)"
55851	1437899	"try_to_del_timer_sync - Try to deactivate a timer
@timer: timer do del

This function tries to deactivate a timer. Upon successful (ret >= 0)
exit the timer is not queued and the handler is not running on any CPU.

It must not be called from interrupt contexts."
55892	1439039	"del_timer_sync - deactivate a timer and wait for the handler to finish.
@timer: the timer to be deactivated

This function only differs from del_timer() on SMP: besides deactivating
the timer it also makes sure the handler has finished executing on other
CPUs.

Synchronization rules: Callers must prevent restarting of the timer,
otherwise this function is meaningless. It must not be called from
interrupt contexts. The caller must not hold locks which would prevent
completion of the timer's handler. The timer's handler must not call
add_timer_on(). Upon exit the timer is not queued and the handler is
not running on any CPU.

The function returns whether it has deactivated a pending timer or not."
55908	1439345	cascade all the timers from tv up one level
55917	1439561	"We are removing _all_ timers from the list, so we
don't have to detach them individually."
55934	1440005	"__run_timers - run all expired timers (if any) on this CPU.
@base: the timer vector to be processed.

This function cascades all vectors and executes all expired timer
vectors."
55947	1440318	Cascade timers:
55992	1441559	"Find out when the next timer event is due to happen. This
is used on S/390 to stop all activity when a cpus is idle.
This functions needs to be called disabled."
56001	1441858	Look for timer events in tv1.
56010	1442100	Look at the cascade bucket(s)?
56019	1442280	Calculate the next cascade event
56024	1442381	Check tv2-tv5.
56043	1442859	"Do we still search for the first timer or are
we looking up the cascade buckets ?"
56045	1442916	Look at the cascade bucket(s)?
56063	1443234	"Check, if the next hrtimer event is before the next timer wheel
event:"
56076	1443547	Expired timer available, let it expire in the next tick
56086	1443772	"Limit the delta to the max value, which is checked in
tick_nohz_stop_sched_tick():"
56095	1444057	"Take rounding errors in to account and make sure, that it
expires in the next tick. Otherwise we go into an endless
ping pong due to tick_nohz_stop_sched_tick() retriggering
the timer softirq"
56107	1444283	"get_next_timer_interrupt - return the jiffy of the next pending timer
@now: current time (in jiffies)"
56148	1445254	"Called from the timer interrupt handler to charge one tick to the current
process. user_tick is 1 if the tick is user time, 0 for system."
56154	1445430	Note: this timer irq context must be accounted for as well.
56165	1445660	Nr of active tasks - counted in fixed-point numbers
56178	1446031	"Hmm.. Changed this, as the GNU make sources (load.c) seems to
imply that avenrun[] is the standard name for this kind of thing.
Nothing else seems to be standardized: the fractional size etc
all seem to differ on different machines.

Requires xtime_lock to access."
56186	1446218	"calc_load - given tick count, update the avenrun load estimates.
This is called while holding a write_lock on xtime_lock."
56189	1446317	fixed-point
56206	1446711	This function runs timers and the timer-tq in bottom half context.
56219	1446980	Called by the local, per-CPU timer interrupt on SMP.
56229	1447159	"Called by the timer interrupt. xtime_lock must already be taken
by the timer IRQ!"
56240	1447430	"The 64-bit jiffies value is not atomic - you MUST NOT read it
without sampling the sequence number in xtime_lock.
jiffies is defined in the linker script..."
56253	1447661	"For backwards compatibility? This can be done in libc so Alpha
and all newer ports shouldn't need it."
56266	1447903	"The Alpha uses getxpid, getxuid, and getxgid instead. Maybe this
should be moved into arch/i386 instead?"
56276	1448256	"sys_getpid - return the thread group id of the current process

Note, despite the name, this returns the tgid not the pid. The tgid and
the pid are identical unless CLONE_THREAD was specified on clone() in
which case the tgid is the same in all threads of the same group.

This is SMP safe as current->tgid does not change."
56287	1448546	"Accessing ->real_parent is not SMP-safe, it could
change from under us. However, we can use a stale
value of ->real_parent under rcu_read_lock(), see
release_task()->call_rcu(delayed_put_task_struct)."
56301	1448794	Only we change this so SMP safe
56307	1448894	Only we change this so SMP safe
56313	1448994	Only we change this so SMP safe
56319	1449094	Only we change this so SMP safe
56355	1450236	"schedule_timeout - sleep until timeout
@timeout: timeout value in jiffies

Make the current task sleep until @timeout jiffies have
elapsed. The routine will return immediately unless
the current task state has been set (see set_current_state()).

You can set the task state as follows -

%TASK_UNINTERRUPTIBLE - at least @timeout jiffies are guaranteed to
pass before the routine returns. The routine will return 0

%TASK_INTERRUPTIBLE - the routine may return early if a signal is
delivered to the current task. In this case the remaining time
in jiffies will be returned, or 0 if the timer expired in time

The current task state is guaranteed to be TASK_RUNNING when this
routine returns.

Specifying a @timeout value of %MAX_SCHEDULE_TIMEOUT will schedule
the CPU away without a bound on the timeout. In this case the return
value will be %MAX_SCHEDULE_TIMEOUT.

In all cases the return value is guaranteed to be non-negative."
56370	1450695	"These two special cases are useful to be comfortable
in the caller. Nothing more. We could take
MAX_SCHEDULE_TIMEOUT from one of the negative value
but I' d like to return a valid offset (>=0) to allow
the caller to do everything it want with the retval."
56380	1451042	"Another bit of PARANOID. Note that the retval will be
0 since no piece of kernel is supposed to do a check
for a negative retval of schedule_timeout() (since it
should never happens anyway). You just have the printk()
that will tell you if something is gone wrong and where."
56407	1451610	"We can use __set_current_state() here because schedule_timeout() calls
schedule() unconditionally."
56422	1452062	"Thread ID - the internal kernel ""pid"""
56431	1452214	"do_sysinfo - fill in sysinfo struct
@info: pointer to buffer to fill"
56449	1452639	"This is annoying. The below is the same thing
posix_get_clock_monotonic() does, but it wants to
take the lock which we want to cover the loads stuff
too."
56478	1453514	"If the sum of all the available memory (i.e. ram + swap)
is less than can be stored in a 32 bit unsigned long then
we can be binary compatible with 2.2.x kernels. If not,
well, in that case 2.2.x was broken anyways...

-Erik Andersen <andersee@debian.org>"
56499	1454049	"If mem_total did not overflow, multiply all memory values by
info->mem_unit and set it to 1. This leaves things compatible
with 2.2.x, and also retains compatibility with earlier 2.4.x
kernels..."
56531	1454687	"lockdep: we want to track each per-CPU base as a separate lock-class,
but timer-bases are kmalloc()-ed, so we need to attach separate
keys to them:"
56546	1455000	The APs use this path later in boot
56553	1455181	Make sure that tvec_base is 2 byte aligned
56566	1455530	"This is for the boot CPU - we use compile-time
static initialisation because per-cpu memory isn't
ready yet and because the memory allocators are not
initialised either."
56634	1457233	CONFIG_HOTPLUG_CPU
56678	1458142	"msleep - sleep safely even with waitqueue interruptions
@msecs: Time in milliseconds to sleep for"
56692	1458435	"msleep_interruptible - sleep waiting for signals
@msecs: Time in milliseconds to sleep for"
56712	1458971	"The ""user cache"".

(C) Copyright 1991-2000 Linus Torvalds

We have a per-user structure to keep track of how many
processes, files etc the user has claimed, in order to be
able to have per-user limits for system resources."
56726	1459309	"UID task count cache, to get fast user lookup in ""alloc_uid""
when changing user ID's (ie setuid() and friends)."
56742	1460018	"The uidhash_lock is mostly taken from process context, but it is
occasionally also taken from softirq/tasklet context, when
task-structs get RCU-freed. Hence all locking must be softirq-safe.
But free_uid() is also called with local interrupts disabled, and running
local_bh_enable() with local interrupts disabled is an error - we'll run
softirq callbacks, and they can unconditionally enable interrupts, and
the caller of free_uid() didn't expect that.."
56762	1460472	These routines must be called with the uidhash spinlock held!
56811	1461371	CONFIG_FAIR_USER_SCHED
56817	1461592	CONFIG_FAIR_USER_SCHED
56821	1461735	represents /sys/kernel/uids directory
56834	1461958	return cpu shares held by the user
56842	1462203	modify cpu shares held by the user
56867	1462809	"Create ""/sys/kernel/uids/<uid>"" directory and
""/sys/kernel/uids/<uid>/cpu_share"" file for this user."
56875	1463058	create under /sys/kernel/uids dir
56898	1463551	"create these in sysfs filesystem:
""/sys/kernel/uids"" directory
""/sys/kernel/uids/0"" directory (for root user)
""/sys/kernel/uids/0/cpu_share"" file (for root user)"
56903	1463637	create under /sys/kernel dir
56918	1464002	"work function to remove sysfs directory for a user and free up
corresponding structures."
56928	1464297	"Make uid_hash_remove() + sysfs_remove_file() + kobject_del()
atomic."
56960	1464984	"IRQs are disabled and uidhash_lock is held upon function entry.
IRQ state (as stored in flags) is restored and uidhash_lock released
upon function exit."
56963	1465090	restore back the count
56971	1465292	CONFIG_FAIR_USER_SCHED && CONFIG_SYSFS
56980	1465628	"IRQs are disabled and uidhash_lock is held upon function entry.
IRQ state (as stored in flags) is restored and uidhash_lock released
upon function exit."
56998	1466092	"Locate the user_struct for the passed UID. If found, take a ref on it. The
caller must undo that ref with free_uid().

If the user_struct could not be found, return NULL."
57032	1466855	"Make uid_hash_find() + user_kobject_create() + uid_hash_insert()
atomic."
57088	1468079	"Before adding this, check whether we raced
on adding the same user already.."
57096	1468376	"This case is not possible when CONFIG_FAIR_USER_SCHED
is defined, since we serialize alloc_uid() using
uids_mutex. Hence no need to call
sched_destroy_user() or remove_user_sysfs_dir()."
57121	1468909	"What if a process setreuid()'s and this brings the
new uid over his NPROC rlimit? We can check this now
cheaply with the new uid cache, so if it matters
we should be checking for it. -DaveM"
57135	1469323	"We need to synchronize with __sigqueue_alloc()
doing a get_uid(p->user).. If that saw the old
user value, we need to wait until it has exited
its critical region before we can free the old
structure."
57155	1469742	"collapse the chains so that the user_struct-s will
be still alive, but not in hashes. subsequent free_uid()
will free them."
57178	1470290	Insert the root user immediately (init already runs as root)
57204	1470946	"linux/kernel/workqueue.c

Generic mechanism for defining kernel helper threads for running
arbitrary tasks in process context.

Started by Ingo Molnar, Copyright (C) 2002

Derived from the taskqueue/keventd code by:

David Woodhouse <dwmw2@infradead.org>
Andrew Morton <andrewm@uow.edu.au>
Kai Petzke <wpp@marie.physik.tu-berlin.de>
Theodore Ts'o <tytso@mit.edu>

Made to use alloc_percpu by Christoph Lameter <clameter@sgi.com>."
57227	1471498	"The per-CPU workqueue (if single thread, we always use the first
possible cpu)."
57239	1471764	Detect run_workqueue() recursion depth
57245	1471883	"The externally visible workqueue abstraction is an array of
per-CPU workqueues:"
57251	1472063	Freeze threads during suspend
57258	1472247	"All the per-cpu workqueues on the system, for hotplug cpu to add/remove
threads to each one as cpus come/go."
57270	1472715	"_cpu_down() first removes CPU from cpu_online_map, then CPU_DEAD
flushes cwq->worklist. This means that flush_workqueue/wait_on_work
which comes in between can't use for_each_online_cpu(). We could
use cpu_possible_map, the cpumask below is more a documentation
than optimization."
57273	1472833	If it's single threaded, it isn't in the list of workqueues.
57296	1473387	"Set the workqueue on which a work item is to be run
- Must *only* be called if the pending flag is set"
57322	1474103	"Ensure that we get the right work->data if we see the
result of list_add() below, see try_to_grab_pending()."
57331	1474283	Preempt must be disabled.
57351	1474800	"queue_work - queue work on a workqueue
@wq: workqueue to use
@work: work to queue

Returns 0 if @work was already on a queue, non-zero otherwise.

We queue the work to the CPU it was submitted, but there is no
guarantee that it will be processed by that CPU."
57382	1475661	"queue_delayed_work - queue work on a workqueue after delay
@wq: workqueue to use
@dwork: delayable work to queue
@delay: number of jiffies to wait before queueing

Returns 0 if @work was already on a queue, non-zero otherwise."
57402	1476263	"queue_delayed_work_on - queue work on specific CPU after delay
@cpu: CPU number to execute work on
@wq: workqueue to use
@dwork: work to queue
@delay: number of jiffies to wait before queueing

Returns 0 if @work was already on a queue, non-zero otherwise."
57414	1476677	This stores cwq for the moment, for the timer_fn
57435	1477182	morton gets to eat his hat
57452	1477787	"It is permissible to free the struct work_struct
from inside the function that is called from it,
this we need to take into account for lockdep too.
To avoid bogus ""held lock freed"" warnings as well
as problems when looking into work->lockdep_map,
make a copy and use that here."
57545	1479954	"Probably keventd trying to flush its own queue. So simply run
it by hand rather than deadlocking."
57578	1480772	"flush_workqueue - ensure that any scheduled work has run to completion.
@wq: workqueue to flush

Forces execution of the workqueue and blocks until its completion.
This is typically used in driver shutdown handlers.

We sleep until all works which were queued on entry have been handled,
but we are not livelocked by new incoming ones.

This function used to run the workqueues itself. Now we just wait for the
helper threads to do it."
57595	1481265	"Upon a successful return (>= 0), the caller ""owns"" WORK_STRUCT_PENDING bit,
so this work can't be re-armed in any way."
57607	1481596	"The queueing is in progress, or it is already queued. Try to
steal it from ->worklist without clearing WORK_STRUCT_PENDING."
57619	1481873	"This work is queued, but perhaps we locked the wrong cwq.
In that case we must see the new value after rmb(), see
insert_work()->wmb()."
57707	1484015	"cancel_work_sync - block until a work_struct's callback has terminated
@work: the work which is to be flushed

Returns true if @work was pending.

cancel_work_sync() will cancel the work if it is queued. If the work's
callback appears to be running, cancel_work_sync() will block until it
has completed.

It is possible to use this function if the work re-queues itself. It can
cancel the work even if it migrates to another workqueue, however in that
case it only guarantees that work->func() has completed on the last queued
workqueue.

cancel_work_sync(&delayed_work->work) should be used only if ->timer is not
pending, otherwise it goes into a busy-wait loop until the timer expires.

The caller must ensure that workqueue_struct on which this work was last
queued can't be destroyed before this function returns."
57722	1484450	"cancel_delayed_work_sync - reliably kill off a delayed work.
@dwork: the delayed work struct

Returns true if @dwork was pending.

It is possible to use this function if @dwork rearms itself via queue_work()
or queue_delayed_work(). See also the comment for cancel_work_sync()."
57736	1484810	"schedule_work - put work task in global workqueue
@work: job to be done

This puts a job in the kernel-global workqueue."
57750	1485197	"schedule_delayed_work - put work task in global workqueue after delay
@dwork: job to be done
@delay: number of jiffies to wait or 0 for immediate execution

After waiting for a given time this puts a job in the kernel-global
workqueue."
57767	1485713	"schedule_delayed_work_on - queue work in global workqueue on CPU after delay
@cpu: cpu to use
@dwork: job to be done
@delay: number of jiffies to wait

After waiting for a given time this puts a job in the kernel-global
workqueue on the specified CPU."
57785	1486177	"schedule_on_each_cpu - call a function on each online CPU from keventd
@func: the function to call

Returns zero on success.
Returns -ve errno on failure.

Appears to be racy against CPU hotplug.

schedule_on_each_cpu() is very slow."
57795	1486375	CPU hotplug
57826	1487232	"execute_in_process_context - reliably execute the routine with user context
@fn: the function to execute
@ew: guaranteed storage for the execute work structure (must
be available when the work executes)

Executes the function immediately if process context is available,
otherwise schedules the function for delayed execution.

Returns: 0 - function was executed
1 - function was scheduled for execution"
57849	1487673	preempt-safe: keventd is per-cpu
57889	1488601	"Nobody can add the work_struct to this cwq,
if (caller is __create_workqueue)
nobody should see this wq
else // caller is CPU_UP_PREPARE
cpu is not on cpu_online_map
so we can abort safely."
57966	1490262	"Our caller is either destroy_workqueue() or CPU_DEAD,
workqueue_mutex protects cwq->thread"
57983	1490913	"If the caller is CPU_DEAD and cwq->worklist was not empty,
a concurrent flush_workqueue() can insert a barrier after us.
However, in that case run_workqueue() won't return and check
kthread_should_stop() until it flushes all work_struct's.
When ->worklist becomes empty it is safe to exit because no
more work_structs can be queued on this cwq: flush_workqueue
checks list_empty(), and a ""normal"" queue_work() can't use
a dead CPU."
57993	1491131	"destroy_workqueue - safely terminate a workqueue
@wq: target workqueue

Safely destroy a workqueue. All work currently pending will be done first."
58091	1493751	"audit -- definition of audit_context structure and supporting types





This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA"
58100	1493908	"0 = no checking
1 = put_count checking
2 = verbose put_count checking"
58105	1494103	"At task start time, the audit_state is set in the audit_context using
a per-task filter. At syscall entry, the audit_state is augmented by
the syscall filter."
58109	1494250	"Do not create per-task audit_context.
No syscall-specific audit records can
be generated."
58113	1494413	"Create the per-task audit_context,
but don't necessarily fill it in at
syscall entry time (i.e., filter
instead)."
58119	1494664	"Create the per-task audit_context,
and always fill it in at syscall
entry time. This makes a full
syscall record available if some
other part of the kernel decides it
should be recorded."
58123	1494850	"Create the per-task audit_context,
always fill it in at syscall entry
time, and always write out the audit
record at syscall exit time."
58126	1494871	Rule lists
58136	1494954	"reference count
insertion path
associated superblock device
associated inode number
associated parent
entry in parent->watches list
associated rules"
58156	1495581	for data alloc on list rules
58158	1495648	ties events to rules
58164	1495739	"quick access to arch field
quick access to an inode field
associated watch
associated watched tree
entry in audit_{watch,tree}.rules list"
58231	1498220	never called
58261	1499119	"linux/kernel/capability.c



Integrated into 2.1.97+, Andrew G. Morgan <morgan@kernel.org>
30 May 2002: Cleanup, Robert M. Love <rml@tech9.net>"
58274	1499441	"This lock protects task->cap_* for all tasks including current.
Locking rule: acquire this prior to tasklist_lock."
58281	1499671	"For sys_getproccap() and sys_setproccap(), any of the three
capability set pointers may be NULL -- indicating that that set is
uninteresting and/or not to be changed."
58291	1499989	"sys_capget - get the capabilities of a given process.
@header: pointer to struct that contains capability version and
target pid data
@dataptr: pointer to struct that contains the effective, permitted,
and inheritable capabilities that are returned

Returns 0 on success and < 0 on error."
58342	1501094	"cap_set_pg - set capabilities for all processes in a given process
group. We call this holding task_capability_lock and tasklist_lock."
58376	1501885	"cap_set_all - set capabilities for all processes other than init
and self. We call this holding task_capability_lock and tasklist_lock."
58420	1503301	"sys_capset - set capabilities for a process or a group of processes
@header: pointer to struct that contains capability version and
target pid data
@data: pointer to struct that contains the effective, permitted,
and inheritable capabilities

Set capabilities for a given process, all processes, or all
processes in a given process group.

The restrictions on setting capabilities are specified as:

[pid is for the 'target' task. 'current' is the calling task.]

I: any raised capabilities must be a subset of the (old current) permitted
P: any raised capabilities must be a subset of the (old current) permitted
E: must be set to a subset of (new target) permitted

Returns 0 on success and < 0 on error."
58464	1504417	"having verified that the proposed changes are legal,
we now put them into effect."
58466	1504494	all procs other than current and init
58469	1504596	all procs in process group
58510	1505354	"kernel/ccontainer_debug.c - Example cgroup subsystem that
exposes debug info



Developed by Paul Menage (menage@google.com)"
58623	1508121	"kernel/configs.c
Echo the kernel .config file used to build the kernel






This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
NON INFRINGEMENT. See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA."
58633	1508332	"**********************************************
the actual current config file"
58644	1508703	"Define kernel_config_data and kernel_config_data_size, which contains the
wrapped and compressed configuration file. The file is first compressed
with gzip and then bounded by two eight byte magic numbers to allow
extraction from a binary kernel image:

IKCFG_ST
<image>
IKCFG_ED"
58657	1509003	"**********************************************
globals and useful constants"
58674	1509475	"***********************************************
ikconfig_init: start up everything we need to"
58680	1509638	create the current config file
58693	1509897	"***********************************************
ikconfig_cleanup: clean up our mess"
58707	1510268	CONFIG_IKCONFIG_PROC
58713	1510376	"CPU control.
(C) 2001, 2002, 2003, 2004 Rusty Russell

This code is licenced under the GPL."
58726	1510709	This protects CPUs going up and down...
58734	1510971	"If set, cpu_up and cpu_down will return -EBUSY and do nothing.
Should always be manipulated under cpu_add_remove_lock"
58739	1511108	Crappy recursive lock-takers in cpufreq! Complain loudly about idiots
58774	1511785	CONFIG_HOTPLUG_CPU
58776	1511831	Need to know about CPUs going up/down?
58820	1512865	Take this CPU down.
58828	1513100	Ensure this CPU doesn't handle any more interrupts.
58834	1513269	"Force idle task to run as soon as we yield: it should
immediately notice cpu is offline and die quickly."
58839	1513349	Requires cpu_add_remove_lock to be held
58871	1514203	Ensure that we are not runnable on dying cpu
58882	1514535	CPU didn't die: tell everyone. Can't complain.
58894	1514778	Wait for it to sleep (leaving idle task).
58898	1514850	This actually kills the CPU.
58901	1514937	CPU is completely dead: tell everyone. Too late to complain.
58930	1515480	CONFIG_HOTPLUG_CPU
58932	1515527	Requires cpu_add_remove_lock to be held
58953	1516122	Arch-specific enabling code.
58961	1516313	Now call notifier in preparation.
59007	1517466	"We take down all of the non-boot CPUs in one shot to avoid races
with the userspace trying to use the CPU hotplug at the same time"
59025	1517914	Make sure the CPUs won't be enabled by someone else
59038	1518167	Allow everyone to use the CPU hotplug again
59057	1518617	CONFIG_PM_SLEEP_SMP
59072	1519221	"delayacct.c - per-task delay accounting



This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it would be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See
the GNU General Public License for more details."
59080	1519424	Delay accounting turned on/off
59106	1519987	"Start accounting for a delay statistic using
its starting timestamp (@start)"
59116	1520216	"Finish delay accounting for a statistic using
its timestamps (@start, @end), accumalator (@total) and @count"
59145	1520835	Swapin block I/O
59150	1521013	Other block I/O
59167	1521439	"Though tsk->delays accessed later, early exit avoids
unnecessary returning of other data"
59185	1522005	"No locking available for sched_info (and too expensive to add one)
Mitigate by taking snapshot of values"
59199	1522419	zero XXX_total, non-zero XXX_count implies XXX stat overflowed
59237	1523699	"$Id: dma.c,v 1.7 1994/12/28 03:35:33 root Exp root $
linux/kernel/dma.c: A DMA channel allocator. Inspired by linux/kernel/irq.c.

Written by Hennus Bergman, 1992.

1994/12/26: Changes by Alex Nash to fix a minor bug in /proc/dma.
In the previous version the reported device could end up being wrong,
if a device requested a DMA channel that was already in use.
[It also happened to remove the sizeof(char *) == sizeof(int)
assumption introduced because of those /proc/dma patches. -- Hennus]"
59264	1524585	"A note on resource allocation:

All drivers needing DMA channels, should allocate and release them
through the public routines `request_dma()' and `free_dma()'.

In order to avoid problems, all processes should allocate resources in
the same sequence and release them in the reverse order.

So, when allocating DMAs and IRQs, first allocate the IRQ, then the DMA.
When releasing them, first release the DMA, then release the IRQ.
If you don't, you may cause allocation requests to fail unnecessarily.
This doesn't really matter now, but it will once we get real semaphores
in the kernel."
59271	1524684	If our port doesn't define this it has no PC like DMA
59279	1524880	"Channel n is busy iff dma_chan_busy[n].lock != 0.
DMA0 used to be reserved for DRAM refresh, but apparently not any more...
DMA4 is reserved for cascading."
59295	1525185	"request_dma - request and reserve a system DMA channel
@dmanr: DMA channel number
@device_id: reserving device ID string, used in /proc/dma"
59306	1525464	old flag was 0, now contains 1 to indicate busy
59308	1525495	request_dma
59313	1525583	"free_dma - free a reserved system DMA channel
@dmanr: DMA channel number"
59326	1525864	free_dma
59362	1526407	MAX_DMA_CHANNELS
59401	1527236	"Handling of different ABIs (personalities).

We group personalities into execution domains which have their
own handlers for kernel entry points, signal mapping, etc...

2001-05-06 Complete rewrite, Christoph Hellwig (hch@infradead.org)"
59433	1527865	"name
lcall7 causes a seg fault.
PER_LINUX personality.
PER_LINUX personality.
Identity map signals.
- both ways."
59559	1530513	"At that point we are guaranteed to be the sole owner of
current->fs."
59620	1532271	"Rewritten by Rusty Russell, on the backs of many others...


This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA"
59629	1532538	Sort the kernel's built-in exception table
59635	1532694	Given an address, look for it in the exception tables.
59678	1533576	"linux/kernel/futex_compat.c

Futex compatibililty routines."
59690	1533775	Fetch a robust-list pointer. Bit 0 signals PI futexes:
59718	1534463	"Walk curr->robust_list (very carefully, it's a userspace list!)
and mark any locks found there dead, and notify any waiters.

We silently return on any sign of list-walking problem."
59731	1534891	"Fetch the list head (which was registered earlier, via
sys_set_robust_list()):"
59736	1535012	Fetch the relative futex offset:
59742	1535160	"Fetch any possibly pending lock-add first, and handle it
if it exists:"
59747	1535306	avoid warning with gcc
59752	1535457	"Fetch the next entry in the list before calling
handle_futex_death:"
59758	1535654	"A pending lock might already be on the list, so
dont process it twice:"
59772	1535940	Avoid excessively long or circular lists:
59862	1537804	linux/kernel/itimer.c
59864	1537870	These are all the functions necessary to implement itimers
59882	1538283	"itimer_get_remtime - get remaining time for the timer

@timer: the timer to read

Returns the delta between the expiry time and now, which can be
less than zero or 1usec for an pending expired timer"
59891	1538559	"Racy but safe: if the itimer expires after the above
hrtimer_get_remtime() call but before this condition
then we return 0 - which is correct."
59926	1539522	about to fire
59952	1540317	about to fire
59986	1541009	The timer is automagically restarted, when interval != 0
59999	1541299	Returns true if the timeval is in canonical form
60012	1541650	Validate the timevals in value.
60027	1542067	We are sharing ->siglock with it_real_fn()
60107	1544517	"alarm_setitimer - set alarm in seconds

@seconds: number of seconds until alarm
0 disables the alarm

Returns the remaining time in seconds of a pending timer or 0 when
the timer is not active.

On 32 bit machines the seconds value is limited to (INT_MAX/2) to avoid
negative timeval settings which would cause immediate expiry."
60125	1544977	"We can't return 0 if we have an alarm pending ... And we'd
better return too much than too little anyway"
60347	1552046	"A simple kernel FIFO implementation.



This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA."
60365	1552604	"kfifo_init - allocates a new FIFO using a preallocated buffer
@buffer: the preallocated buffer to be used.
@size: the size of the internal buffer, this have to be a power of 2.
@gfp_mask: get_free_pages mask, passed to kmalloc()
@lock: the lock to be used to protect the fifo buffer

Do NOT pass the kfifo to kfifo_free() after use! Simply free the
&struct kfifo with kfree()."
60371	1552766	size must be a power of 2
60394	1553315	"kfifo_alloc - allocates a new FIFO and its internal buffer
@size: the size of the internal buffer to be allocated.
@gfp_mask: get_free_pages mask, passed to kmalloc()
@lock: the lock to be used to protect the fifo buffer

The size will be rounded-up to a power of 2."
60403	1553559	"round up to the next power of 2, since our 'let the indices
wrap' tachnique works only in this case."
60425	1553931	"kfifo_free - frees the FIFO
@fifo: the fifo to be freed."
60445	1554502	"__kfifo_put - puts some data into the FIFO, no locking version
@fifo: the fifo to be used.
@buffer: the data to be added.
@len: the length of the data to be added.

This function copies at most @len bytes from the @buffer into
the FIFO depending on the free space, and returns the number of
bytes copied.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these functions."
60456	1554772	"Ensure that we sample the fifo->out index -before- we
start putting bytes into the kfifo."
60460	1554848	first put the data starting from fifo->in to buffer end
60464	1555039	then put the rest (if any) at the beginning of the buffer
60470	1555182	"Ensure that we add the bytes to the kfifo -before-
we update the fifo->in index."
60491	1555703	"__kfifo_get - gets some data from the FIFO, no locking version
@fifo: the fifo to be used.
@buffer: where the data must be copied.
@len: the size of the destination buffer.

This function copies at most @len bytes from the FIFO into the
@buffer and returns the number of copied bytes.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these functions."
60502	1555960	"Ensure that we sample the fifo->in index -before- we
start removing bytes from the kfifo."
60506	1556043	first get the data from fifo->out until the end of the buffer
60510	1556238	then get the rest (if any) from the beginning of the buffer
60516	1556387	"Ensure that we remove the bytes from the kfifo -before-
we update the fifo->out index."
60534	1556687	"kernel/ksysfs.c - sysfs attributes in /sys/kernel, which
are not related to any other subsystem



This file is release under the GPLv2"
60552	1557182	current uevent sequence number
60559	1557402	uevent helper program, used during early boo
60598	1558426	CONFIG_KEXEC
60602	1558512	Make /sys/kernel/notes give the raw contents of our kernel .notes section.
60658	1559835	"Create ""/sys/kernel/uids"" directory and corresponding root user's
directory under it."
60673	1560174	"Kernel thread helper functions.


Creation is done via kthreadd, so that we get a clean environment
even if we're invoked from userspace (think modprobe, hotplug cpu,
etc.)."
60690	1560611	Information passed to kthread() from kthreadd.
60695	1560744	Result passed back to kthread_create() from kthreadd.
60710	1561017	"Thread stopping is done by setthing this var: lock serializes
multiple kthread_stop calls."
60720	1561370	"kthread_should_stop - should this kthread return now?

When someone calls kthread_stop() on your kthread, it will be woken
and this will return true. You should then return, and your return
value will be passed through to kthread_stop()."
60734	1561671	Copy data: it's on kthread's stack
60738	1561784	OK, tell user we're spawned, wait for stop or wakeup
60746	1561991	It might have exited on its own, w/o kthread_stop. Check.
60758	1562253	We want our own signal handler (we take no signals by default).
60789	1563398	"kthread_create - create a kthread.
@threadfn: the function to run until signal_pending(current).
@data: data ptr for @threadfn.
@namefmt: printf-style name for the thread.

Description: This helper function creates and names a kernel
thread. The thread will be stopped: use wake_up_process() to start
it. See also kthread_run(), kthread_create_on_cpu().

When woken, the thread will run @threadfn() with @data as its
argument. @threadfn() can either call do_exit() directly if it is a
standalone thread for which noone will call kthread_stop(), or
return when 'kthread_should_stop()' is true (which means
kthread_stop() has been called). The return value should be zero
or a negative error number; it will be passed to kthread_stop().

Returns a task_struct or ERR_PTR(-ENOMEM)."
60828	1564479	"kthread_bind - bind a just-created kthread to a cpu.
@k: thread created by kthread_create().
@cpu: cpu (might not be online, must be possible) for @k to run on.

Description: This function is equivalent to set_cpus_allowed(),
except that @cpu doesn't need to be online, and the thread must be
stopped (i.e., just returned from kthread_create())."
60835	1564677	Must have done schedule() in kthread() before we set_task_cpu
60854	1565314	"kthread_stop - stop a thread created by kthread_create().
@k: thread created by kthread_create().

Sets kthread_should_stop() for @k to return true, wakes it, and
waits for it to exit. Your threadfn() must not call do_exit()
itself if you use this function! This can also be called after
kthread_create() instead of calling wake_up_process(): the thread
will exit without calling threadfn().

Returns the result of threadfn(), or %-EINTR if wake_up_process()
was never called."
60861	1565473	It could exit after stop_info.k set, but before wake_up_process.
60864	1565564	Must init completion *before* thread sees kthread_stop_info.k
60868	1565682	Now set kthread_should_stop() to true, and wake it up.
60873	1565818	Once it dies, reset stop ptr, gather result and we're done.
60887	1566129	Setup a clean context for our children to inherit.
60950	1568165	"latency.c: Explicit system-wide latency-expectation infrastructure

The purpose of this infrastructure is to allow device drivers to set
latency constraint they have and to collect and summarize these
expectations globally. The cummulated result can then be used by
power management and similar users to make decisions that have
tradoffs with a latency component.

An example user of this are the x86 C-states; each higher C state saves
more power, but has a higher exit latency. For the idle loop power
code to make a good decision which C-state to use, information about
acceptable latencies is required.

An example announcer of latency is an audio driver that knowns it
will get an interrupt when the hardware has 200 usec of samples
left in the DMA buffer; in that case the driver can set a latency
constraint of, say, 150 usec.

Multiple drivers can each announce their maximum accepted latency,
to keep these appart, a string based identifier is used.


(C) Copyright 2006 Intel Corporation
Author: Arjan van de Ven <arjan@linux.intel.com>

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; version 2
of the License."
60971	1568633	"locking rule: all modifications to current_max_latency and
latency_list need to be done while holding the latency_lock.
latency_lock needs to be taken _irqsave."
60982	1568928	"This function returns the maximum latency allowed, which
happens to be the minimum of all maximum latencies on the
list."
61008	1569685	"set_acceptable_latency - sets the maximum latency acceptable
@identifier: string that identifies this driver
@usecs: maximum acceptable latency for this driver

This function informs the kernel that this device(driver)
can accept at most usecs latency. This setting is used for
power management and similar tradeoffs.

This function sleeps and can only be called from process
context.
Calling this function with an existing identifier is valid
and will cause the existing latency setting to be changed."
61045	1570643	"if we inserted the new one, we're done; otherwise there was
an existing one so we need to free the redundant data"
61070	1571447	"modify_acceptable_latency - changes the maximum latency acceptable
@identifier: string that identifies this driver
@usecs: maximum acceptable latency for this driver

This function informs the kernel that this device(driver)
can accept at most usecs latency. This setting is used for
power management and similar tradeoffs.

This function does not sleep and can be called in any context.
Trying to use a non-existing identifier silently gets ignored.

Due to the atomic nature of this function, the modified latency
value will only be used for future decisions; past decisions
can still lead to longer latencies in the near future."
61099	1572361	"remove_acceptable_latency - removes the maximum latency acceptable
@identifier: string that identifies this driver

This function removes a previously set maximum latency setting
for the driver and frees up any resources associated with the
bookkeeping needed for this.

This function does not sleep and can be called in any context.
Trying to use a non-existing identifier silently gets ignored."
61120	1572853	"If we just deleted the system wide value, we need to
recalculate with a full search"
61136	1573309	"system_latency_constraint - queries the system wide latency maximum

This function returns the system wide maximum latency in
microseconds.

This function does not sleep and can be called in any context."
61156	1574055	"synchronize_acceptable_latency - recalculates all latency decisions

This function will cause a callback to various kernel pieces that
will make those pieces rethink their latency decisions. This implies
that if there are overlong latencies in hardware state already, those
latencies get taken right now. When this call completes no overlong
latency decisions should be active anymore.

Typical usecase of this is after a modify_acceptable_latency() call,
which in itself is non-blocking and non-synchronizing.

This function blocks and should not be called with locks held."
61175	1574780	"Latency notifier: this notifier gets called when a non-atomic new
latency value gets set. The expectation nof the caller of the
non-atomic set is that when the call returns, future latencies
are within bounds, so the functions on the notifier list are
expected to take the overlong latencies immediately, inside the
callback, and not make a overlong latency decision anymore.

The callback gets called when the new latency value is made
active so system_latency_constraint() returns the new latency."
61194	1575341	"we don't want by default to have longer latencies than 2 ticks,
since that would cause lost ticks"
61207	1575571	"kernel/lockdep_internals.h

Runtime locking correctness validator

lockdep subsystem internal functions and variables."
61217	1575897	"MAX_LOCKDEP_ENTRIES is the maximum number of lock dependencies
we track.

We use the per-lock dependency maps in two ways: we grow it by adding
every to-be-taken lock to all currently held lock's own dependency
table (if it's not there yet), and we check it for lock order
conflicts and deadlocks."
61229	1576224	"Stack-trace: tightly packed array of stack backtrace
addresses. Protected by the hash_lock."
61253	1576910	Various lockdep statistics:
61376	1581455	"kernel/mutex-debug.c

Debugging code for mutexes

Started by Ingo Molnar:



lock debugging, locking tree, deadlock detection started by:


Released under the General Public License (GPL)."
61390	1581752	Must be called with lock->wait_lock held.
61422	1582737	Mark the current thread as blocked on the lock:
61456	1583649	Make sure we are not reinitializing a held lock:
61471	1584060	"mutex_destroy - mark a mutex unusable
@lock: the mutex to be destroyed

This function marks the mutex uninitialized, and any subsequent
use of the mutex is forbidden. The mutex must not be locked when
this function is called."
61490	1584567	"Mutexes: blocking mutual exclusion locks

started by Ingo Molnar:



This file contains mutex debugging related internal declarations,
prototypes and inline functions, for the CONFIG_DEBUG_MUTEXES case.
More details are in kernel/mutex-debug.c."
61494	1584625	This must be called with lock->wait_lock held.
61547	1586271	"kernel/mutex.c

Mutexes: blocking mutual exclusion locks

Started by Ingo Molnar:



Many thanks to Arjan van de Ven, Thomas Gleixner, Steven Rostedt and
David Howells for suggestions and improvements.

Also see Documentation/mutex-design.txt."
61558	1586555	"In the DEBUG case we are using the ""NULL fastpath"" for mutexes,
which forces all calls into the slowpath:"
61574	1586897	"mutex_init - initialize the mutex
@lock: the mutex to be initialized

Initialize the mutex to unlocked state.

It is not allowed to initialize an already locked mutex."
61593	1587457	"We split the mutex lock/unlock logic into separate fastpath and
slowpath functions, to reduce the register pressure on the fastpath.
We also put the fastpath first in the kernel image, to make sure the
branch is predicted by the CPU as default-untaken."
61617	1588382	"mutex_lock - acquire the mutex
@lock: the mutex to be acquired

Lock the mutex exclusively for this task. If the mutex is not
available right now, it will sleep until it can get it.

The mutex must later on be released by the same task that
acquired it. Recursive locking is not allowed. The task
may not exit without first unlocking the mutex. Also, kernel
memory where the mutex resides mutex must not be freed with
the mutex still locked. The mutex must first be initialized
(or statically defined) before it can be locked. memset()-ing
the mutex to 0 is not allowed.

( The CONFIG_DEBUG_MUTEXES .config option turns on debugging
checks that will enforce the restrictions and will also do
deadlock debugging. )

This function is similar to (but not equivalent to) down()."
61624	1588558	"The locking fastpath is the 1->0 transition from
'unlocked' into 'locked' state."
61644	1589064	"mutex_unlock - release the mutex
@lock: the mutex to be released

Unlock a mutex that has been locked by this task previously.

This function must not be used in interrupt context. Unlocking
of a not locked mutex is not allowed.

This function is similar to (but not equivalent to) up()."
61650	1589221	"The unlocking fastpath is the 0->1 transition from 'locked'
into 'unlocked' state:"
61658	1589378	Lock a mutex (possibly interruptible), slowpath:
61674	1589880	add waiting tasks to the end of the waitqueue (FIFO):
61693	1590468	"Lets try to take the lock again - this is needed even if
we get here for the first time (shortly after failing to
acquire the lock), to make sure that we get a wakeup once
it's unlocked. Later on, if we sleep, this is the
operation that gives us the lock. We xchg it to -1, so
that when we release the lock, we properly wake up the
other waiters:"
61701	1590639	"got a signal? (This code gets eliminated in the
TASK_UNINTERRUPTIBLE case.)"
61713	1591002	didnt get the lock, go to sleep:
61721	1591179	got the lock - rejoice!
61725	1591344	set it to 0 if there are no waiters left:
61758	1592031	Release the lock, slowpath:
61773	1592509	"some architectures leave the lock unlocked in the fastpath failure
case, others need to leave it locked. In the later case we have to
unlock it here"
61778	1592668	get the first entry from the wait-list:
61795	1592972	Release the lock, slowpath:
61806	1593264	"Here come the less common (and hence less performance-critical) APIs:
mutex_lock_interruptible() and mutex_trylock()."
61820	1593767	"mutex_lock_interruptible - acquire the mutex, interruptable
@lock: the mutex to be acquired

Lock the mutex like mutex_lock(), and return 0 if the mutex has
been acquired or sleep until the mutex becomes available. If a
signal arrives while waiting for the lock then this function
returns -EINTR.

This function is similar to (but not equivalent to) down_interruptible()."
61850	1594551	"Spinlock based trylock, we take the spinlock and check whether we
can get the lock:"
61864	1594985	Set it back to 0 if there are no waiters:
61886	1595667	"mutex_trylock - try acquire the mutex, without waiting
@lock: the mutex to be acquired

Try to acquire the mutex atomically. Returns 1 if the mutex
has been acquired successfully, and 0 on contention.

NOTE: this function follows the spin_trylock() convention, so
it is negated to the down_trylock() return values! Be careful
about this when converting semaphore users to mutexes.

This function must not be used in interrupt context. The
mutex must be released by the same task that acquired it."
61904	1596134	"Mutexes: blocking mutual exclusion locks

started by Ingo Molnar:



This file contains mutex debugging related internal prototypes, for the
!CONFIG_DEBUG_MUTEXES case. Most of them are NOPs:"
61930	1597031	ns_cgroup.c - namespace cgroup subsystem
61964	1597838	"Rules:
1. you can only enter a cgroup which is a child of your current
cgroup
2. you can only place another process into a cgroup if
a. you have CAP_SYS_ADMIN
b. your cgroup is an ancestor of task's destination cgroup
(hence either you are in the same cgroup as task, or in an
ancestor cgroup thereof)"
61992	1598434	"Rules: you can only create a cgroup if
1. you are capable(CAP_SYS_ADMIN)
2. the target cgroup is a descendant of your own cgroup"
62040	1599599	"Author: Serge Hallyn <serue@us.ibm.com>

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License as
published by the Free Software Foundation, version 2 of the
License.

Jun 2006 - namespaces support
OpenVZ, SWsoft Inc.
Pavel Emelianov <xemul@openvz.org>"
62057	1599988	"creates a copy of ""orig"" with refcount 1."
62074	1600443	"Create new nsproxy and all of its the associated namespaces.
Return the newly created nsproxy. Do not attach this to the task,
leave it to the caller to do proper locking and attach it to task."
62146	1602085	"called from clone. This now handles copy for nsproxy and all
namespaces therein."
62205	1603173	"Called from unshare. Unshare all the namespaces part of nsproxy.
On success, returns the new nsproxy."
62249	1604117	"wait for others to get what they want from this nsproxy.

cannot release this nsproxy via the call_rcu() since
put_mnt_ns() will want to sleep"
62272	1604476	linux/kernel/panic.c
62277	1604586	"This function is used through-out the kernel (including mm and fs)
to indicate a major problem."
62315	1605358	Returns how long it waited in ms
62326	1605571	"panic - halt the system
@fmt: The text string to print

Display a message, then perform cleanups.

This function never returns."
62341	1605978	"It's possible to come here directly from a panic-assertion and not
have preempt disabled. Some functions called from here want
preempt to be disabled. No point enabling it later though..."
62355	1606343	"If we have crashed and we have a crash kernel loaded let it handle
everything else.
Do we want to call this before we try to display a message?"
62363	1606537	"Note smp_send_stop is the usual smp shutdown function, which
unfortunately means it may not be hardened to work in a panic
situation."
62376	1606829	"Delay timeout seconds before rebooting the machine.
We can't use the ""normal"" timers since we just panicked.."
62387	1607166	"This will not be a clean reboot, with everything
shutting down. But if there is a chance of
rebooting the system it will be rebooted."
62393	1607301	Make sure the user can actually press Stop-A (L1-A)
62424	1608059	"print_tainted - return a string to represent the kernel taint state.

'P' - Proprietary module has been loaded.
'F' - Module has been forcibly loaded.
'S' - SMP with CPUs not designed for SMP.
'R' - User forced a module unload.
'M' - System experienced a machine check exception.
'B' - System has hit bad_page.
'U' - Userspace-defined naughtiness.

The string is overwritten by the next call to print_taint()."
62447	1608711	can't trust the integrity of the kernel anymore
62472	1609130	"It just happens that oops_enter() and oops_exit() are identically
implemented..."
62483	1609378	This CPU may now print the oops message
62486	1609448	We need to stall this CPU
62488	1609513	This CPU gets to do the counting
62497	1609768	This CPU waits for a different one
62511	1610059	"Return true if the calling CPU is allowed to print oops-related info. This
is a bit racy.."
62529	1610737	"Called when the architecture enters its oops handler, before it prints
anything. If this is the first CPU to oops, and it's oopsing the first time
then let it proceed.

This is all enabled by the pause_on_oops kernel boot option. We do all this
to ensure that oopses don't scroll off the screen. It has the side-effect
of preventing later-oopsing CPUs from mucking up the display, too.

It turns out that the CPU which is allowed to print ends up pausing for the
right duration, whereas all the other CPUs pause for twice as long: once in
oops_enter(), once in oops_exit()."
62532	1610835	can't trust the integrity of the kernel anymore
62538	1610900	64-bit random ID for oopses:
62553	1611152	"Called when the architecture exits its oops handler, after printing
everything."
62566	1611458	"Called when gcc's -fstack-protector feature is used, and
gcc detects corruption of the on-stack canary value"
62583	1611929	"RT Mutexes: blocking mutual exclusion locks with PI support

started by Ingo Molnar and Thomas Gleixner:




This file contains the private data structure and API definitions."
62595	1612333	"The rtmutex in kernel tester is independent of rtmutex debugging. We
call schedule_rt_mutex_test() instead of schedule() for the tasks which
belong to the tester. That way we can delay the wakeup path of those
threads to provoke lock stealing and testing of complex boosting scenarios."
62619	1612994	"This is the control structure for tasks blocked on a rt_mutex,
which is allocated on the kernel stack on of the blocked task.

@list_entry: pi node to enqueue into the mutex waiters list
@pi_list_entry: pi node to enqueue into the mutex owner waiters list
@task: task reference to the blocked task"
62634	1613315	Various helpers to access the waiters-plist:
62666	1613990	lock->owner state tracking:
62690	1614629	PI-futex support (proxy locking functions, etc.):
62722	1615654	"RT-Mutexes: blocking mutual exclusion locks with PI support

started by Ingo Molnar and Thomas Gleixner:




This code is based on the rt.c implementation in the preempt-rt tree.
Portions of said code are




Steven Rostedt <rostedt@goodmis.org>

See rt.c in preempt-rt for proper credits and further information"
62788	1617070	"deadlock detection flag. We turn it off when we detect
the first problem because we dont want to recurse back
into the tracing code when doing error printk or
executing a BUG():"
62826	1617950	"We fill out the fields in the waiter to store the information about
the deadlock. We print when we return. act_waiter can be NULL in
case of a remove waiter operation."
62924	1620498	Make sure we are not reinitializing a held lock:
62948	1621083	"RT-Mutexes: blocking mutual exclusion locks with PI support

started by Ingo Molnar and Thomas Gleixner:




This file contains macros used solely by rtmutex.c. Debug version."
62980	1622327	"RT-Mutex-tester: scriptable tester for rt mutexes

started by Thomas Gleixner:"
63025	1623099	"1 Sched other, data = nice
2 Sched fifo, data = prio
3 Lock uninterruptible, data = lockindex
4 Lock uninterruptible no wait in wakeup, data = lockindex
5 Lock interruptible, data = lockindex
6 Lock interruptible no wait in wakeup, data = lockindex
7 Continue locking after the wakeup delay
8 Unlock, data = lockindex
9 Lock BKL
10 Unlock BKL
11 Signal other test thread, data = thread id
98 Reset event counter
99 Reset all pending operations"
63131	1625882	"Schedule replacement for rtsem_down(). Only called for threads with
PF_MUTEX_TESTER set.

This allows us to have finegrained control over the event flow."
63137	1626020	We have to lookup the task
63220	1627393	Wait for the next command to be executed
63224	1627452	Restore previous command and data
63249	1627911	Wait for the next command to be executed
63271	1628246	"sysfs_test_command - interface for test commands
@dev: thread reference
@buf: command for actual step
@count: length of buffer

command syntax:

opcode:data"
63283	1628581	strings from sysfs write are not 0 terminated!
63287	1628650	strip of \n:
63334	1629550	"sysfs_test_status - sysfs interface for rt tester
@dev: thread to query
@buf: char buffer to be filled with thread status info"
63426	1631751	"RT-Mutexes: blocking mutual exclusion locks with PI support

started by Ingo Molnar and Thomas Gleixner:




This file contains macros used solely by rtmutex.c.
Non-debug version."
63447	1632682	"kernel/rwsem.c: R/W semaphores, public implementation

Written by David Howells (dhowells@redhat.com).
Derived from asm-i386/semaphore.h"
63460	1632887	lock for reading
63473	1633167	trylock for reading -- returns 1 if successful, 0 if contention
63487	1633401	lock for writing
63500	1633680	trylock for writing -- returns 1 if successful, 0 if contention
63514	1633915	release a read lock
63526	1634077	release a write lock
63538	1634255	downgrade write lock to read lock
63544	1634385	"lockdep: a downgraded write will live on as a write
dependency."
63602	1635493	"kernel/time/sched_debug.c

Print the CFS rbtree



This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License version 2 as
published by the Free Software Foundation."
63613	1635709	"This allows printing both to /proc/sched_debug and
to the console"
63624	1635878	Ease the printing of nsec fields:
63994	1644704	"idle-task scheduling class.

(NOTE: these are not related to SCHED_IDLE tasks which are
handled in sched_fair.c)"
63998	1644759	Idle tasks are unconditionally rescheduled:
64014	1645096	"It is not legal to sleep in the idle task - print a warning
message if some code attempts to do it:"
64056	1646002	Simple, special scheduling class for the per-CPU idle tasks:
64059	1646069	".next is NULL
no enqueue/yield_task for idle tasks"
64061	1646175	dequeue is not valid, we print a debug message there:
64076	1646558	no .task_new for idle tasks
64082	1646651	"Real-Time Scheduling Class (mapped to the SCHED_FIFO and SCHED_RR
policies)"
64087	1646768	"Update the current task's runtime statistics. Skip current tasks that
are not in our scheduling class."
64117	1647479	Adding/removing a task to/from a priority array:
64132	1647838	"Put task to the end of the run list without the overhead of dequeue
followed by enqueue."
64148	1648160	Preempt the current task with a newly woken task if needed:
64187	1649099	"Load-balancing iterator. Note: while the runqueue stays locked
during the whole iteration, the current task might be
dequeued so the iterator has to be dequeue-safe. Here we
achieve that by always pre-iterating before returning
the current task:"
64229	1650032	"If we arrived back to the head again then
iterate to the next queue (if any):"
64265	1650882	"pass 'busiest' rq argument into
load_balance_[start|next]_rt iterators"
64294	1651632	"RR tasks need a special form of timeslice management.
FIFO tasks have no timeslices."
64306	1651827	"Requeue to the end of queue if we are not the only element
on the queue:"
64345	1652680	"bump this up when changing the output format or the meaning of an existing
format, so that tools can adapt (or abort)"
64361	1653041	runqueue-specific stats
64374	1653473	domain-specific stats
64436	1655046	Expects runqueue lock to be held for atomicity of update
64448	1655283	Expects runqueue lock to be held for atomicity of update
64458	1655654	!CONFIG_SCHEDSTATS
64485	1656826	"Called when a process is dequeued from the active array and given
the cpu. We should note that with the exception of interactive
tasks, the expired queue will become the active queue after the active
queue is empty, without explicitly dequeuing and requeuing tasks in the
expired queue. (Interactive tasks may be requeued directly to the
active queue, thus delaying tasks in the expired queue from running;
see scheduler_tick()).

This function is only called from sched_info_arrive(), rather than
dequeue_task(). Even though a task may be queued and dequeued multiple
times as it is shuffled about, we're really interested in knowing how
long it was from the *first* time it was queued to the time that it
finally hit a cpu."
64495	1657120	"Called when a task finally hits the cpu. We can now calculate how
long it was waiting to run. We also note when it began so that we
can keep stats on how long its timeslice is."
64524	1658280	"Called when a process is queued into either the active or expired
array. The time is noted and later used to determine how long we
had to wait for us to reach the cpu. Since the expired queue will
become the active queue after active queue is empty, without dequeuing
and requeuing any tasks, we are interested in queuing to either. It
is unusual but not impossible for tasks to be dequeued and immediately
requeued in the same or another array: this can happen in sched_yield(),
set_user_nice(), and even load_balance() as it moves tasks from runqueue
to runqueue.

This function is only called from enqueue_task(), but also only updates
the timestamp if it is already not set. It's assumed that
sched_info_dequeued() will clear that stamp when appropriate."
64535	1658613	"Called when a process ceases being the active-running process, either
voluntarily or involuntarily. Now we can calculate how long we ran."
64549	1659050	"Called when tasks are switched involuntarily due, typically, to expiring
their time slice. (This may also be called when switching to or from
the idle task.) We are only called when prev != next."
64559	1659330	"prev now departs the cpu. It's not interesting to record
stats about how efficient we were at scheduling the idle
process, however."
64575	1659755	CONFIG_SCHEDSTATS || CONFIG_TASK_DELAY_ACCT
64584	1659917	"linux/kernel/seccomp.c



This defines a simple but solid secure-computing mode."
64589	1660001	#define SECCOMP_DEBUG 1
64596	1660197	"Secure computing mode 1 allows only read/write/exit/sigreturn.
To be fully secure this must be combined with rlimit
to limit the stack allocations too."
64599	1660338	null terminated
64605	1660515	null terminated
64645	1661121	can set it only once to be even more secure
64671	1661633	"Detect Soft Lockups

started by Ingo Molnar, Copyright (C) 2005, 2006 Red Hat, Inc.

this code detects soft lockups: incidents in where on a CPU
the kernel does not reschedule for 10 seconds or more."
64708	1662494	"Returns seconds, approximately. We don't need nanosecond
resolution, and we don't need to waste time with a big divide when
2^30ns == 1.074s."
64711	1662600	2^30 ~= 10^9
64726	1662916	Cause each CPU to re-update its timestamp rather than complain
64735	1663143	"This callback runs from the timer interrupt, and checks
whether the watchdog thread has hung or not:"
64751	1663536	report at most once a second
64758	1663735	do not print during early bootup:
64766	1663919	Wake up the high-prio watchdog task every second:
64770	1664058	Warn about unreasonable 10+ seconds delays:
64789	1664509	The watchdog thread - runs every second and touches the timestamp.
64796	1664694	initialize timestamp
64803	1664910	"Run briefly once per second to reset the softlockup timestamp.
If this gets delayed for more than 10 seconds then the
debug-printout triggers in softlockup_tick()."
64815	1665111	Create/destroy watchdog threads as CPUs come and go:
64844	1665930	Unbind so it can run. Fall thru.
64853	1666196	CONFIG_HOTPLUG_CPU
64898	1667611	"Sleepable Read-Copy Update mechanism for mutual exclusion.

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.



Author: Paul McKenney <paulmck@us.ibm.com>

For detailed explanation of Read-Copy Update mechanism see -
Documentation/RCU/ *.txt"
64917	1668111	"init_srcu_struct - initialize a sleep-RCU structure
@sp: structure to initialize.

Must invoke this on a given srcu_struct before passing that srcu_struct
to any other function. Each srcu_struct represents a separate domain
of SRCU protection."
64929	1668434	"srcu_readers_active_idx -- returns approximate number of readers
active on the specified rank of per-CPU counters."
64949	1668977	"srcu_readers_active - returns approximate number of readers.
@sp: which srcu_struct to count active readers (holding srcu_read_lock).

Note that this is not an atomic primitive, and can therefore suffer
severe errors when invoked on an active srcu_struct. That said, it
can be useful as an error check at cleanup time."
64961	1669343	"cleanup_srcu_struct - deconstruct a sleep-RCU structure
@sp: structure to clean up.

Must invoke this after you are finished using a given srcu_struct that
was initialized via init_srcu_struct(), else you leak memory."
64967	1669496	Leakage unless caller handles error.
64981	1669920	"srcu_read_lock - register a new reader for an SRCU-protected structure.
@sp: srcu_struct in which to register the new reader.

Counts the new reader in the appropriate per-CPU element of the
srcu_struct. Must be called from process context.
Returns an index that must be passed to the matching srcu_read_unlock()."
64988	1670090	ensure compiler looks -once- at sp->completed.
64990	1670224	ensure compiler won't misorder critical section.
65004	1670726	"srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
@sp: srcu_struct in which to unregister the old reader.
@idx: return value from corresponding srcu_read_lock().

Removes the count for the old reader from the appropriate per-CPU
element of the srcu_struct. Note that this may well be a different
CPU than that which was incremented by the corresponding srcu_read_lock().
Must be called from process context."
65008	1670876	ensure compiler won't misorder critical section.
65026	1671621	"synchronize_srcu - wait for prior SRCU read-side critical-section completion
@sp: srcu_struct with which to synchronize.

Flip the completed counter, and wait for the old count to drain to zero.
As with classic RCU, the updater must use some separate means of
synchronizing concurrent updates. Can block; must be called from
process context.

Note that it is illegal to call synchornize_srcu() from the corresponding
SRCU read-side critical section; doing so will result in deadlock.
However, it is perfectly legal to call synchronize_srcu() on one
srcu_struct from some other srcu_struct's read-side critical section."
65042	1672194	"Check to see if someone else did the work for us while we were
waiting to acquire the lock. We need -two- advances of
the counter, not just one. If there was but one, we might have
shown up -after- our helper's first synchronize_sched(), thus
having failed to prevent CPU-reordering races with concurrent
srcu_read_unlock()s on other CPUs (see comment below). So we
either (1) wait for two or (2) supply the second ourselves."
65049	1672335	Force memory barrier on all CPUs.
65061	1672788	"The preceding synchronize_sched() ensures that any CPU that
sees the new value of sp->completed will also see any preceding
changes to data structures made by this CPU. This prevents
some other CPU from reordering the accesses in its SRCU
read-side critical section to precede the corresponding
srcu_read_lock() -- ensuring that such references will in
fact be protected.

So it is now safe to do the flip."
65066	1672899	Force memory barrier on all CPUs.
65074	1673202	"At this point, because of the preceding synchronize_sched(),
all srcu_read_lock() calls using the old counters have completed.
Their corresponding critical sections might well be still
executing, but the srcu_read_lock() primitives themselves
will have finished executing."
65079	1673346	Force memory barrier on all CPUs.
65107	1674644	"The preceding synchronize_sched() forces all srcu_read_unlock()
primitives that were executing concurrently with the preceding
for_each_possible_cpu() loop to have completed by this point.
More importantly, it also forces the corresponding SRCU read-side
critical sections to have also completed, and the corresponding
references to SRCU-protected data items to be dropped.

Note:

Despite what you might think at first glance, the
preceding synchronize_sched() -must- be within the
critical section ended by the following mutex_unlock().
Otherwise, a task taking the early exit can race
with a srcu_read_unlock(), which might have executed
just before the preceding srcu_readers_active() check,
and whose CPU might have reordered the srcu_read_unlock()
with the preceding critical section. In this case, there
is nothing preventing the synchronize_sched() task that is
taking the early exit from freeing a data structure that
is still being referenced (out of order) by the task
doing the srcu_read_unlock().

Alternatively, the comparison with ""2"" on the early exit
could be changed to ""3"", but this increases synchronize_srcu()
latency for bulk loads. So the current code is preferred."
65118	1674940	"srcu_batches_completed - return batches completed.
@sp: srcu_struct on which to report batch completion.

Report the number of batches, correlated with, but not necessarily
precisely the same as, the number of grace periods that have elapsed."
65139	1675432	"kernel/stacktrace.c

Stack trace management functions"
65160	1675862	GPL v2 and any later version.
65176	1676302	"Since we effect priority and affinity (both of which are visible
to, and settable by outside processes) we do indirection via a
kthread."
65178	1676350	Thread to stop each CPU in user context.
65199	1676807	"Ack: we are alive
Theoretically the ack = 0 might not be on this CPU yet."
65202	1676947	Simple state machine
65210	1677173	"Ack: irqs disabled.
Must read state first."
65214	1677372	Everyone is in place, hold CPU.
65217	1677453	Must read state first.
65221	1677592	"Yield in first stage: migration threads need to
help our sisters onto their CPUs."
65229	1677694	"Ack: we are exiting.
Must read state first."
65240	1677895	Change the thread state
65267	1678534	Wait for them all to come to life.
65271	1678657	If some failed, kill them all.
65277	1678799	Now they are all started, make them hold the CPUs, ready.
65281	1678896	Make them disable irqs.
65314	1679462	We're done: you can kthread_stop us now
65317	1679518	Wait for kthread_stop
65339	1680033	If they don't care which CPU fn runs on, bind to any online one.
65347	1680286	One high-prio thread per cpu. We'll do this one.
65362	1680611	No CPUs can come up or down during this.
65382	1681002	"we can't #include <linux/syscalls.h> here,
but tell gcc to not warn with -Wmissing-prototypes"
65387	1681101	Non-implemented system calls get redirected here.
65499	1684389	arch-specific weak syscall entries
65509	1684670	mmu depending weak syscall entries
65523	1685055	block-layer dependent
65528	1685170	New file descriptors
65551	1685940	"tsacct.c - System accounting over taskstats interface




This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details."
65561	1686117	fill in basic accounting fields
65569	1686325	calculate task elapsed time in timespec
65572	1686449	rebase elapsed time to usec
65616	1687708	fill in extended accounting fields
65621	1687844	convert pages-jiffies to Mbyte-usec
65626	1688044	adjust to KB unit
65651	1688719	"acct_update_integrals - update mm integral fields in task_struct
@tsk: task_struct for accounting"
65669	1689190	"acct_clear_integrals - clear the mm integral fields in task_struct
@tsk: task_struct whose accounting fields are cleared"
65681	1689475	"Wrapper functions for 16bit uid back compatibility. All nicely tied
together in the faint hope we can take the out in five years time."
65700	1689995	avoid REGPARM breakage on x86:
65708	1690241	avoid REGPARM breakage on x86:
65716	1690468	avoid REGPARM breakage on x86:
65724	1690676	avoid REGPARM breakage on x86:
65732	1690843	avoid REGPARM breakage on x86:
65740	1691051	avoid REGPARM breakage on x86:
65748	1691218	avoid REGPARM breakage on x86:
65757	1691468	avoid REGPARM breakage on x86:
65777	1692036	avoid REGPARM breakage on x86:
65796	1692525	avoid REGPARM breakage on x86:
65804	1692696	avoid REGPARM breakage on x86:
65913	1694782	"This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License as
published by the Free Software Foundation, version 2 of the
License."
65935	1695239	"Clone a new ns copying an original user ns, setting refcount to 1
@old_ns: namespace to clone
Return NULL on error (failure to kmalloc), new ns otherwise"
65951	1695611	Insert new root user.
65958	1695755	Reset current->user with a new one
65995	1696387	CONFIG_USER_NS
66006	1696684	"Author: Eric Biederman <ebiederm@xmision.com>

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License as
published by the Free Software Foundation, version 2 of the
License."
66041	1697381	"Special case of dostring for the UTS structure. This has locks
to observe. Should this be in kernel/sys.c ????"
66059	1697872	The generic string strategy routine:
66154	1700198	"Author: Serge Hallyn <serue@us.ibm.com>

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License as
published by the Free Software Foundation, version 2 of the
License."
66166	1700496	"Clone a new ns copying an original utsname, setting refcount to 1
@old_ns: namespace to clone
Return NULL on error (failure to kmalloc), new ns otherwise"
66187	1701022	"Copy task tsk's utsname namespace, or clone it if flags
specifies CLONE_NEWUTS. In latter case, changes to the
utsname of this process won't be seen by parent, and vice
versa."
66216	1701513	"Generic waiting primitives.

(C) 2004 William Irwin, Oracle"
66276	1703190	"Note: we use ""set_current_state()"" _after_ the wait-queue add,
because we need a memory barrier there on SMP, so that any
wake-function that tests for the wait-queue being active
will be guaranteed to see waitqueue addition _or_ subsequent
tests in this thread will see the wakeup having taken place.

The spin_unlock() itself is semi-permeable and only protects
one way (it only protects stuff inside the critical region and
stops them from bleeding out - it would still allow subsequent
loads to move into the critical region)."
66289	1703539	"don't alter the task state if this is just going to
queue an async wait queue callback"
66308	1704033	"don't alter the task state if this is just going to
queue an async wait queue callback"
66332	1704761	"We can check for list emptiness outside the lock
IFF:
- we use the ""careful"" check that verifies both
the next and prev pointers, so that there cannot
be any half-pending updates in progress on other
CPU's that we haven't seen yet (and that might
still change the stack area.
and
- all other users take the lock (ie we can only
have _one_ other CPU that looks at or modifies
the list)."
66370	1705861	"To allow interruptible waiting and asynchronous (i.e. nonblocking)
waiting, the actions of __wait_on_bit() and __wait_on_bit_lock() are
permitted return codes. Nonzero return codes halt waiting and return."
66449	1708409	"wake_up_bit - wake up a waiter on a bit
@word: the word being waited on, a kernel virtual address
@bit: the bit of the word being waited on

There is a standard hashed waitqueue table for generic use. This
is the part of the hashtable's accessor API that wakes up waiters
on a bit. For instance, if one were to have waiters on a bitflag,
one would call wake_up_bit() after clearing the bit.

In order for this to function properly, as it uses waitqueue_active()
internally, some kind of memory barrier must be done prior to calling
this. Typically, this will be smp_mb__after_clear_bit(), but in some
cases where bitflags are manipulated non-atomically under a lock, one
may need to use a less regular barrier, such fs/inode.c's smp_mb(),
because spin_unlock() does not guarantee a memory barrier."
